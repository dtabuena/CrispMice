{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crispy_Analyze_ABF_files.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+apj98l4EWArOGqfihh2l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtabuena/CrispyMice/blob/main/RNF182/Crispy_RNF182_Analyze_ABF_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### SORCE DATA: RNF182 Dropbox\n",
        "link = \"https://www.dropbox.com/sh/n9t8p257wnzlijk/AAC9Z36JodisyZjnM3mkJC3Xa?dl=0\""
      ],
      "metadata": {
        "id": "s-aIbCWF-RlD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZKwnij3xH0D7"
      },
      "outputs": [],
      "source": [
        "################ Helpers ######################\n",
        "def print_assert():\n",
        "    'Print assertion failure messages (typicaly when try/except is employed)'\n",
        "    import sys\n",
        "    import traceback\n",
        "    _, _, tb = sys.exc_info()\n",
        "    # traceback.print_tb(tb) # Fixed format\n",
        "    tb_info = traceback.extract_tb(tb)\n",
        "    filename, line, func, text = tb_info[-1]\n",
        "    print(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def flatten(list_of_lists):\n",
        "    'Recursively Flatten list of lists'\n",
        "    if len(list_of_lists) == 0:\n",
        "        return list_of_lists\n",
        "    if isinstance(list_of_lists[0], list):\n",
        "        return flatten(list_of_lists[0]) + flatten(list_of_lists[1:])\n",
        "    return list_of_lists[:1] + flatten(list_of_lists[1:])\n",
        "\n",
        "\n",
        "def protocol_baseline_and_stim(abf):\n",
        "    'Return two boolean arrays, distiguishing holding I/V and electrical stimuli'\n",
        "    # use command signal variance to determine stimulus periods\n",
        "    commands = []\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(sweepNumber=s)\n",
        "        commands.append(abf.sweepC)\n",
        "    commands = np.stack(commands)\n",
        "    \n",
        "    std = np.std(commands, axis=0)\n",
        "    is_base = std==0\n",
        "    is_stim = np.logical_not(is_base)\n",
        "    return is_base, is_stim\n",
        "\n",
        "def abf_or_name(abf):\n",
        "    if str(type(abf))==\"<class 'str'>\":\n",
        "        abf = pyabf.ABF( abf )\n",
        "    return abf\n",
        "\n",
        "def plot_sweeps_and_command(abf,figsize = [8,2],windows=[]):\n",
        "    'Plot an abf file with sweeps and command'\n",
        "    'also attempts to calibrate telegraph offset when a Ch1 is a secondary ouput of the amp'\n",
        "    abf = abf_or_name(abf)\n",
        "    axs_2_right = []\n",
        "    # plot sweeps and command (single channel)\n",
        "\n",
        "\n",
        "    discretized_cmap = matplotlib.cm.get_cmap('viridis', len(abf.sweepList))\n",
        "\n",
        "    num_ch = 1;\n",
        "    if len(abf.channelList)>1:\n",
        "        num_ch = 2\n",
        "    fig, axs = plt.subplots(num_ch, figsize = np.array(figsize)*np.array([1,num_ch]))\n",
        "    if num_ch ==1:\n",
        "        axs=[axs]\n",
        "    theta, offset, correct_ch1 = predict_telegraph(abf)\n",
        "    if num_ch==2:\n",
        "        axs1_r = axs[1].twinx()\n",
        "        axs[1].set_ylabel(str('Ch1 '+str(abf.sweepLabelC)))\n",
        "        axs1_r.set_ylabel(str('Corrrected '+str(abf.sweepLabelC)[-4:]) )\n",
        "        axs[1].set_title(str(theta) +','+ str(offset))\n",
        "        axs[1].set_title('Command Correction')\n",
        "    axs0_r = axs[0].twinx()\n",
        "    for sweep in abf.sweepList:\n",
        "        abf.setSweep(sweepNumber=sweep)\n",
        "        axs[0].plot(abf.sweepX, abf.sweepY,color=discretized_cmap(sweep))        \n",
        "        axs0_r.plot(abf.sweepX, abf.sweepC,color='grey' )#discretized_cmap(sweep),linestyle='dotted')\n",
        "        com = abf.sweepC\n",
        "        if len(abf.channelList)>1:\n",
        "            abf.setSweep(sweepNumber=sweep, channel=1)\n",
        "            axs[1].plot(abf.sweepX, abf.sweepY,'m')\n",
        "            corrected_com = correct_ch1(theta,abf.sweepY)\n",
        "            axs1_r.plot(abf.sweepX, corrected_com,'c',linestyle='dotted')\n",
        "            # axs_2.set_title( str(theta)+','+ str(correct_ch1(theta,abf.sweepY[0])) )\n",
        "    axs[0].set_title(abf.abfID)\n",
        "    abf.setSweep(0)\n",
        "    axs[0].set_ylabel(str(abf.sweepLabelY))\n",
        "    axs[0].set_xlabel(str(abf.sweepLabelX))\n",
        "    axs0_r.set_ylabel(str(abf.sweepLabelC))\n",
        "    axs0_r.set_xlabel(str(abf.sweepLabelX))\n",
        "\n",
        "    cmap = matplotlib.cm.get_cmap('Dark2')\n",
        "    color_num=0\n",
        "    for w in windows:\n",
        "        color_num+=1\n",
        "        rgba = cmap(color_num/len(windows))\n",
        "        ylim = axs[0].get_ylim()\n",
        "        sr = np.mean(np.diff( w ))\n",
        "        gaps = np.where(np.diff(w)>sr*4)[0]\n",
        "        sw = flatten([w[0],[ list(w[[i,i+1]]) for i in np.arange(len(w)-1) if np.diff(w)[i] > sr*3 ] , w[-1] ])\n",
        "        for i in range(0, len(sw), 2):\n",
        "            axs[0].axvspan(sw[i], sw[i+1], ylim[0], ylim[1], alpha=0.2,color=rgba)\n",
        "\n",
        "\n",
        "    axs[0].set_zorder(1)  # default zorder is 0 for ax1 and ax2\n",
        "    axs[0].patch.set_visible(False)  # prevents ax1 from hiding ax2\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    return fig, axs , theta\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lV9qOxbL-P2r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################# Math ######################\n",
        "import math\n",
        "\n",
        "\n",
        "def movmean(x, w):\n",
        "    'A moving mean filter'\n",
        "    w = int(w)\n",
        "    # plt.plot(np.arange(len(x)) - int(len(x)/2), x)\n",
        "    if len(x) < w: w = len(x)\n",
        "    px = np.pad(x,int(np.ceil((w-1)/2)),'edge') \n",
        "    if len(px)-len(x) == w: px = px[0:-1]\n",
        "    conv = np.convolve(px, np.ones(w), 'valid') / w\n",
        "    return conv\n",
        "\n",
        "def mono_exp(time, peak, tau, ss):\n",
        "    'A single exponential decay function'\n",
        "    return (peak * np.exp(-time / (tau)) + ss)\n",
        "\n",
        "def rms_noise(x):\n",
        "    return np.sqrt(    np.sum((x-x.mean())**2)/len(x)    )   \n",
        "\n",
        "def predict_telegraph_DEFUNCT(abf,to_plot=False,stabilize_one_to_one = True,v_res = 10):\n",
        "    'Uses the difference between the voltage protocol command and'\n",
        "    'Secondary ouput channel to calcualte the offset and gain between'\n",
        "    'amplifier and digitizer'\n",
        "    abf = abf_or_name(abf)\n",
        "    theta = [1,0]\n",
        "    try:\n",
        "        if len(abf.channelList)>1:\n",
        "            all_ch1_y =[]\n",
        "            all_ch0_com =[]\n",
        "            for sweep in abf.sweepList:\n",
        "                abf.setSweep(sweepNumber=sweep, channel=1)\n",
        "                all_ch1_y.append(abf.sweepY)\n",
        "                abf.setSweep(sweepNumber=sweep, channel=0)\n",
        "                all_ch0_com.append(abf.sweepC)\n",
        "            all_ch1_y = np.concatenate(all_ch1_y)\n",
        "            all_ch0_com = np.concatenate(all_ch0_com)\n",
        "            theta = np.polyfit(all_ch0_com, all_ch1_y, 1)\n",
        "            # print(theta,'theta_1')\n",
        "            modes = []\n",
        "            steps = list(set(all_ch0_com))\n",
        "            steps.sort()\n",
        "            steps = np.array(steps)\n",
        "            for s in steps:\n",
        "                mode = scipy.stats.mode( all_ch1_y[all_ch0_com==s] )[0]\n",
        "                modes.append(mode)\n",
        "            modes = np.concatenate(modes)\n",
        "            m_ind = [i for i in np.arange(len(modes)) if modes[i]<-40]\n",
        "            steps = steps[m_ind]\n",
        "            modes = modes[m_ind]\n",
        "            theta = np.polyfit(steps, modes, 1)\n",
        "            # print(theta,'theta_2')\n",
        "            if to_plot:\n",
        "                plt.scatter(steps,modes)\n",
        "                y_hat = steps*theta[0]+theta[1]\n",
        "                plt.plot(steps,  y_hat )\n",
        "                plt.show()\n",
        "            if to_plot:\n",
        "                for s in steps:\n",
        "                    counts, bin_edges = np.histogram( all_ch1_y[all_ch0_com==s],bins=50,range=(-120,70),density=True )\n",
        "                    plt.plot(bin_edges[1:],counts)\n",
        "                    mode = scipy.stats.mode( all_ch1_y[all_ch0_com==s] )\n",
        "                    # plt.scatter(s,mode)\n",
        "                plt.gca().xaxis.set_major_locator(plt.MultipleLocator(10))\n",
        "                plt.gca().grid()\n",
        "                plt.show()\n",
        "    except:\n",
        "        'keep default theta'\n",
        "    if stabilize_one_to_one:\n",
        "        # if np.round(theta[0],1)==1.0:\n",
        "        theta = [theta[0] , round(theta[1]/v_res)*v_res ]\n",
        "        if theta[0]>1: theta[0]=math.floor(theta[0])\n",
        "\n",
        "    offset = theta[1]/theta[0]\n",
        "    def correct_ch1(t,signal):\n",
        "        return t[1]/t[0] + (signal - t[1]) / t[0]\n",
        "    return theta, offset, correct_ch1\n",
        "\n",
        "def predict_telegraph(abf,to_plot=False,stabilize_one_to_one = True,v_res = 10):\n",
        "    'Uses the difference between the voltage protocol command and'\n",
        "    'Secondary ouput channel to calcualte the offset and gain between'\n",
        "    'amplifier and digitizer'\n",
        "    abf = abf_or_name(abf)\n",
        "    theta = [1,0]\n",
        "    try:\n",
        "        if len(abf.channelList)>1:\n",
        "            all_ch1_y =[]\n",
        "            all_ch0_com =[]\n",
        "            for sweep in abf.sweepList:\n",
        "                abf.setSweep(sweepNumber=sweep, channel=1)\n",
        "                all_ch1_y.append(abf.sweepY)\n",
        "                abf.setSweep(sweepNumber=sweep, channel=0)\n",
        "                all_ch0_com.append(abf.sweepC)\n",
        "            all_ch1_y = np.concatenate(all_ch1_y)\n",
        "            all_ch0_com = np.concatenate(all_ch0_com)\n",
        "\n",
        "            test_theta = np.polyfit(all_ch0_com, all_ch1_y, 1)\n",
        "            y_hat = test_theta[1]/test_theta[0] + (all_ch0_com - test_theta[1]) / test_theta[0]\n",
        "            err = abs(y_hat-all_ch1_y)\n",
        "            small_err = err<(np.mean(err)+0.2*np.std(err))\n",
        "            if to_plot:\n",
        "                plt.scatter(all_ch0_com[small_err],all_ch1_y[small_err])\n",
        "                plt.show()\n",
        "            theta = np.polyfit(all_ch0_com[small_err], all_ch1_y[small_err], 1)\n",
        "    except:\n",
        "        'keep default theta'\n",
        "    offset = theta[1]/theta[0]\n",
        "    def correct_ch1(theta,signal):\n",
        "        return theta[1]/theta[0] + (signal - theta[1]) / theta[0]\n",
        "    return theta, offset, correct_ch1"
      ],
      "metadata": {
        "id": "IFLDC-cJGMWb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########## Importing ABFs From DropBox ################\n",
        "\n",
        "def get_drobox_folder(link, new_filename):\n",
        "    'Download a folder from dropbox and unzip'\n",
        "\n",
        "    suffix_start = new_filename.find(\".zip\")\n",
        "    new_filename_stripped = new_filename[0:suffix_start]\n",
        "    zipped_file_path = \"/content/\"+new_filename\n",
        "    unzipped_file_path = \"/content/\"+new_filename_stripped\n",
        "    if not( os.path.exists(zipped_file_path)):\n",
        "        !wget -O $new_filename $link    # download with new name\n",
        "    # if not( os.path.exists(new_filename_stripped)):\n",
        "    !echo A | unzip $zipped_file_path -d $unzipped_file_path \n",
        "    return new_filename_stripped\n",
        "\n",
        "def get_sub_files(rootdir):\n",
        "    'Recursively search subfolders and return a list of all files'\n",
        "    file_list =[]\n",
        "    for rootdir, dirs, files in os.walk(file_loc): \n",
        "            file_list.extend([os.path.join(rootdir,f) for f in files])\n",
        "    return file_list\n",
        "\n",
        "\n",
        "\n",
        "def catalogue_recs(file_loc,cell_id_order):\n",
        "    'Read metadata from abf files stored in chosen folder and assigns'\n",
        "    'them to a dataframe for further processing. All further abf analyses'\n",
        "    'read files from this df and report values in the df.'\n",
        "\n",
        "    file_list = get_sub_files(file_loc)\n",
        "    # file_list = [file_loc+'/'+f for f in file_list]\n",
        "\n",
        "    file_list=[f for f in file_list if '.abf' in f]\n",
        "\n",
        "    abf_recordings_df = pd.DataFrame(data = file_list, columns=['file_name'])    \n",
        "    abf_recordings_df = abf_recordings_df.set_index('file_name')\n",
        "\n",
        "    abf_recordings_df['Recording_name'] = None\n",
        "    abf_recordings_df['cell_id'] = None\n",
        "    for c in cell_id_order:\n",
        "        abf_recordings_df[c] = None\n",
        "    \n",
        "    abf_recordings_df[\"protocol\"] = None\n",
        "    abf_recordings_df[\"abf_timestamp\"] = None\n",
        "    abf_recordings_df[\"channelList\"] = None\n",
        "\n",
        "\n",
        "    for r in np.arange(len(abf_recordings_df)):\n",
        "        row_filename = abf_recordings_df.index[r]\n",
        "        if '.sta' in row_filename:\n",
        "            continue\n",
        "        base_name = os.path.basename(row_filename)\n",
        "        abf_recordings_df.loc[row_filename,'Recording_name'] = base_name\n",
        "        split_words = base_name.split('_')\n",
        "        re_code = ['_'+split_words[i] for i in range(len(cell_id_order))]\n",
        "        re_code = ''.join(re_code)[1:]\n",
        "        abf_recordings_df.loc[row_filename,'cell_id'] = re_code\n",
        "        for ci in range(len(cell_id_order)):\n",
        "            abf_recordings_df.loc[row_filename,cell_id_order[ci]] = split_words[ci]\n",
        "\n",
        "        abf = pyabf.ABF(row_filename)\n",
        "        abf_recordings_df.loc[row_filename,'protocol'] = abf.protocol\n",
        "        abf_recordings_df.at[row_filename,'channelList'] = abf.channelList\n",
        "        abf_recordings_df.at[row_filename,'abf_timestamp'] = abf.abfDateTimeString\n",
        "    abf_recordings_df.sort_values('file_name',inplace=True)\n",
        "    protocol_set = list(set(abf_recordings_df['protocol']))\n",
        "    return abf_recordings_df, protocol_set\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_pulses(command):\n",
        "    'Searches a command signal for start and stop times'\n",
        "    'of stimuli'\n",
        "    is_base = command==command[0]\n",
        "    is_step = np.logical_not(is_base)\n",
        "    step_start = np.logical_and(is_base[:-1], is_step[1:])\n",
        "    step_stop = np.logical_and(is_step[:-1], is_base[1:])\n",
        "    starts = np.where(step_start)[0]\n",
        "    stops = np.where(step_stop)[0]\n",
        "    return starts, stops\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R67DNyK1GReQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "##########################################################\n",
        "##########  FIRING RATE GAIN ################\n",
        "\n",
        "def IF_curve_analysis(abf_recordings_df,protocol_aliases,R2_thresh = 0.8, to_plot = 0 ):\n",
        "    'Loop through all abfs finding approproate voltage protocols and calculates the firing rate gain, spikes per pA of all'\n",
        "\n",
        "    abf_recordings_df['Gain_Stims_pA'] = None\n",
        "    abf_recordings_df['Gain_NumSpikes'] = None\n",
        "    abf_recordings_df['Gain_Stims_pA'] = abf_recordings_df['Gain_Stims_pA'].astype(object)\n",
        "    abf_recordings_df['Gain_NumSpikes'] = abf_recordings_df['Gain_NumSpikes'].astype(object)\n",
        "    abf_recordings_df['v_before_stim'] = None\n",
        "    abf_recordings_df['Firing_Duration_%'] = np.nan\n",
        "    \n",
        "    spike_args = {'spike_thresh':10,\n",
        "                    'high_dv_thresh': 25,\n",
        "                    'low_dv_thresh': -5,\n",
        "                     'window_ms': 2}\n",
        "\n",
        "\n",
        "\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol] ):\n",
        "        try:\n",
        "            gain_slope, R2, stim_currents, spike_counts, v_before_stim, fire_dur = analyze_gain_abf(file_name,spike_args,R2_thresh,to_plot) \n",
        "\n",
        "            abf_recordings_df.at[file_name,'Gain_Stims_pA'] = stim_currents\n",
        "            abf_recordings_df.at[file_name,'Gain_NumSpikes'] = spike_counts\n",
        "            abf_recordings_df.at[file_name,'v_before_stim'] = np.mean(v_before_stim)\n",
        "            abf_recordings_df.at[file_name,'Firing_Gain_(Hz/pA)'] = gain_slope\n",
        "            abf_recordings_df.at[file_name,'R2 (Firing_Gain_R2)'] = R2\n",
        "            abf_recordings_df.at[file_name,'Firing_Duration_%'] = np.nanmedian(fire_dur)\n",
        "        except: \n",
        "            print('\\n' + 'unknown error on ', file_name)\n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "\n",
        "def analyze_gain_abf(file_name,spike_args,R2_thresh = 0.8, to_plot = 0):\n",
        "    # print(\"#\" , list(abf_recordings_df.index).index(file_name))\n",
        "    abf = pyabf.ABF( file_name )\n",
        "    if len(abf.sweepList)<5: #print( 'not enough sweeps')\n",
        "        return np.nan, np.nan, np.nan, np.nan, np.nan , np.nan \n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "    stim_currents, spike_counts, spike_rates,_,v_before_stim, fire_dur = spikes_per_stim(abf,spike_args, mode='count', to_plot=to_plot)\n",
        "    if sum(spike_counts)==0:    #if no spikes return none\n",
        "        return np.nan, np.nan, np.nan, np.nan, np.nan , np.nan \n",
        "    if_fit = fit_firing_gain( stim_currents, spike_counts, spike_rates ,to_plot=to_plot>0)\n",
        "    gain_slope = if_fit['slope']\n",
        "    R2 = if_fit['R2']\n",
        "    return gain_slope, R2, stim_currents, spike_counts, v_before_stim, fire_dur\n",
        "\n",
        "def spikes_per_stim(abf,spike_args,thresh=20,mode='count', to_plot=0):\n",
        "    'Loops through sweeps of and abf to find spikes'\n",
        "    # init\n",
        "    stim_currents = []\n",
        "    spike_rates = []\n",
        "    spike_counts = []\n",
        "    v_before_spike1 = []\n",
        "    v_before_stim = []\n",
        "    fire_dur = []\n",
        "    # get sweep info\n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "\n",
        "    # get spike per sweep\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s)\n",
        "        dVds, over_thresh, inds, mean_spike_rate = find_spike_in_trace(abf.sweepY,abf.sampleRate,spike_args,is_stim=is_stim,mode='count',to_plot=to_plot)\n",
        "        rel_firing_duration = check_inactivation( abf.sweepX, abf.sweepY, is_stim, abf.sampleRate, dVds, inds, mean_spike_rate, to_plot=0 )\n",
        "        # plot id'd spikes\n",
        "        if to_plot>1:\n",
        "            fig, axs = plt.subplots(1)\n",
        "            axs.scatter(abf.sweepX[inds],abf.sweepY[inds],color='red',zorder=2)\n",
        "            axs.plot(abf.sweepX ,abf.sweepY,zorder=1)\n",
        "            plt.show()\n",
        "        # calc multi sweep params\n",
        "        stim_level = np.median(abf.sweepC[is_stim])\n",
        "        stim_currents.append(stim_level)\n",
        "        spike_rates.append(mean_spike_rate)\n",
        "        spike_counts.append(len(inds))\n",
        "        is_prestim = np.equal(np.cumsum( np.diff(is_base,prepend=1)),0)\n",
        "        v_before_stim.append( np.mean(abf.sweepY[is_prestim] ))\n",
        "        fire_dur.append(rel_firing_duration)\n",
        "\n",
        "        if len(inds)>0:\n",
        "            v_before_spike1.append(abf.sweepY[inds[0]])\n",
        "        else:\n",
        "            v_before_spike1.append(np.nan)\n",
        "\n",
        "    return np.array(stim_currents), np.array(spike_counts), np.array(spike_rates), np.array(v_before_spike1), np.array(v_before_stim) , np.array(fire_dur)\n",
        "\n",
        "\n",
        "def find_spike_in_trace(trace,rate,spike_args,refract=0.005,is_stim = None ,mode='count',sanity_check=True,to_plot=0):\n",
        "    'Takes in a voltage trace from current clamp mode and uses derivative (dVds) to find action potentials.'\n",
        "    'Returns the dVds trace, boolean array indicating if dVds>threshold, inicies where dV crossed threshold,'\n",
        "    'and the mean firing rate given # spikes in trace of given length. Optional ways to count are:'\n",
        "    'isi (1/interspike interval) or count (spike count per second). Default is count'\n",
        "\n",
        "    high_dv_thresh = spike_args['high_dv_thresh']\n",
        "    low_dv_thresh = spike_args['low_dv_thresh']\n",
        "    spike_thresh = spike_args['spike_thresh']\n",
        "    window_ms = spike_args['window_ms']\n",
        "\n",
        "    if any(is_stim == None):\n",
        "        is_stim = [True for i in trace]\n",
        "    dVds = np.diff(trace, prepend=trace[0])*rate/1000\n",
        "    over_thresh = dVds>spike_thresh\n",
        "    over_thresh[np.logical_not(is_stim)] = False\n",
        "    refract_window = int(np.round((refract*rate)))\n",
        "    inds = [t for t in np.arange(refract_window,len(over_thresh)) if all([over_thresh[t], all(over_thresh[t-refract_window:t]==False)])]    \n",
        "    if sanity_check:\n",
        "        old_inds = inds\n",
        "        inds = []\n",
        "        for i in old_inds:\n",
        "            samp_window = window_ms/1000 * rate\n",
        "            ind_range = np.arange(i-samp_window,i+samp_window).astype(int)\n",
        "            nearby_dVds = dVds[ind_range]\n",
        "            if False: print(i,'max', np.max(nearby_dVds))\n",
        "            if False: print(i,'min', np.min(nearby_dVds))\n",
        "            if np.max(nearby_dVds)>high_dv_thresh and np.min(nearby_dVds) < low_dv_thresh:\n",
        "                inds.append(i)\n",
        "                if False: print(inds)\n",
        "    if to_plot>2:\n",
        "        fig1, axs1 = plt.subplots(1,figsize = [9,2])\n",
        "        axs1.plot(np.arange(len(dVds))/rate,dVds,zorder=1)\n",
        "        axs1.scatter((np.arange(len(dVds))/rate)[inds],dVds[inds],color='red',zorder=2)\n",
        "        plt.show()\n",
        "    if len(inds)<1:\n",
        "        mean_spike_rate = 0\n",
        "    else:\n",
        "        if mode=='isi':\n",
        "            mean_spike_rate = np.mean(rate/np.diff(inds))\n",
        "        elif mode=='count':\n",
        "            mean_spike_rate = len(inds)/(np.sum(is_stim)/rate)\n",
        "        else:\n",
        "            print('invalid mode. using default (count)')\n",
        "    return dVds, over_thresh, inds, mean_spike_rate\n",
        "\n",
        "def fit_firing_gain(stim_currents, spike_counts, spike_rates, to_plot=False):\n",
        "    'Gathers the firing rate of each stimuli and fits the linear portion of the curve to return the Gain in Hz/pA (the slope)'\n",
        "    is_pos_slope = np.diff(spike_counts,prepend=0)>0\n",
        "    is_pos_slope = movmean(np.diff(spike_counts,prepend=0),4)>0\n",
        "    peak_ind = np.where(spike_counts==np.max(spike_counts))[0]\n",
        "    if len(peak_ind)>1:\n",
        "        peak_ind = np.min(peak_ind)\n",
        "    \n",
        "    before_peak = np.arange(len(spike_counts))<peak_ind\n",
        "    is_nonzero = spike_counts>0\n",
        "    use_for_fit = np.logical_and.reduce((is_pos_slope,is_nonzero,before_peak))\n",
        "\n",
        "    if_fit = {}\n",
        "    if_fit['stim_currents'] = stim_currents\n",
        "    if_fit['spike_rates'] = spike_rates\n",
        "    if 0 == np.sum(spike_rates):\n",
        "        # print('no spikes detected')\n",
        "        if_fit['slope'] = np.nan\n",
        "        if_fit['intercept'] = np.nan\n",
        "        if_fit['R2'] = 0\n",
        "        return if_fit\n",
        "\n",
        "    if np.sum(spike_rates>0)<3:\n",
        "        # print('not enough spikes generated')\n",
        "        if_fit['slope'] = np.nan\n",
        "        if_fit['intercept'] = np.nan\n",
        "        if_fit['R2'] = 0\n",
        "        return if_fit\n",
        "\n",
        "    if_fit['slope'], if_fit['intercept'] , r_value, p_value, std_err = stats.linregress(stim_currents[use_for_fit], spike_rates[use_for_fit])\n",
        "    if_fit['R2'] = r_value**2\n",
        "\n",
        "    if to_plot:\n",
        "        fig, ax = plt.subplots(1, figsize=[3,3])\n",
        "        ax.scatter( if_fit['stim_currents'] ,if_fit['spike_rates'] )\n",
        "        ax.plot( if_fit['stim_currents'], if_fit['slope']* if_fit['stim_currents']+if_fit['intercept'])\n",
        "        ax.scatter( if_fit['stim_currents'][use_for_fit] ,if_fit['spike_rates'][use_for_fit], color='r' )\n",
        "        ax.scatter(if_fit['stim_currents'][peak_ind],if_fit['spike_rates'][peak_ind], color='m')\n",
        "        ax.set_xlabel('current')\n",
        "        ax.set_ylabel('Spike Rate (Hz)')\n",
        "        (min,max) = ax.get_ylim()\n",
        "        ax.text(0, max/2, 'R**2='+str(round(if_fit['R2'],2)),fontsize='large')\n",
        "        plt.show()\n",
        "    return if_fit\n",
        "\n",
        "def check_inactivation( time, trace, is_stim, sample_rate, dVds, inds, mean_spike_rate, to_plot=0 ):\n",
        "    time_ms = time*1000\n",
        "    sum_isi = np.nan\n",
        "    rel_firing_duration = np.nan\n",
        "    if len(inds)>1:\n",
        "        isi = np.diff(time_ms[inds])\n",
        "        sum_isi = np.sum(isi)\n",
        "        stim_time = time_ms[np.where(is_stim)[0][0]]\n",
        "        first_time = time_ms[inds[0]]-stim_time\n",
        "        firing_duration = first_time+sum_isi\n",
        "        rel_firing_duration = firing_duration /(np.max(time[is_stim]*1000)-stim_time)\n",
        "    return rel_firing_duration\n",
        "\n",
        "\n",
        "\n",
        "# file_name = '2022-08-08_hipp_data/2022x08x04_NEL2_E4KI_F_P243_s001_c003_DGxNEG_0004.abf'\n",
        "# R2_thresh = 0\n",
        "# to_plot = 1\n",
        "\n",
        "# gain_slope, R2, stim_currents, spike_counts, v_before_stim, fire_dur = analyze_gain_abf(file_name,spike_args,R2_thresh,to_plot) \n",
        "# print(np.nanmedian(fire_dur))"
      ],
      "metadata": {
        "id": "dZQqWaElmaE8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########## Rheobase analysis ################\n",
        "def Rheo_curve_analysis(abf_recordings_df, protocol_aliases,to_plot = False,single_spike=True,verbose=False):\n",
        "    'Loop through abfs to calculate Rheobase. For each abf the first sweep with '\n",
        "    'a spike that occurs after a sweep without is chosen, and its stimulus amplitude'\n",
        "    'is written to the dataframe'\n",
        "    # return dataframe with updated min stim intensity\n",
        "    abf_recordings_df['Rheobase_(pA)'] = np.nan\n",
        "    abf_recordings_df['step_resolution_(pA)']  = np.nan\n",
        "    abf_recordings_df['Rheo Ihold_(pA)']  = np.nan\n",
        "    abf_recordings_df['Vhold_Rheo_(mV)']  = np.nan\n",
        "\n",
        "\n",
        "    spike_args = {'spike_thresh':10,\n",
        "                    'high_dv_thresh': 25,\n",
        "                    'low_dv_thresh': -5,\n",
        "                     'window_ms': 2}\n",
        "\n",
        "\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol]):\n",
        "        if verbose: print(file_name)\n",
        "        rheo, step_resolution, offset, Vhold_spike, ap_thresh = analyze_rheo(file_name,spike_args,to_plot=to_plot,verbose=verbose,single_spike=single_spike)\n",
        "\n",
        "        abf_recordings_df.at[file_name,'Rheobase_(pA)'] = rheo\n",
        "        abf_recordings_df.at[file_name,'step_resolution_(pA)'] = step_resolution\n",
        "        abf_recordings_df.at[file_name,'Rheo Ihold_(pA)'] = offset\n",
        "        abf_recordings_df.at[file_name,'Vhold_Rheo_(mV)'] = Vhold_spike\n",
        "        abf_recordings_df.at[file_name,'AP_Threshold(mV)'] = ap_thresh\n",
        "\n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "\n",
        "def analyze_rheo(file_name,spike_args,to_plot=False,verbose=False,single_spike=True):\n",
        "    abf = abf_or_name(file_name)\n",
        "    if to_plot: plot_sweeps_and_command(abf)\n",
        "    if len(abf.sweepList)<2:\n",
        "        return np.nan,np.nan,np.nan,np.nan,np.nan  \n",
        "    else:\n",
        "        is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "        stim_currents, spike_counts, spike_rates, V_before,_,_ = spikes_per_stim(abf, spike_args,thresh=20,to_plot=to_plot)\n",
        "        if verbose: print(spike_counts)\n",
        "        single_spikes = spike_counts==1\n",
        "        zero_spikes = spike_counts==0\n",
        "        if single_spike:\n",
        "            none_to_one = np.full(single_spikes.shape, False)\n",
        "            none_to_one[1:] = np.logical_and(single_spikes[1:], zero_spikes[:-1])\n",
        "            first_spike_stim = np.where(none_to_one)[0]\n",
        "        else:\n",
        "            some_spikes = spike_counts>0\n",
        "            none_to_some = np.full(single_spikes.shape, False)\n",
        "            none_to_some[1:] = np.logical_and(some_spikes[1:], zero_spikes[:-1])\n",
        "            first_spike_stim = np.where(none_to_some)[0]\n",
        "    if first_spike_stim.size == 0:\n",
        "        return np.nan,np.nan,np.nan,np.nan,np.nan \n",
        "    else:\n",
        "        if first_spike_stim.size >1:\n",
        "            first_spike_stim = np.min(first_spike_stim)\n",
        "        rheo = stim_currents[first_spike_stim]\n",
        "        _, _, _, QC_val_df = Iclamp_QC(file_name)\n",
        "        offset = np.mean(QC_val_df['I_leak'])\n",
        "        Vhold_spike = QC_val_df['V_hold'][first_spike_stim]\n",
        "        ap_thresh = V_before[first_spike_stim]\n",
        "        step_resolution = np.mean(np.diff(stim_currents))\n",
        "\n",
        "    return rheo, step_resolution, offset, Vhold_spike, ap_thresh \n",
        "\n",
        "def spikes_per_stim(abf,spike_args,thresh=20,mode='count', to_plot=0):\n",
        "    'Loops through sweeps of and abf to find spikes'\n",
        "    # init\n",
        "    stim_currents = []\n",
        "    spike_rates = []\n",
        "    spike_counts = []\n",
        "    v_before_spike1 = []\n",
        "    v_before_stim = []\n",
        "    fire_dur = []\n",
        "    # get sweep info\n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "\n",
        "    # get spike per sweep\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s)\n",
        "        dVds, over_thresh, inds, mean_spike_rate = find_spike_in_trace(abf.sweepY,abf.sampleRate,spike_args,is_stim=is_stim,mode='count',to_plot=to_plot)\n",
        "        rel_firing_duration = check_inactivation( abf.sweepX, abf.sweepY, is_stim, abf.sampleRate, dVds, inds, mean_spike_rate, to_plot=0 )\n",
        "        # plot id'd spikes\n",
        "        if to_plot>1:\n",
        "            fig, axs = plt.subplots(1)\n",
        "            axs.scatter(abf.sweepX[inds],abf.sweepY[inds],color='red',zorder=2)\n",
        "            axs.plot(abf.sweepX ,abf.sweepY,zorder=1)\n",
        "            plt.show()\n",
        "        # calc multi sweep params\n",
        "        stim_level = np.median(abf.sweepC[is_stim])\n",
        "        stim_currents.append(stim_level)\n",
        "        spike_rates.append(mean_spike_rate)\n",
        "        spike_counts.append(len(inds))\n",
        "        is_prestim = np.equal(np.cumsum( np.diff(is_base,prepend=1)),0)\n",
        "        v_before_stim.append( np.mean(abf.sweepY[is_prestim] ))\n",
        "        fire_dur.append(rel_firing_duration)\n",
        "\n",
        "        if len(inds)>0:\n",
        "            v_before_spike1.append(abf.sweepY[inds[0]])\n",
        "        else:\n",
        "            v_before_spike1.append(np.nan)\n",
        "\n",
        "    return np.array(stim_currents), np.array(spike_counts), np.array(spike_rates), np.array(v_before_spike1), np.array(v_before_stim) , np.array(fire_dur)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_spike_in_trace(trace,rate,spike_args,refract=0.005,is_stim = None ,mode='count',sanity_check=True,to_plot=0):\n",
        "    'Takes in a voltage trace from current clamp mode and uses derivative (dVds) to find action potentials.'\n",
        "    'Returns the dVds trace, boolean array indicating if dVds>threshold, inicies where dV crossed threshold,'\n",
        "    'and the mean firing rate given # spikes in trace of given length. Optional ways to count are:'\n",
        "    'isi (1/interspike interval) or count (spike count per second). Default is count'\n",
        "\n",
        "    high_dv_thresh = spike_args['high_dv_thresh']\n",
        "    low_dv_thresh = spike_args['low_dv_thresh']\n",
        "    spike_thresh = spike_args['spike_thresh']\n",
        "    window_ms = spike_args['window_ms']\n",
        "\n",
        "    if any(is_stim == None):\n",
        "        is_stim = [True for i in trace]\n",
        "    dVds = np.diff(trace, prepend=trace[0])*rate/1000\n",
        "    over_thresh = dVds>spike_thresh\n",
        "    over_thresh[np.logical_not(is_stim)] = False\n",
        "    refract_window = int(np.round((refract*rate)))\n",
        "    inds = [t for t in np.arange(refract_window,len(over_thresh)) if all([over_thresh[t], all(over_thresh[t-refract_window:t]==False)])]    \n",
        "    if sanity_check:\n",
        "        old_inds = inds\n",
        "        inds = []\n",
        "        for i in old_inds:\n",
        "            samp_window = window_ms/1000 * rate\n",
        "            ind_range = np.arange(i-samp_window,i+samp_window).astype(int)\n",
        "            nearby_dVds = dVds[ind_range]\n",
        "            if False: print(i,'max', np.max(nearby_dVds))\n",
        "            if False: print(i,'min', np.min(nearby_dVds))\n",
        "            if np.max(nearby_dVds)>high_dv_thresh and np.min(nearby_dVds) < low_dv_thresh:\n",
        "                inds.append(i)\n",
        "                if False: print(inds)\n",
        "    if to_plot>2:\n",
        "        fig1, axs1 = plt.subplots(1,figsize = [9,2])\n",
        "        axs1.plot(np.arange(len(dVds))/rate,dVds,zorder=1)\n",
        "        axs1.scatter((np.arange(len(dVds))/rate)[inds],dVds[inds],color='red',zorder=2)\n",
        "        plt.show()\n",
        "    if len(inds)<1:\n",
        "        mean_spike_rate = 0\n",
        "    else:\n",
        "        if mode=='isi':\n",
        "            mean_spike_rate = np.mean(rate/np.diff(inds))\n",
        "        elif mode=='count':\n",
        "            mean_spike_rate = len(inds)/(np.sum(is_stim)/rate)\n",
        "        else:\n",
        "            print('invalid mode. using default (count)')\n",
        "    return dVds, over_thresh, inds, mean_spike_rate"
      ],
      "metadata": {
        "id": "WRo06YBZG3Ze"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################## Analyze R-Input In Current Clamp ##################################\n",
        "\n",
        "def inputR_analysis(abf_recordings_df, protocol_aliases,to_plot=False, verbose = False):\n",
        "    'Loop through abfs to calculate the input resistance in Current Clamp'\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol] ):\n",
        "        abf = pyabf.ABF( file_name )\n",
        "        if len(abf.sweepList)<2:\n",
        "            # print(list(abf_recordings_df.index).index(file_name),    'not enough sweeps')\n",
        "            abf_recordings_df.at[file_name,'Rinput_(MO)'] = np.nan\n",
        "            abf_recordings_df.at[file_name,'Cmf_IC_(pF)'] = np.nan\n",
        "            continue\n",
        "            \n",
        "        inputR_fit = input_res_curve(abf,to_plot=to_plot)\n",
        "        abf_recordings_df.at[file_name,'Rinput_(MO)'] = inputR_fit['slope']*1000\n",
        "        if len(abf_recordings_df.at[file_name,'passing_sweeps'])==0:\n",
        "            abf_recordings_df.at[file_name,'Cmf_IC_(pF)'] = np.nan\n",
        "            continue\n",
        "        Cmf_IC = IC_sweep_capacitance_mean(abf,abf_recordings_df.at[file_name,'passing_sweeps'],to_plot=to_plot,verbose=verbose)\n",
        "        abf_recordings_df.at[file_name,'Cmf_IC_(pF)'] = Cmf_IC\n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "def input_res_curve(abf,to_plot=False):\n",
        "    'Calulates the series of delta Vs and delta Is and fits with a line to find the resistance.'\n",
        "    from scipy.signal import butter,filtfilt\n",
        "    stim_currents = []\n",
        "    ss_voltage = []\n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s)\n",
        "        delta_v, _, _ = sweep_VIR(abf.sweepY, abf.sampleRate, is_stim = is_stim)\n",
        "        delta_I, _, _    = sweep_VIR(abf.sweepC, abf.sampleRate, is_stim = is_stim) # repurpose but for command current\n",
        "        stim_currents.append( delta_I)\n",
        "        ss_voltage.append(delta_v)\n",
        "    \n",
        "    inputR_fit = {}\n",
        "    inputR_fit['slope'], inputR_fit['intercept'] , r_value, p_value, std_err = stats.linregress(stim_currents, ss_voltage)\n",
        "    inputR_fit['R2'] = r_value**2\n",
        "    if to_plot:\n",
        "        plt.scatter(stim_currents, ss_voltage)\n",
        "        matplotlib.pyplot.text(10, 0, 'Ri = ' + str(round(inputR_fit['slope']*1000)) + 'MO')\n",
        "    return inputR_fit        \n",
        "\n",
        "def sweep_VIR(trace,rate,is_stim = None, window_t=0.100):\n",
        "    'Takes a trace snd calulates the steady state delta V from'\n",
        "    'a stimulus in Current Clamp'\n",
        "    if any(is_stim == None):\n",
        "        is_stim = [True for i in trace]\n",
        "    base_v = trace[:np.where(is_stim==True)[0][0]]\n",
        "    cutoff = 5\n",
        "    nyq = rate/2\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(3, normal_cutoff, btype='low')\n",
        "    filtered_step_v = filtfilt(b, a, trace[is_stim])\n",
        "    window_wid = int(window_t*rate)\n",
        "    med_base_v = np.median(base_v[-window_wid:-1])\n",
        "    med_stim_v = np.median(filtered_step_v[-window_wid:-1])\n",
        "    delta_v = med_stim_v - med_base_v\n",
        "    return delta_v, med_base_v, med_stim_v\n",
        "\n",
        "def binary_exp(time, amp_1, amp_2, tau_1, tau_2, ss):\n",
        "    'A double exponential decay function'\n",
        "    return (amp_1 * np.exp(-time /tau_1)) + (amp_2 * np.exp(-time / tau_2))  + ss\n",
        "\n",
        "\n",
        "def IC_sweep_capacitance_mean(abf,passing_sweeps,to_plot=False, verbose = False):\n",
        "    'Takes an abf and calulates the membrane capacitance using'\n",
        "    'methods described in https://journals.physiology.org/doi/epdf/10.1152/jn.00160.2009'\n",
        "    \n",
        "    \n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "    rVm_list = []\n",
        "    V0_list = []\n",
        "    SS_list = []\n",
        "    Stim_list = []\n",
        "    time = abf.sweepX[is_stim]\n",
        "    time = time - time[0]\n",
        "    # for s in abf.sweepList:\n",
        "    for s in passing_sweeps:\n",
        "        abf.setSweep(s)\n",
        "        Istim = abf.sweepC[is_stim]\n",
        "        if not( np.mean(Istim) < 0 ):\n",
        "            continue\n",
        "        Vm = abf.sweepY[is_stim]\n",
        "        SS = np.percentile(Vm,.0001)\n",
        "        V0 = Vm[0]\n",
        "        relative_Vm = (Vm - SS) / (V0-SS)\n",
        "        rVm_list.append(relative_Vm)\n",
        "        V0_list.append(V0)\n",
        "        SS_list.append(SS)\n",
        "        Stim_list.append(np.median(Istim))\n",
        "    if len(V0_list)<1:\n",
        "        'likely there are passing sweeps but they are positive current'\n",
        "        return np.nan\n",
        "    \n",
        "    rVm_array = np.stack(rVm_list).T\n",
        "    V0_array = np.array(V0_list)\n",
        "    SS_array = np.array(SS_list)\n",
        "    Stim_array = np.array(Stim_list)\n",
        "    mean_rVm = np.mean(rVm_array,axis=-1)\n",
        "    if to_plot:\n",
        "        plt.plot(time,rVm_array,'gray')\n",
        "        plt.plot(time,mean_rVm,'k')\n",
        "\n",
        "\n",
        "    p0 = ( 0.1, 0.8, 0.0001,  0.01, 0)\n",
        "    bounds=([0,.3,0,.01,-.2], [0.25,1.2,.01,2,.2])\n",
        "\n",
        "    try:\n",
        "        fit_params, cv = scipy.optimize.curve_fit(binary_exp, time, mean_rVm, p0, bounds=bounds) #\n",
        "        amp_1, amp_2, tau_1, tau_2, ss = fit_params\n",
        "        Vm_hat = binary_exp(time, amp_1, amp_2, tau_1, tau_2, ss)\n",
        "\n",
        "        Vm_hat1 = binary_exp(time, amp_1, 0, tau_1, tau_2, ss) + amp_2\n",
        "        Vm_hat2 = binary_exp(time, 0, amp_2,  tau_1, tau_2, ss) + amp_1\n",
        "\n",
        "        t_slow = tau_2*np.ones_like(V0_array)\n",
        "        dv_slow = (SS_array - V0_array) * amp_2 *1e-3\n",
        "        i_stim = Stim_array*1e-12\n",
        "        r_slow = dv_slow/i_stim\n",
        "        c_slow = t_slow /r_slow\n",
        "        Cmf_IC = c_slow*1e12\n",
        "    except:\n",
        "        print(V0_array)\n",
        "        print(Stim_array)\n",
        "        print(SS_array)\n",
        "        return np.nan \n",
        "\n",
        "    if verbose:\n",
        "        print('V0', V0_array)\n",
        "        print('I', i_stim*1e12)\n",
        "        print('dV', dv_slow*1e3)\n",
        "        print('SSv', SS_array)\n",
        "        print('Rm', r_slow*1e-6)\n",
        "        print('Cm', Cmf_IC)\n",
        "        print('amp_1, amp_2, tau_1, tau_2, ss')\n",
        "        print('Fit', fit_params)\n",
        "\n",
        "    if to_plot:\n",
        "        plt.plot(time,Vm_hat,'r')\n",
        "        plt.plot(time,Vm_hat1,'c')\n",
        "        plt.plot(time,Vm_hat2,'m')\n",
        "        plt.show()\n",
        "\n",
        "    return np.mean(Cmf_IC)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "TiU-YFY-G0kq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################  Quality Control Filtering ##################################\n",
        "def QC_full_dataset(abf_recordings_df,to_plot=False,verbose=False,VC_prot=[],IC_prot=[],MT_prot=[]):\n",
        "    'Loop through all abfs to look for signs of a poor recording including leak current, unstable holding V/I,'\n",
        "    'high noise, and in appropriate holidng potential'\n",
        "\n",
        "    # abf_recordings_df['QC_checks'] = None\n",
        "    # abf_recordings_df['QC_values'] = None\n",
        "    abf_recordings_df['passing_sweeps'] = None\n",
        "\n",
        "    print('Voltage Clamp Protocols')\n",
        "    VC_idx = [p in VC_prot for p in abf_recordings_df['protocol']]\n",
        "    for fn in tqdm( abf_recordings_df[VC_idx].index ):\n",
        "        if verbose: print(np.where(fn==abf_recordings_df.index)[0])\n",
        "        try:\n",
        "            pass_rate, passing_sweeps, QC_check_df, QC_val_df = Vclamp_QC(fn,to_plot=to_plot,verbose=verbose)\n",
        "            abf_recordings_df.at[fn,'passing_sweeps'] = np.array(passing_sweeps)\n",
        "        except AssertionError:\n",
        "            if verbose: print_assert()\n",
        "            abf_recordings_df.at[fn,'passing_sweeps'] = []\n",
        "\n",
        "    print('Current Clamp Protocols')\n",
        "    IC_idx = [p in IC_prot for p in abf_recordings_df['protocol']]\n",
        "    for fn in tqdm( abf_recordings_df[IC_idx].index ):\n",
        "        if verbose: print(np.where(fn==abf_recordings_df.index)[0])\n",
        "        try:\n",
        "            pass_rate, passing_sweeps, QC_check_df, QC_val_df = Iclamp_QC(fn,to_plot=to_plot,verbose=verbose)\n",
        "            abf_recordings_df.at[fn,'passing_sweeps'] = np.array(passing_sweeps)\n",
        "        except AssertionError:\n",
        "            if verbose: print_assert()\n",
        "            abf_recordings_df.at[fn,'passing_sweeps'] = []\n",
        "       \n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "def Vclamp_QC(file_name, max_leak=200,max_high_freq_noise = 10, max_low_freq_noise = 15,\n",
        "              Vhold_range = 8, to_plot=False,verbose=False):\n",
        "    'Look for signs of a poor recording for Voltage Clamp recordings'\n",
        "    is_VC = np.nan\n",
        "    is_Ic = np.nan\n",
        "    abf = pyabf.ABF( file_name )\n",
        "    if 'mV' in abf.sweepLabelY:\n",
        "        is_IC = True\n",
        "        is_VC = False\n",
        "    if 'pA' in abf.sweepLabelY:\n",
        "        is_IC = False\n",
        "        is_VC = True\n",
        "\n",
        "\n",
        "    try:\n",
        "        assert is_VC==True, 'Wrong clamp mode for protocol. Voltage protocol used during current clamp!'\n",
        "    except AssertionError as e:\n",
        "        print(e)\n",
        "        return 0, [], [], []\n",
        "\n",
        "\n",
        "\n",
        "    # assert is_VC==True, 'Wrong clamp mode for protocol. Voltage protocol used during current clamp!'\n",
        "    \n",
        "\n",
        "    theta, command_offset, correct_ch1 =  predict_telegraph(abf)\n",
        "\n",
        "\n",
        "    QC_check_list = []\n",
        "    QC_val_list = []\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s,0)\n",
        "        QC_checks, QC_values, windows = qc_sweep(abf.sweepX,abf.sweepC,abf.sweepY,command_offset,is_IC,is_VC,abf.sampleRate,                                                 \n",
        "                                                 max_leak=max_leak,\n",
        "                                                 max_high_freq_noise = max_high_freq_noise,\n",
        "                                                 max_low_freq_noise = max_low_freq_noise,\n",
        "                                                 Vhold_range = Vhold_range, to_plot=False)\n",
        "        QC_check_list.append(QC_checks)\n",
        "        QC_val_list.append(QC_values)\n",
        "    \n",
        "    QC_check_df = pd.DataFrame({'sweep' : np.arange(len(QC_val_list))}).set_index('sweep')\n",
        "    for i in np.arange(len(QC_check_list)):\n",
        "        d = QC_check_list[i]\n",
        "        for (k,v) in d.items():\n",
        "            QC_check_df.at[i,k]=v\n",
        "\n",
        "    QC_val_df = pd.DataFrame({'sweep' : np.arange(len(QC_val_list))}).set_index('sweep')\n",
        "    for i in np.arange(len(QC_val_list)):\n",
        "        d = QC_val_list[i]\n",
        "        for (k,v) in d.items():\n",
        "            QC_val_df.at[i,k]=v\n",
        "\n",
        "    pass_rate = {}\n",
        "    for c in QC_check_df.columns:\n",
        "        pass_rate[c] = np.round(np.mean(np.array(QC_check_df[c].values).astype(int))*100,2)\n",
        "\n",
        "    if to_plot:\n",
        "        fig, ax, theta = plot_sweeps_and_command(abf,windows=windows)\n",
        "        plt.show()\n",
        "\n",
        "    passing_sweeps = [s for s in QC_check_df.index if all(QC_check_df.loc[s,:])]\n",
        "    if verbose: print('\\n','pass_rate:',pass_rate)\n",
        "    if verbose: print('passing_sweeps:',passing_sweeps)\n",
        "    return pass_rate, passing_sweeps, QC_check_df, QC_val_df\n",
        "\n",
        "def Iclamp_QC(file_name, max_leak=250, to_plot=False,verbose=False):\n",
        "    'Look for signs of a poor recording for Current Clamp recordings'\n",
        "    abf = abf_or_name(file_name)\n",
        "    # abf = pyabf.ABF( file_name )\n",
        "    if 'mV' in abf.sweepLabelY:\n",
        "        is_IC = True\n",
        "        is_VC = False\n",
        "    if 'pA' in abf.sweepLabelY:\n",
        "        is_IC = False\n",
        "        is_VC = True\n",
        "    try:\n",
        "        assert is_IC==True, 'Wrong clamp mode for protocol. IC protocol used during voltage clamp!'\n",
        "    except AssertionError as e:\n",
        "        print(e)\n",
        "\n",
        "    # assert is_IC==True, 'Wrong clamp mode for protocol. IC protocol used during voltage clamp!'\n",
        "\n",
        "    theta, command_offset, correct_ch1 =  predict_telegraph(abf)\n",
        "    \n",
        "    QC_check_list = []\n",
        "    QC_val_list = []\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s,0)\n",
        "        QC_checks, QC_values, windows = qc_sweep(abf.sweepX,abf.sweepC,abf.sweepY,command_offset,is_IC,is_VC,\n",
        "                                                 abf.sampleRate,to_plot=to_plot, max_high_freq_noise = .05, max_low_freq_noise = 0.4, Vhold_range=3)\n",
        "        QC_check_list.append(QC_checks)\n",
        "        QC_val_list.append(QC_values)\n",
        "    \n",
        "    QC_check_df = pd.DataFrame({'sweep' : np.arange(len(QC_val_list))}).set_index('sweep')\n",
        "    for i in np.arange(len(QC_check_list)):\n",
        "        d = QC_check_list[i]\n",
        "        for (k,v) in d.items():\n",
        "            QC_check_df.at[i,k]=v\n",
        "\n",
        "    QC_val_df = pd.DataFrame({'sweep' : np.arange(len(QC_val_list))}).set_index('sweep')\n",
        "    for i in np.arange(len(QC_val_list)):\n",
        "        d = QC_val_list[i]\n",
        "        for (k,v) in d.items():\n",
        "            QC_val_df.at[i,k]=v\n",
        "\n",
        "    pass_rate = {}\n",
        "    mean_values = {}\n",
        "    for c in QC_check_df.columns:\n",
        "        pass_rate[c] = np.round(np.mean(np.array(QC_check_df[c].values).astype(int))*100,2)\n",
        "        mean_values[c] = np.round(np.mean(np.array(QC_val_df[c].values)),3)\n",
        "    passing_sweeps = [s for s in QC_check_df.index if all(QC_check_df.loc[s,:])]\n",
        "    \n",
        "    if verbose==True: \n",
        "        print('')\n",
        "        print('pass_rate:',pass_rate)\n",
        "        print('mean_values:',mean_values)\n",
        "    if verbose==2:\n",
        "        print('\\n','pass_rate:',pass_rate)\n",
        "        print('passing_sweeps:',passing_sweeps, str(len(passing_sweeps)/len(QC_check_df)*100)+'%')\n",
        "        print(QC_val_df)\n",
        "\n",
        "    if to_plot:\n",
        "        fig, ax, theta = plot_sweeps_and_command(abf,windows=windows)\n",
        "        plt.show()\n",
        "\n",
        "    return [pass_rate,passing_sweeps,QC_check_df,QC_val_df] # return pass_rate, passing_sweeps, QC_check_df, QC_val_df\n",
        "\n",
        "def qc_sweep(sweepX,sweepC,sweepY,command_offset,is_IC,is_VC,sampleRate,\n",
        "             max_leak=100, \n",
        "             max_high_freq_noise = 10,\n",
        "             max_low_freq_noise = 10,\n",
        "             Vhold_range = 5, to_plot=False):\n",
        "    'Recieves a sweep and and calculates leak, noise and holding potential'\n",
        "    'returns a dict of calculated values and a dict of boolean indicating'\n",
        "    'pass/fail, (True/False)'\n",
        "\n",
        "\n",
        "    stim_buffer_time = 250 #ms\n",
        "    filtered_command = movmean((sweepC==sweepC[0])*1, stim_buffer_time/1000*sampleRate)\n",
        "    ss_no_stim_bool = filtered_command==1\n",
        "    ss_no_stim_idx = np.arange(len(ss_no_stim_bool))[ss_no_stim_bool]\n",
        "    no_stim_sig = sweepY[ss_no_stim_bool]\n",
        "    no_stim_t = sweepX[ss_no_stim_idx]\n",
        "    baseline = np.mean( no_stim_sig )\n",
        "\n",
        "    QC_checks = {}\n",
        "    QC_values = {}\n",
        "    if is_IC:\n",
        "        QC_values['V_hold'] = baseline\n",
        "        QC_values['I_leak'] = command_offset\n",
        "    if is_VC:\n",
        "        QC_values['V_hold'] = command_offset\n",
        "        QC_values['I_leak'] = baseline\n",
        "    \n",
        "    QC_checks['V_hold'] = abs(QC_values['V_hold'] - -70)< Vhold_range\n",
        "    QC_checks['I_leak'] = QC_values['I_leak']<max_leak\n",
        "\n",
        "\n",
        "    \n",
        "    HF_noise_idx = ss_no_stim_idx[:int(0.0015*sampleRate)]\n",
        "    HF_noise_signal = sweepY[ HF_noise_idx ] \n",
        "    HF_noise = rms_noise(HF_noise_signal)     \n",
        "    \n",
        "    QC_checks['HF_noise'] = HF_noise<max_high_freq_noise\n",
        "    QC_values['HF_noise'] = HF_noise\n",
        "    \n",
        "    LF_noise_idx = ss_no_stim_idx[-int(0.1*sampleRate):] \n",
        "    if int(0.1*sampleRate)>len(LF_noise_idx): LF_noise_idx = np.random.choice(LF_noise_idx, size=int(0.1*sampleRate))\n",
        "    LF_noise_signal = sweepY[ LF_noise_idx ] \n",
        "    LF_noise = rms_noise(LF_noise_signal)\n",
        "    QC_checks['LF_noise'] = LF_noise<max_low_freq_noise\n",
        "    QC_values['LF_noise'] = LF_noise\n",
        "\n",
        "    HF_noise_time = sweepX[HF_noise_idx]\n",
        "    LF_noise_time = sweepX[LF_noise_idx]\n",
        "    LF_noise_time = np.sort(np.array(list(set(LF_noise_time))))\n",
        "\n",
        "    return QC_checks, QC_values, [HF_noise_time, LF_noise_time]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x46MR5nvGrV4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################## Membrane Resistance & Capacitance Testing ##################################\n",
        "def Icapacitance_analysis(abf_recordings_df, protocol_aliases, to_plot=False,verbose=False,report_params=True):\n",
        "    'Loops through abfs with Vclamp pulstrains to calculate membrane properties: Ra, Rm, Cm'\n",
        "    if report_params:\n",
        "        report_params = ['Ra', 'Rm', 'Cm', 'tau',\t'Cmq',\t'Cmf',\t'Cmqf', 'Cm_pc']\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    print(len(correct_protocol),'files to analyze...')\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol] ) : \n",
        "        abf = pyabf.ABF( file_name )\n",
        "        # passing_sweeps=abf_recordings_df.at[file_name,'passing_sweeps']\n",
        "\n",
        "        try:\n",
        "            pass_rate, passing_sweeps, QC_check_df, QC_val_df = Vclamp_QC(file_name,to_plot=False,verbose=verbose)\n",
        "            passing_sweeps = [s for s in QC_check_df.index if all(QC_check_df.loc[s,['I_leak','HF_noise','LF_noise']])] # ignore Vhold Filters\n",
        "            mem_params_df = fit_Icapacitave_mean_current(abf,to_plot=to_plot,verbose=verbose,passing_sweeps=passing_sweeps)\n",
        "            pclamp_mem_params_df = pclamp_mem_test(abf,to_plot = to_plot, verbose = verbose)\n",
        "            mem_params_df = mem_params_df.join(pclamp_mem_params_df,how='outer')\n",
        "\n",
        "\n",
        "        except:\n",
        "            _=print_assert()\n",
        "            mem_params_df = pd.DataFrame()\n",
        "        for c in mem_params_df.columns:\n",
        "            for d in mem_params_df.index:\n",
        "                if c in report_params:\n",
        "                    abf_recordings_df.at[file_name, c+'_'+str(d)] = mem_params_df.at[d,c]\n",
        "    return abf_recordings_df, correct_protocol\n",
        "       \n",
        "\n",
        "def fit_Icapacitave_mean_current(abf, to_plot=False, verbose=False, passing_sweeps = []):\n",
        "    'Takes in an abf file and finds all pulses. Pulses with matching duration are averaged together.'\n",
        "    'For each pulse duration the mean pulse is fit using the methods described at https://swharden.com/blog/2020-10-11-model-neuron-ltspice/ '\n",
        "    'For each pulse length returns, Ra, Rm, and three Cm measures (Cmf, Cmq, Cmqf).'\n",
        "    'Respectively these are capacitance determined by: fitting tau and computing,'\n",
        "    'calculating the area under the capcitave transient, and calculating the area'\n",
        "    'under the fit line.'\n",
        "\n",
        "    command = abf.sweepC\n",
        "    # trace,time,command,rate,\n",
        "\n",
        "    base_v = command[0]\n",
        "    step_v = np.median( command[np.logical_not(command==base_v)])\n",
        "    is_base = command==base_v\n",
        "    is_step = command==step_v\n",
        "\n",
        "    delta_V = abs(step_v-base_v)\n",
        "\n",
        "    step_start = np.logical_and(is_base[:-1], is_step[1:])\n",
        "    step_stop = np.logical_and(is_step[:-1], is_base[1:])\n",
        "\n",
        "    starts = np.where(step_start)[0]\n",
        "    stops = np.where(step_stop)[0]\n",
        "\n",
        "    assert len(starts)==len(stops), 'unable to match pulse starts and stops'\n",
        "    assert any(( starts-stops)<0), 'unable to match pulse starts and stops'\n",
        "    assert len(starts)>0, 'no pulse found'\n",
        "    # parse_pulses\n",
        "\n",
        "    if verbose: print('passing_sweeps',passing_sweeps)\n",
        "\n",
        "    params = []\n",
        "    p_len_list = []\n",
        "    Icap_list = []\n",
        "    step_time_list = []\n",
        "    # for s in abf.sweepList:\n",
        "    for s in passing_sweeps:\n",
        "        abf.setSweep(s)\n",
        "        trace = abf.sweepY\n",
        "        sweep_time = abf.sweepX\n",
        "        if (base_v>step_v):\n",
        "            trace = -trace\n",
        "        for p in np.arange(len(starts)):\n",
        "            pulse_start = starts[p]\n",
        "            pulse_stop = stops[p]\n",
        "            pulse_len = stops[p] - starts[p]\n",
        "            p_len_list.append(pulse_len)\n",
        "            pulse_index = np.arange(int(pulse_start-pulse_len*0.05),pulse_stop)\n",
        "\n",
        "            step_times = sweep_time[pulse_index]\n",
        "            step_times = step_times-sweep_time[starts[p]]\n",
        "            step_time_list.append(step_times)\n",
        "\n",
        "            Icap_transient = trace[pulse_index]\n",
        "            Icap_list.append(Icap_transient)\n",
        "\n",
        "    p_len_list = np.array(p_len_list)/abf.sampleRate*1000\n",
        "    pulse_set = np.array(sorted(set(p_len_list)))\n",
        "    mem_params_df = pd.DataFrame(None,index=pulse_set,columns=['>90%','Ib','Iss','Ip','Ra','Rm','tau','Cmq','Cmf','Cmqf'])\n",
        "    \n",
        "    if to_plot:\n",
        "        fig, axs = plt.subplots(1,len(pulse_set),figsize=[12, 3])\n",
        "        fig.suptitle(abf.abfFilePath)\n",
        "        if verbose: print(abf.abfFilePath)\n",
        "        if str(type(axs)) == \"<class 'matplotlib.axes._subplots.AxesSubplot'>\":\n",
        "            axs = [axs]\n",
        "    \n",
        "    \n",
        "    for p in pulse_set:\n",
        "        # pulse_dur =p/abf.sampleRate*1000\n",
        "        matching_traces = [Icap_list[n] for n in np.arange(len(p_len_list)) if p_len_list[n]==p ]\n",
        "        matching_traces = np.stack(matching_traces)\n",
        "\n",
        "        mean_trace = np.mean(matching_traces,axis=0)\n",
        "        mean_time = np.mean(np.stack([step_time_list[n] for n in np.arange(len(p_len_list)) if p_len_list[n]==p ]),axis=0)\n",
        "\n",
        "        sweep_var = abs((matching_traces-mean_trace)/mean_trace)\n",
        "        outlier_percent = round(np.mean(sweep_var>1.645)*100,3)\n",
        "        # base_ind = np.arange(len(mean_time))\n",
        "        base_t = np.mean(mean_time[mean_time<0])\n",
        "        base_I = np.mean(mean_trace[mean_time<0])\n",
        "        \n",
        "        steady_state_t = np.mean(mean_time[mean_time>mean_time[-1]*0.95])\n",
        "        steady_state_I = np.mean(mean_trace[mean_time>mean_time[-1]*0.95])\n",
        "\n",
        "\n",
        "        peak_I = np.max(mean_trace)\n",
        "        peak_t = mean_time[mean_trace==peak_I]\n",
        "        if peak_t.shape[0]>1: peak_t = min(peak_t)\n",
        "        Icap_curve = (mean_trace[mean_time>=peak_t])\n",
        "        Icap_curve_t = mean_time[mean_time>=peak_t]\n",
        "\n",
        "\n",
        "        rel_dif_Icap = movmean(np.diff(Icap_curve,append=Icap_curve[-1]),10)/peak_I\n",
        "        excess_plat_t = Icap_curve_t[rel_dif_Icap>=0]\n",
        "        if len(excess_plat_t)>0:\n",
        "            excess_plat_start = np.min(excess_plat_t)*10\n",
        "            if excess_plat_start >0.005:\n",
        "                Icap_curve = Icap_curve[Icap_curve_t<excess_plat_start]\n",
        "                Icap_curve_t = Icap_curve_t[Icap_curve_t<excess_plat_start]\n",
        "                steady_state_t = np.mean(Icap_curve_t[Icap_curve_t>Icap_curve_t[-1]*0.95])\n",
        "                steady_state_I = np.mean(Icap_curve[Icap_curve_t>Icap_curve_t[-1]*0.95])\n",
        "                # plt.scatter(Icap_curve_t,Icap_curve)\n",
        "                # plt.scatter(excess_plat_t,Icap_curve[rel_dif_Icap>=0])\n",
        "                # plt.gca().set_xscale('log')\n",
        "                # plt.scatter(excess_plat_start,peak_I)\n",
        "                # plt.show()\n",
        "\n",
        "\n",
        "        \n",
        "        delta_I_steady = steady_state_I - base_I\n",
        "        delta_I_peak = peak_I - steady_state_I\n",
        "        # if verbose: print('len(Icap_curve)',len(Icap_curve))\n",
        "        # if verbose: print('delta_V',delta_V)\n",
        "        # if verbose: print('delta_I_peak',delta_I_peak)\n",
        "        Ra = (delta_V*1e-3)/(delta_I_peak*1e-12) *1e-6 #(O/MO)\n",
        "        Rm = ((delta_V*1e-3) - Ra*1e6 * delta_I_steady*1e-12) / (delta_I_steady*1e-12) *1e-6 #(O/MO)\n",
        "        Q = np.sum(Icap_curve-steady_state_I) * (1/abf.sampleRate)\n",
        "        Cmq = Q / delta_V*1000\n",
        "        # if verbose: print('Cmq',Cmq)\n",
        "        \n",
        "\n",
        "        try:\n",
        "            bounds=([peak_I*0.1,.0001,0], [peak_I*1.5,500, steady_state_I*3])\n",
        "            p0 = (peak_I, 0.02 , steady_state_I) # start with values near those we expect\n",
        "            fit_params, cv = scipy.optimize.curve_fit(mono_exp, Icap_curve_t[int(0.0005*abf.sampleRate):], Icap_curve[int(0.0005*abf.sampleRate):], p0, bounds=bounds) #\n",
        "            peak_hat, tau_hat, ss_hat = fit_params\n",
        "            Icap_hat =  mono_exp(Icap_curve_t, peak_hat, tau_hat, ss_hat)\n",
        "            perr = np.sqrt(np.diag(cv))\n",
        "            # if verbose: print('tau_hat',tau_hat)\n",
        "            # if verbose: print('Icap_curve_t',Icap_curve_t)\n",
        "            # if verbose: print('Ra',Ra)\n",
        "            # if verbose: print('Rm',Rm)\n",
        "            Cmf = tau_hat / (1/(1/(Ra*1e6) + 1/(Rm*1e6)))\n",
        "            Cmf = Cmf*1e12\n",
        "            # if verbose: print('Cmf',Cmf)\n",
        "            \n",
        "        except:\n",
        "            Cmf = None\n",
        "            Icap_hat = np.empty_like(Icap_curve_t)\n",
        "            Icap_hat[:] =np.nan\n",
        "            ss_hat = np.nan\n",
        "            tau_hat = np.nan\n",
        "\n",
        "        Cmqf = np.sum(Icap_hat-ss_hat) * (1/abf.sampleRate) / delta_V*1000\n",
        "        # if verbose: print('Cmqf',Cmqf)\n",
        "\n",
        "        mem_params_df.at[p] = [outlier_percent,base_I,steady_state_I,peak_I,Ra,Rm,tau_hat,Cmq,Cmf,Cmqf]\n",
        "        \n",
        "\n",
        "        if to_plot:\n",
        "            i = int(np.where(p==pulse_set)[0][0])\n",
        "            mean_time_0 = -mean_time[0]\n",
        "            axs[i].plot(mean_time_0+mean_time,matching_traces.T,color = (0.8,0.8,0.8))\n",
        "            axs[i].plot(mean_time_0+mean_time,mean_trace,color='k')\n",
        "            axs[i].plot(mean_time_0+Icap_curve_t[[0,-1]],base_I*np.array([1,1]),color='r',linestyle = 'dotted')\n",
        "            axs[i].scatter(mean_time_0+peak_t,peak_I,color='r',zorder=5)\n",
        "            axs[i].plot(mean_time_0+Icap_curve_t[[0,-1]],steady_state_I*np.array([1,1]),color='r',linestyle = 'dotted')\n",
        "            axs[i].plot(mean_time_0+Icap_curve_t[int(0.001*abf.sampleRate):],Icap_curve[int(0.001*abf.sampleRate):],color='m')\n",
        "            axs[i].plot(mean_time_0+Icap_curve_t, Icap_hat,'c',linestyle = 'dashed')\n",
        "            # if verbose: print(steady_state_I)\n",
        "            # axs[i].plot(mean_time_0+Icap_curve_t, np.cumsum(Icap_hat-ss_hat)/3)\n",
        "            # axs[i].set_xscale('log')\n",
        "            axs[i].set_xlim([0,mean_time_0+Icap_curve_t[-1]*1.2]) #(mean_time_0+peak_t)*0.7\n",
        "            axs[i].set_title(str(p)+'ms')\n",
        "            \n",
        "    if verbose: display(mem_params_df)\n",
        "    if to_plot:\n",
        "        plt.tight_layout()\n",
        "        fig.subplots_adjust(top=0.8)\n",
        "        plt.show()          \n",
        "    return mem_params_df\n",
        "\n",
        "def pclamp_mem_test(abf,to_plot = False, verbose =False):\n",
        "    # load file if name given instead of true abf\n",
        "    abf = abf_or_name(abf)\n",
        "    command = abf.sweepC*1e-3\n",
        "    trace = abf.sweepY*1e-12\n",
        "    sweep_time = abf.sweepX\n",
        "\n",
        "    # make all pos\n",
        "    if np.mean(command)<0:\n",
        "        command = -command\n",
        "        trace = -trace\n",
        "        \n",
        "    \n",
        "\n",
        "    # Find step and recovery\n",
        "    base_v = command[0]\n",
        "    # plt.plot(sweep_time,command)\n",
        "    step_v = np.median( command[np.logical_not(command==base_v)])\n",
        "    dvdt = np.diff(command,prepend=command[0])\n",
        "    # plt.plot(sweep_time,dvdt)\n",
        "    up_step = np.where(dvdt==np.max(dvdt))[0]\n",
        "    # print('up_step',up_step)\n",
        "    down_step = np.where(dvdt==np.min(dvdt))[0]\n",
        "    # print('down_step',down_step)\n",
        "    updn_ticks = down_step - up_step\n",
        "    # print('updn_ticks',updn_ticks)\n",
        "\n",
        "    pulse_dur_set = np.sort(list(set(updn_ticks)))\n",
        "    # print('pulse_dur_set',pulse_dur_set)\n",
        "\n",
        "\n",
        "    mem_params_df = pd.DataFrame(None,index=pulse_dur_set/abf.sampleRate*1000,columns=['Tau_pc','Rm_pc','Ra_pc','Cm_pc'])\n",
        "\n",
        "    'For Each Pulse Duration Length'\n",
        "    for p in pulse_dur_set:\n",
        "        'Average up the pulses'\n",
        "        matching_starts = [up_step[i] for i in range(len(up_step)) if updn_ticks[i]==p ]\n",
        "        tick_range = np.arange(p*2)\n",
        "        pulse_indicies_mat = np.add.outer(matching_starts,tick_range)\n",
        "        pulse_trace_set = trace[pulse_indicies_mat]\n",
        "        mean_pulse_trace = np.mean(pulse_trace_set,axis = 0)\n",
        "        mean_pulse_command = np.mean(command[pulse_indicies_mat],axis = 0)\n",
        "        if to_plot:\n",
        "            fig, ax = plt.subplots(1)\n",
        "            ax.plot(pulse_trace_set.transpose(),color='grey')\n",
        "            ax.plot(mean_pulse_trace,color='k')\n",
        "            # plt.show()\n",
        "        'Get pclamp fitting variables'\n",
        "        'Get Is and Vs'\n",
        "        V1 = np.max(mean_pulse_command)\n",
        "        V2 = np.min(mean_pulse_command)\n",
        "        delta_V = V1-V2\n",
        "        I1_index = range(int(p*0.8),p)\n",
        "        I1 = np.mean(mean_pulse_trace[I1_index])\n",
        "\n",
        "        I2_index = I1_index + p\n",
        "        I2 = np.mean(mean_pulse_trace[I2_index])\n",
        "        delta_I = I1-I2\n",
        "\n",
        "        if to_plot:\n",
        "            ax.plot(I1_index,I1*np.ones_like(I1_index),color='magenta',linewidth=3)\n",
        "            ax.plot(I2_index,I2*np.ones_like(I1_index),color='magenta',linewidth=3)\n",
        "            \n",
        "\n",
        "        'Fitting Tau'\n",
        "        def linearized_exp_decay(time,tau,beta):\n",
        "            'Linear form of ln(y) for exponential decay'\n",
        "            return -(1/tau)*(time+ beta) \n",
        "        \n",
        "        peak_I = np.max(mean_pulse_trace)\n",
        "        ind_of_peak = np.where(mean_pulse_trace==peak_I)[0]\n",
        "\n",
        "\n",
        "        single_pulse_trace = mean_pulse_trace[np.arange(ind_of_peak,p)]\n",
        "        single_pulse_time = np.arange(ind_of_peak,p)/abf.sampleRate\n",
        "        fraction_to_fit = [0.20, 0.80]\n",
        "\n",
        "        'LinearFraction'\n",
        "        I_max = np.max(single_pulse_trace)\n",
        "        I_min = np.min(single_pulse_trace)\n",
        "        delta = I_max - I_min\n",
        "        lower = I_min + delta*fraction_to_fit[0]\n",
        "        upper = I_min + delta*fraction_to_fit[1]\n",
        "        trimmed_fit_range = np.logical_and(single_pulse_trace>lower, single_pulse_trace<upper)\n",
        "        trace_to_fit = single_pulse_trace[trimmed_fit_range]\n",
        "        time_to_fit = single_pulse_time[trimmed_fit_range]\n",
        "        if to_plot:\n",
        "            ax.plot(time_to_fit*abf.sampleRate,trace_to_fit,color='green',linewidth=3)\n",
        "\n",
        "        'linear fit of ln_trace'\n",
        "        shift = np.min(trace_to_fit)\n",
        "\n",
        "        ln_trace = np.log(trace_to_fit-shift*2)\n",
        "        [tau_hat, beta_hat], cv = scipy.optimize.curve_fit(linearized_exp_decay, time_to_fit, ln_trace) #\n",
        "        \n",
        "\n",
        "\n",
        "        I_hat = linearized_exp_decay(time_to_fit,tau_hat, beta_hat)\n",
        "        if to_plot:\n",
        "            ax.plot(time_to_fit*abf.sampleRate,np.exp(I_hat)+shift*2,color='turquoise',linewidth=3)\n",
        "        \n",
        "        'Calculate Pclamp Values'\n",
        "        delta_I = I1-I2\n",
        "        Q2 = delta_I * tau_hat # This doesnt make sense to me\n",
        "        I_ss = np.mean([I1,I2])\n",
        "\n",
        "        Q1_ind = np.where(single_pulse_trace>I1)[0]\n",
        "        Q1 = np.sum(single_pulse_trace[Q1_ind] - I1) / abf.sampleRate\n",
        "        \n",
        "        'Plot Area'\n",
        "        patch_points = np.ones([len(Q1_ind)*2,2])\n",
        "        if to_plot:\n",
        "            patch_points[:,0] = np.concatenate((Q1_ind,np.flip(Q1_ind)))\n",
        "            patch_points[:,1] = np.concatenate((single_pulse_trace[Q1_ind],I1*np.ones_like(Q1_ind)))\n",
        "            poly = matplotlib.patches.Polygon(patch_points, color='orange')\n",
        "            ax.add_patch(poly)\n",
        "\n",
        "        'Calculate Pclamp Values'\n",
        "        Qt = Q1 + Q2\n",
        "        Cm = Qt / delta_V\n",
        "        Rt = delta_V/delta_I\n",
        "\n",
        "        'Iterateively Solve Ra using Newton-Raphson Method'\n",
        "        Ra_guess = 20*1e6\n",
        "        delta_guess = 1e10\n",
        "        tol = 1\n",
        "        while delta_guess>tol:\n",
        "            f_of_guess = Ra_guess**2 - Ra_guess*Rt + Rt*(tau_hat/Cm)\n",
        "            f_prime_of_guess = Ra_guess/2 - Rt\n",
        "            Ra_guess_new = Ra_guess - (f_of_guess/f_prime_of_guess)\n",
        "            delta_guess = Ra_guess_new - Ra_guess\n",
        "            Ra_guess = Ra_guess_new\n",
        "        Ra = Ra_guess\n",
        "        Rm = Rt - Ra\n",
        "\n",
        "\n",
        "        if verbose:\n",
        "            print('tau_hat',tau_hat*1000,'ms')\n",
        "            print('Cm',Cm*1e12,'pF')\n",
        "            print('Rt',Rt*1e-6,'MO')\n",
        "            print('Ra',Ra*1e-6,'MO')\n",
        "            print('Rm',Rm*1e-6,'MO')\n",
        "\n",
        "\n",
        "        'Return a dataframe of parameters'\n",
        "        p_ms = int(p/abf.sampleRate*1000)\n",
        "        mem_params_df.at[p_ms,'Tau_pc'] = tau_hat\n",
        "        mem_params_df.at[p_ms,'Rm_pc'] = Rm*1e-6\n",
        "        mem_params_df.at[p_ms,'Ra_pc'] = Ra*1e-6\n",
        "        mem_params_df.at[p_ms,'Cm_pc'] = Cm*1e12\n",
        "\n",
        "        if to_plot: plt.show()\n",
        "\n",
        "    return mem_params_df\n"
      ],
      "metadata": {
        "id": "Ne5WoYGnGmhY"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "####################### SPIKE LATENCY #######################\n",
        "\n",
        "def Spike_latency(abf_recordings_df, protocol_aliases,to_plot=False):\n",
        "    'Loops through abfs and calcualtes the time to first action potential'\n",
        "    'during a ramp current stimulation'\n",
        "\n",
        "\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    # print(np.sum(correct_protocol),'files to analyze...')\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol]): #tqdm( ) : \n",
        "        abf = pyabf.ABF( file_name )\n",
        "        # pass_rate, passing_sweeps, QC_check_df, QC_val_df = Vclamp_QC(file_name,to_plot=False,verbose=False)\n",
        "        passing_sweeps=abf_recordings_df.at[file_name,'passing_sweeps']\n",
        "        latencey_list = []\n",
        "        v_hold_list = []\n",
        "        \n",
        "\n",
        "        for s in abf.sweepList:\n",
        "            abf.setSweep(s)\n",
        "            # plot_sweeps_and_command(abf)\n",
        "            latencey, v_hold = analyze_ramp_sweep(abf.sweepX,abf.sweepY,abf.sweepC,\n",
        "                                              abf.sampleRate,to_plot=to_plot)\n",
        "            latencey_list.append(latencey)\n",
        "            v_hold_list.append(v_hold)\n",
        "        latencey_list =np.array(latencey_list)\n",
        "        v_hold_list =np.array(v_hold_list)\n",
        "        # if len(passing_sweeps)==0:\n",
        "        #     abf_recordings_df.at[file_name,'Spike_Latency_(ms)'] = np.nan\n",
        "        #     abf_recordings_df.at[file_name,'V_hold_(Latency)'] = np.nan\n",
        "        # else:      \n",
        "        # #     latencey_list = latencey_list[passing_sweeps]\n",
        "        # #     v_hold_list = v_hold_list[passing_sweeps]\n",
        "        abf_recordings_df.at[file_name,'Spike_Latency_(ms)'] = np.median(latencey_list)\n",
        "        abf_recordings_df.at[file_name,'V_hold_(Latency)'] = np.median(v_hold_list)\n",
        "    return abf_recordings_df\n",
        "        \n",
        "        \n",
        "def analyze_ramp_sweep(sweepX,sweepY,sweepC,rate,to_plot=False):\n",
        "    'Receives sweep data and finds the first AP and returns it.'\n",
        "    'Also retuns Vhold for quality control.'\n",
        "    is_base = sweepC==sweepC[0]\n",
        "    is_stim = np.logical_not(sweepC==sweepC[0])\n",
        "    ramp_start_ind = np.min(np.where(is_base==False))\n",
        "    v_hold = np.mean( sweepY[0:ramp_start_ind])\n",
        "    # print(sweepX,sweepY)\n",
        "\n",
        "    spike_args = {'spike_thresh':10,\n",
        "                    'high_dv_thresh': 25,\n",
        "                    'low_dv_thresh': -5,\n",
        "                     'window_ms': 2}\n",
        "\n",
        "    dVds, over_thresh, inds, mean_spike_rate = find_spike_in_trace(sweepY, rate,spike_args,is_stim=is_stim)\n",
        " \n",
        "    if len(inds)==0:\n",
        "        # print('no spikes found')\n",
        "        return np.nan,v_hold\n",
        "    latencey = sweepX[np.min(inds)-ramp_start_ind]*1000\n",
        "    if to_plot:\n",
        "        # plt.scatter(sweepX,dVds,color='k')\n",
        "        plt.plot(sweepX,sweepY,color='k')\n",
        "        plt.scatter(sweepX[inds],sweepY[inds],color='r' )\n",
        "        zoom_x_relativ = np.array([ 0.75, 1.5])\n",
        "        zoom_x = zoom_x_relativ*(latencey/1000+sweepX[ramp_start_ind])\n",
        "        # print('zoom_x',zoom_x)\n",
        "        # print('latencey',latencey)\n",
        "        # print('sweepX[ramp_start_ind]',sweepX[ramp_start_ind])\n",
        "        # print('ramp_start_ind',ramp_start_ind)\n",
        "        plt.gca().set_xlim(zoom_x)\n",
        "        plt.show()\n",
        "\n",
        "    return latencey, v_hold\n"
      ],
      "metadata": {
        "id": "6nDDntRwGZ2C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################### Combine memtest measures ##########################\n",
        "def combine_memtest_durations(column_pairs,abf_recordings_df):\n",
        "    'For the niche case where membrane parameter calculated from'\n",
        "    'two pulse durations need to be combined, take a list of columns'\n",
        "    'to combine as a list of tuples, and creates a new column tagged'\n",
        "    '_Combo which consolidates the columns pairs.'\n",
        "\n",
        "    print('Combine long MemTest pulses...')\n",
        "    for p in tqdm(column_pairs) :\n",
        "        col_name = p[0][:p[0].index('_')]+'_Combo'\n",
        "        abf_recordings_df[col_name] = None\n",
        "        for rec in abf_recordings_df.index:\n",
        "            p0 = abf_recordings_df.at[rec,p[0]]\n",
        "            p1 = abf_recordings_df.at[rec,p[1]]\n",
        "            if np.isnan(p0) and np.isnan(p1):\n",
        "                abf_recordings_df.at[rec,col_name] = np.nan\n",
        "            else:\n",
        "                if np.isnan(p0):\n",
        "                    abf_recordings_df.at[rec,col_name] = p1\n",
        "                elif np.isnan(p1):\n",
        "                    abf_recordings_df.at[rec,col_name] = p0\n",
        "                else:\n",
        "                    print('unspecified types:', p0,p1)\n",
        "                    abf_recordings_df.at[rec,col_name] = np.nan\n",
        "    return abf_recordings_df\n"
      ],
      "metadata": {
        "id": "XdC03YaXX2eU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#################################### Summarize Data Cell by Cell ####################################\n",
        "def parse_file_name(abf_recordings_df):\n",
        "    'Takes the abf data frame and sorts the files in to groups based on cell ids in the abf file name.'\n",
        "    'Parses the file name extract the cell meta data and stores in a new summary dataframe'\n",
        "    'The new dataframe is populated with measured data from each abf. repeated measures are combined into'\n",
        "    'a list'\n",
        "\n",
        "    cell_list = []\n",
        "    abf_recordings_df.sort_values('file_name',inplace=True)\n",
        "    for f in abf_recordings_df.index:\n",
        "        cell_id = f[f.rfind('/')+1:] # drop directory info\n",
        "        cell_id = cell_id[:cell_id.find('.abf')] # drop file extension\n",
        "        cell_id = cell_id[:cell_id.rfind('_')] # drop rec number\n",
        "        abf_recordings_df.at[f,'cell_id'] = cell_id # write Id\n",
        "        cell_list.append(cell_id) # keep a list\n",
        "    # print(cell_list)\n",
        " \n",
        "    # populate new cell based dataframe\n",
        "    cell_list = list(set(cell_list))\n",
        "    cell_df = pd.DataFrame( {'cell_id': cell_list}).set_index('cell_id')\n",
        "\n",
        "\n",
        "    verbose = False\n",
        "    # LABELING\n",
        "    for c in cell_df.index:\n",
        "        dashes = [i for i in range(len(str(c))) if '_' in str(c)[i] ]\n",
        "        if verbose: _ = [print('   '+ c[:d]) for d in dashes   ]\n",
        "        date = c[:dashes[0]].lower()\n",
        "        virus = c[dashes[0]+1:dashes[1]].upper()\n",
        "        genotype = c[dashes[1]+1:dashes[2]].upper()\n",
        "        sex = c[dashes[2]+1:dashes[3]]\n",
        "        age =c[dashes[3]+1:dashes[4]].upper()\n",
        "        slice_num = c[dashes[4]+1:dashes[5]].upper()\n",
        "        cell_num = c[dashes[5]+1:].upper()\n",
        "        cell_type = c[dashes[6]+1:].upper()\n",
        "\n",
        "        age = int(age[1:])\n",
        "\n",
        "        # keeps\n",
        "        cell_df.at[c,'Date'] = date\n",
        "        cell_df.at[c,'virus'] = virus\n",
        "        cell_df.at[c,'geno'] = genotype\n",
        "        cell_df.at[c,'age (days)'] = age\n",
        "        cell_df.at[c,'sex'] = sex\n",
        "        cell_df.at[c,'slice'] = slice_num\n",
        "        cell_df.at[c,'cell_type'] = cell_type\n",
        "        if all([age>=(7*30), age<((9+1)*30)]):\n",
        "            cell_df.at[c,'Age_bin'] = '7_to_9'\n",
        "        elif all([age>=(17*30), age<((19+1)*30)]):\n",
        "            cell_df.at[c,'Age_bin'] = '17_to_19'\n",
        "        elif all([age>=(2*30), age<((5+1)*30)]): \n",
        "            cell_df.at[c,'Age_bin'] = '2_to_5'\n",
        "        else:\n",
        "            cell_df.at[c,'Age_bin'] = 'other'\n",
        "\n",
        "    date_to_animal = {}\n",
        "    set_list = list(set(cell_df['Date']))\n",
        "    for i in range(len(set_list)):\n",
        "        d = set_list[i]\n",
        "        date_to_animal[d] = i\n",
        "    \n",
        "    for c in cell_df.index:\n",
        "        d = cell_df.at[c,'Date']\n",
        "        cell_df.at[c,'Animal'] = date_to_animal[d]\n",
        "\n",
        "    #### Actual Values\n",
        "    cell_df['rec_list'] = None\n",
        "    for c in cell_df.index:\n",
        "        is_cell = abf_recordings_df['cell_id'] == c\n",
        "        cell_recs = list(abf_recordings_df.index[is_cell])\n",
        "        cell_df.at[c,'rec_list'] = cell_recs\n",
        "\n",
        "    data_columns = abf_recordings_df.columns\n",
        "    data_columns = [c for c in data_columns if 'cell_id' not in c]\n",
        "    for dc in data_columns:\n",
        "        cell_df[dc] = None\n",
        "        for c in cell_df.index:\n",
        "            is_cell = abf_recordings_df['cell_id'] == c\n",
        "            cell_data = list(abf_recordings_df.loc[is_cell,dc])\n",
        "            if not(type(cell_data[0]) == str):\n",
        "                # remove recs that report 'none'\n",
        "                cd = cell_data[0]\n",
        "                cell_data = [cd for cd in cell_data if not( str(type(cd)) == str(type(None)) )]\n",
        "                # remove recs that report 'nan' but check if its a float first because np.isnan is a little bitch.\n",
        "                float_data = []\n",
        "                for cd in cell_data:\n",
        "                    if str(type(cd)) == \"<class 'float'>\":\n",
        "                        if np.isnan(cd):\n",
        "                            'skip'\n",
        "                        else:\n",
        "                            float_data.append(cd)\n",
        "                    else:\n",
        "                        float_data.append(cd)\n",
        "\n",
        "                cell_data = float_data\n",
        "                # print('new',cell_data)\n",
        "                \n",
        "            cell_df.at[c,dc] = np.array(cell_data)\n",
        "\n",
        "    for c in cell_df.index:\n",
        "        prots = cell_df.at[c,'protocol']\n",
        "        cell_df.at[c,'protocol'] = set(prots)\n",
        "\n",
        "    print(len(cell_df),' cells from',len(set(cell_df['Date'])),'animals')\n",
        "\n",
        "    return cell_df, abf_recordings_df\n"
      ],
      "metadata": {
        "id": "WafNAcyEGhk5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#################### Pull Data From Summary ##############################\n",
        "\n",
        "def get_summary_data(cell_summary_df, data_keys, conditional_dict,filter_empty=False,single_val=False):\n",
        "    'WIP. Takes in a summary dataframe, a list of data keys to look up, and'\n",
        "    'as dict of cell type conditions eg: {\"cell_type\"; \"CA3\"}. Returns cell data.'\n",
        "    'Options include collapsing lists of repeat measures into a single value, or removing'\n",
        "    'cells without values for all data keys (any missing value disqualifies the cell)'\n",
        "\n",
        "\n",
        "    condition_list = [] \n",
        "    for (k,v) in conditional_dict.items():\n",
        "        cond_met =cell_summary_df[k] == v\n",
        "        condition_list.append( cond_met )\n",
        "\n",
        "    condition_list = np.stack(condition_list).T\n",
        "    all_met = [all(r) for r in condition_list]\n",
        "    all_met_ind = cell_summary_df.index[all_met]\n",
        "    cell_vals = {}\n",
        "    for dk in data_keys:\n",
        "        cell_vals[dk] = cell_summary_df.loc[all_met_ind,dk].values\n",
        "   \n",
        "    not_empty_list = []\n",
        "    for dk in data_keys:\n",
        "        # print(cell_vals[dk])\n",
        "        # [print(v) for v in cell_vals[dk]]\n",
        "        # cell_vals[dk] = [ [] if  else v for v in cell_vals[dk] ]\n",
        "        not_empty = [ len(v)>0 for v in cell_vals[dk] ]\n",
        "        not_empty_list.append( not_empty )\n",
        "\n",
        "    not_empty_list = np.stack(not_empty_list).T\n",
        "    all_not_empty = [all(r) for r in not_empty_list]\n",
        "    if filter_empty:\n",
        "        for dk in data_keys:\n",
        "            cell_vals[dk] = cell_vals[dk][all_not_empty]\n",
        "\n",
        "    if single_val:\n",
        "        for dk in data_keys:\n",
        "            # cell_vals[dk] = np.array([v[0] if len(v)>0 else v for v in cell_vals[dk]]) # first value\n",
        "            # cell_vals[dk] = np.array([np.mean(v) for v in cell_vals[dk]]) # mean value\n",
        "            cell_vals[dk] = np.array([np.median(v) for v in cell_vals[dk]]) # median value\n",
        "\n",
        "    return cell_vals, all_met, all_not_empty\n",
        "\n"
      ],
      "metadata": {
        "id": "Oj9ESi3oGYo6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################### ANALYZE IV CURVE #######################\n",
        "\n",
        "\n",
        "\n",
        "from IPython.core.pylabtools import figsize\n",
        "def IV_analyisis(abf_recordings_df,protocol_aliases, post_stim_times=[(0.0165,.03),(0.100,0.120)], stats_to_split= [(0, 'I_peak'),  (0, 'V_stim'), (1, 'I_mean')],to_plot=False):\n",
        "    'Calulates results from an IV protocol. The peak and mean are measured'\n",
        "    'from windows defined in a list of tuples [(start,stop),...]. All values'\n",
        "    'are written to the main dataframe under \"IV_stats\". This is a list of dicts'\n",
        "    'each dict corresponds to one anaysis window reporting the time range, peak etc.'\n",
        "    'Optional variable stats_to_split is used to pull specific measures from IV_stats'\n",
        "    'and write them to their own column. Stats_to_split is a list of tuples, indicating'\n",
        "    'which window and witch measure to write to the dataframe. [(window index,measure key)...]'\n",
        "\n",
        "\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    abf_recordings_df['IV_stats'] = None\n",
        "    abf_recordings_df['IV_stats'] = abf_recordings_df['IV_stats'].astype(object)\n",
        "\n",
        "    stats_to_split = ( (0, 'I_peak'),  (0, 'V_stim'), ( (1, 'I_mean')))\n",
        "    for t in stats_to_split:\n",
        "        name = t[1]\n",
        "        abf_recordings_df['IV_'+name] = None\n",
        "        abf_recordings_df['IV_'+name] = abf_recordings_df['IV_'+name].astype(object)\n",
        "\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol]):\n",
        "        abf = pyabf.ABF( file_name )\n",
        "        passing_sweeps=abf_recordings_df.at[file_name,'passing_sweeps']\n",
        "\n",
        "        pass_rate, passing_sweeps, QC_check_df, QC_val_df = Vclamp_QC(file_name,to_plot=False,verbose=False,Vhold_range = 100)\n",
        "        # display(QC_val_df)\n",
        "        if to_plot: plot_sweeps_and_command(abf)\n",
        "        # passing_sweeps = abf.sweepList # OVERRIDE\n",
        "        print(passing_sweeps)\n",
        "        abf = pyabf.ABF( file_name )\n",
        "        if to_plot: print(file_name)\n",
        "        IV_stats =analyze_IV(abf, passing_sweeps, post_stim_times,to_plot =to_plot)\n",
        "        # [print(r) for r in IV_stats]\n",
        "        \n",
        "        abf_recordings_df.at[file_name,'IV_stats'] = IV_stats\n",
        "\n",
        "        #### \n",
        "        if len(IV_stats) > 0:\n",
        "            for t in stats_to_split:\n",
        "                name = t[1]\n",
        "                range = t[0]\n",
        "                values = IV_stats[range][name]\n",
        "                values = np.array(values).tolist()\n",
        "                abf_recordings_df.at[file_name,'IV_'+name] = values\n",
        "\n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "\n",
        "def get_IV_measures(abf,passing_sweeps,post_stim_times,to_plot=False,spike_thresh=100000):\n",
        "    'Takes the abf data and calculates the mean and peak for each window.'\n",
        "    'Optionaly sweeps with APs can be filtered out using spike_threshold.'\n",
        "    'Only sweeps passing the QC filter are used. Returns a list of dicts with'\n",
        "    'each dict corresponding to one anaysis window reporting the time range, peak etc.'\n",
        "\n",
        "    IV_stats = []\n",
        "    theta, offset, correct_ch1 = predict_telegraph(abf)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    if len(passing_sweeps)==0:\n",
        "        return IV_stats\n",
        "\n",
        "    if to_plot:\n",
        "        fig, axs = plt.subplots(len(post_stim_times),figsize = [12,4] )\n",
        "        if len(post_stim_times)==1: axs = [axs]\n",
        "    \n",
        "    for t in range(len(post_stim_times)):\n",
        "        window_stats = {}\n",
        "        start = post_stim_times[t][0]\n",
        "        stop = post_stim_times[t][1]\n",
        "        delta_t = stop-start\n",
        "        flank_neg = start - (delta_t)*0.5\n",
        "        flank_pos = stop + (delta_t)*0.5\n",
        "        full_plot_range_ind =   np.logical_and(  abf.sweepX>flank_neg, abf.sweepX<flank_pos )\n",
        "        spikeless_sweeps = []\n",
        "        for s in passing_sweeps:\n",
        "            if to_plot: s_shift_t = delta_t*0.03*s\n",
        "            if to_plot: s_shift_I = 100*s\n",
        "            abf.setSweep(s)\n",
        "            time_abrg = abf.sweepX[full_plot_range_ind]\n",
        "            I_abrg = abf.sweepY[full_plot_range_ind]\n",
        "\n",
        "            time_abrg = abf.sweepX[full_plot_range_ind]\n",
        "            I_abrg = abf.sweepY[full_plot_range_ind]\n",
        "            analysis_range = np.logical_and(  abf.sweepX>start, abf.sweepX<stop )\n",
        "            time_analysis = abf.sweepX[analysis_range]\n",
        "            I_analysis = movmean( abf.sweepY[analysis_range], 4)\n",
        "            dIdt = movmean(np.diff(I_abrg, prepend=I_abrg[0]),1)\n",
        "\n",
        "            if np.max(abs(dIdt))>spike_thresh:\n",
        "                'indent holder'\n",
        "                # if to_plot:axs[t].plot( time_abrg+s_shift_t, dIdt+s_shift_I,'r')\n",
        "            else:\n",
        "                # if to_plot:axs[t].plot( time_abrg+s_shift_t, dIdt+s_shift_I,'k')\n",
        "                spikeless_sweeps.append(s)\n",
        "\n",
        "\n",
        "        I_peak_l = []\n",
        "        time_peak_l = []\n",
        "        I_mean_l = []\n",
        "        V_stim_l = []\n",
        "        \n",
        "        for s in  spikeless_sweeps :\n",
        "            abf.setSweep(s)\n",
        "            s_shift_t = delta_t*0.03*s * 0\n",
        "            s_shift_I = 100*s * 0\n",
        "\n",
        "            time_abrg = abf.sweepX[full_plot_range_ind]\n",
        "            I_abrg = abf.sweepY[full_plot_range_ind]\n",
        "            analysis_range = np.logical_and(  abf.sweepX>start, abf.sweepX<stop )\n",
        "            time_analysis = abf.sweepX[analysis_range]\n",
        "            I_analysis = abf.sweepY[analysis_range]\n",
        "            \n",
        "\n",
        "            # Peaks\n",
        "            if to_plot: axs[t].plot( time_abrg+s_shift_t, I_abrg+s_shift_I,'k')\n",
        "            \n",
        "            I_mean = np.median( I_analysis )\n",
        "            if to_plot: axs[t].plot( time_analysis+s_shift_t, (I_mean*np.ones_like(time_analysis))+s_shift_I,'r' )            \n",
        "            \n",
        "            local_I_mean = movmean(I_analysis,100)\n",
        "            peak_ind = np.where(abs(I_analysis-I_mean) == np.max(abs(I_analysis-I_mean)))[0]\n",
        "            # print(peak_ind)\n",
        "            if len(peak_ind)>1: peak_ind=[np.min(peak_ind)]\n",
        "                # print('drop')\n",
        "                # print(peak_ind)\n",
        "\n",
        "            I_peak =I_analysis[peak_ind]\n",
        "            t_peak = time_analysis[peak_ind]\n",
        "            if to_plot: axs[t].scatter( t_peak+s_shift_t, I_peak+s_shift_I, color='m' )\n",
        "\n",
        "            abf.setSweep(sweepNumber=s, channel=1)\n",
        "            command_trace = abf.sweepY\n",
        "\n",
        "\n",
        "            I_peak_l.append(I_peak)\n",
        "            time_peak_l.append(t_peak)\n",
        "            I_mean_l.append(I_mean)\n",
        "            # raw_v = np.median( abf.sweepC[analysis_range])\n",
        "            # V_stim_l.append( 10*np.round( (raw_v +offset )/10))\n",
        "\n",
        "            command_v = np.median( command_trace[analysis_range])\n",
        "            V_stim_l.append(command_v)\n",
        "    \n",
        "\n",
        "        I_peak_l = np.concatenate(I_peak_l)\n",
        "        time_peak_l = np.concatenate(time_peak_l)\n",
        "\n",
        "\n",
        "        # print(I_peak_l)\n",
        "        # print(time_peak_l)\n",
        "        # print(I_mean_l)\n",
        "        # print(V_stim_l)\n",
        "\n",
        "\n",
        "        window_stats['range'] = (start,stop)\n",
        "        window_stats['I_peak'] = I_peak_l\n",
        "        window_stats['I_peak_time'] = time_peak_l\n",
        "        window_stats['I_mean'] = I_mean_l\n",
        "        window_stats['V_stim'] = np.stack(V_stim_l).flatten()\n",
        "        IV_stats.append(window_stats)\n",
        "\n",
        "    return IV_stats\n",
        "    \n",
        "\n",
        "def analyze_IV(abf, passing_sweeps, post_stim_times, to_plot=False):\n",
        "    'This is largely a wrapper for get_IV_measures(). The main role'\n",
        "    'of this function is to plot the summary data, ie the traditional IV plot'\n",
        "\n",
        "    IV_stats = get_IV_measures(abf, passing_sweeps,post_stim_times,to_plot=to_plot,spike_thresh=1e6)\n",
        "    any_sweeps = any(np.greater([len(w['V_stim']) for w in IV_stats],0))\n",
        "    if not any_sweeps:\n",
        "        return IV_stats\n",
        "\n",
        "    if to_plot:\n",
        "        fig, axs = plt.subplots(1,2,figsize = (9,4))\n",
        "        early_peak = IV_stats[0]['I_peak']\n",
        "        V_stim = IV_stats[0]['V_stim']\n",
        "        axs[0].plot(V_stim,early_peak,color='m',marker='o',label='Peak Current')\n",
        "\n",
        "\n",
        "        # ax.scatter(V_stim,early_peak,color='k')\n",
        "        late_mean = IV_stats[1]['I_mean']\n",
        "        V_stim = IV_stats[1]['V_stim']\n",
        "        axs[0].plot(V_stim,late_mean,color='r',marker='o',label='Steady State Current')\n",
        "\n",
        "        from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
        "        axs[0].xaxis.set_major_locator(MultipleLocator(20))\n",
        "        axs[0].legend(loc=\"upper left\",frameon=True,framealpha=1)\n",
        "        axs[0].set_ylabel('Current (pA)')\n",
        "        axs[0].set_xlabel('Membrane Potential (mV)')\n",
        "\n",
        "        \n",
        "        # ax.grid(True)\n",
        "        axs[0].spines['top'].set_visible(False)\n",
        "        axs[0].spines['right'].set_visible(False)\n",
        "        axs[0].spines['bottom'].set_visible(False)\n",
        "        axs[0].spines['left'].set_visible(False)\n",
        "        yL = axs[0].get_ylim()\n",
        "        xL = axs[0].get_xlim()\n",
        "        axs[0].plot(xL,[0,0],':k')\n",
        "        axs[0].plot([-70,-70],yL,':k')\n",
        "\n",
        "\n",
        "        for s in passing_sweeps:\n",
        "            abf.setSweep(s)\n",
        "            axs[1].plot(abf.sweepX,abf.sweepC,'k')\n",
        "        \n",
        "        yL = axs[1].get_ylim()\n",
        "        axs[1].axvspan( IV_stats[0]['range'][0],IV_stats[0]['range'][1],yL[0],yL[1], alpha=0.2 ,color='m')\n",
        "        axs[1].axvspan( IV_stats[1]['range'][0],IV_stats[1]['range'][1],yL[0],yL[1], alpha=0.2 ,color='r')\n",
        "        plt.show()\n",
        "    return IV_stats\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJTX1AU04ypU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################### STREAMLINE SUMMARY ###########################\n",
        "def stream_line_cell_summary(cell_summary_df_messy):\n",
        "    'Takes in the cell-summary_df and stream lines it into single'\n",
        "    'measures per cell that can be easily collated into a readable csv'\n",
        "    'that is broadly shareable between analysis platforms.'\n",
        "    cell_summary_df =pd.DataFrame(cell_summary_df_messy.index ).set_index('cell_id')\n",
        "\n",
        "    # print(cell_summary_df_messy.columns)\n",
        "\n",
        "    'Cell Details'\n",
        "    cols = ['Date', 'geno', 'age (days)', 'sex', 'slice', 'cell_type', 'Age_bin']\n",
        "    # Copy relevant info\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "\n",
        "\n",
        "\n",
        "    'Fast Pulse Capcitance VC'\n",
        "    cols = ['Ra_10.0','Rm_10.0','Cmq_10.0','Cm_pc_10.0']\n",
        "    # Test Pulse No QC here\n",
        "    # Consolidate with averaging\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "    for c in cols:\n",
        "        for id in cell_summary_df.index:\n",
        "            multi_values = cell_summary_df.at[id,c]\n",
        "            consolidated = np.mean(multi_values)\n",
        "            cell_summary_df.at[id,c] = consolidated\n",
        "    \n",
        "    'Slow Pulse Capcitance VC'\n",
        "    cols = ['Ra_160.0','Rm_160.0','Cmq_160.0','Cm_pc_160.0']\n",
        "    # Test Pulse No QC here\n",
        "    # Consolidate with averaging\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "    for c in cols:\n",
        "        for id in cell_summary_df.index:\n",
        "            multi_values = cell_summary_df.at[id,c]\n",
        "            consolidated = np.mean(multi_values)\n",
        "            cell_summary_df.at[id,c] = consolidated\n",
        "\n",
        "\n",
        "\n",
        "    'Spike Latency - IC'\n",
        "    cols = ['Spike_Latency_(ms)', 'V_hold_(Latency)']\n",
        "    # QC: V_hold should be -70+/-3 to get consistant charge\n",
        "    # Consolidate with averaging\n",
        "    for id in cell_summary_df.index:\n",
        "        multi_values_SL = cell_summary_df_messy.at[id,'Spike_Latency_(ms)']\n",
        "        multi_values_VH = cell_summary_df_messy.at[id,'V_hold_(Latency)']\n",
        "        condition  = np.logical_and(multi_values_VH>-80,multi_values_VH<-60)\n",
        "        good_vals = [multi_values_SL[i] for i in range(len(multi_values_SL)) if condition[i]]\n",
        "        consolidated = np.mean(  good_vals  )\n",
        "        cell_summary_df.at[id,'Spike_Latency_(ms)'] = consolidated\n",
        "\n",
        "\n",
        "    'Rheobase - IC'\n",
        "    cols = ['Rheobase_(pA)', 'AP_Threshold(mV)', 'Vhold_Rheo_(mV)', 'Rheo Ihold_(pA)']\n",
        "    # QC: V_hold should be -70+/-3 to get consistant charge. I_hold_(pA) should be ~<100 for good clamping\n",
        "    # Consolidate with mean\n",
        "    for c in cols[:4]:\n",
        "        for id in cell_summary_df.index:\n",
        "            multi_values = cell_summary_df_messy.at[id,c]\n",
        "            multi_values_VH = cell_summary_df_messy.at[id,'Vhold_Rheo_(mV)']\n",
        "            multi_values_IH = cell_summary_df_messy.at[id,'Rheo Ihold_(pA)']\n",
        "            condition_V  = np.logical_and(multi_values_VH>-80,multi_values_VH<-60)\n",
        "            condition_I  = abs(multi_values_IH)<250\n",
        "            condition  = np.logical_and(condition_V,condition_I)\n",
        "            good_vals = [multi_values[i] for i in range(len(multi_values)) if condition[i]]\n",
        "            consolidated = np.mean(  good_vals  )\n",
        "            cell_summary_df.at[id,c] = consolidated\n",
        "\n",
        "\n",
        "    'Input Resistance - IC'\n",
        "    cols = ['Rinput_(MO)', 'Cmf_IC_(pF)']\n",
        "    # QC: None\n",
        "    # Consolidate with averaging\n",
        "    for c in cols:\n",
        "        for id in cell_summary_df.index:\n",
        "            multi_values = cell_summary_df_messy.at[id,c]\n",
        "            consolidated = np.mean(  multi_values )\n",
        "            cell_summary_df.at[id,c] = consolidated\n",
        "\n",
        "\n",
        "    'Fireing Rate Gain - IC'\n",
        "    cols = ['Firing_Gain_(Hz/pA)','R2 (Firing_Gain_R2)', 'Firing_Duration_%','Gain_Stims_pA','Gain_NumSpikes']\n",
        "    # QC: R2 (r squared) should be decent so that (>0.8) we fit a reasonable section of curve with a line\n",
        "    # Consolidate by selecting best Range of Response\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "    for id in cell_summary_df.index:\n",
        "        try:\n",
        "            multi_values_Spikes = cell_summary_df_messy.at[id,'Gain_NumSpikes']\n",
        "            \n",
        "            range_list =[]\n",
        "            for trial in range(len(multi_values_Spikes)):\n",
        "                range_list.append(np.max( multi_values_Spikes[trial]) - np.min( multi_values_Spikes[trial]))\n",
        "            if len(multi_values_Spikes) > 0  :             \n",
        "                chosen_one = np.where(range_list == np.max(range_list))[0]\n",
        "                if len(chosen_one)>0: chosen_one = chosen_one[0]\n",
        "                for c in cols: \n",
        "                    if len(cell_summary_df_messy.at[id,c]) == 0:\n",
        "                        cell_summary_df.at[id,c] = np.nan\n",
        "                    else:\n",
        "                        consolidated = cell_summary_df_messy.at[id,c][chosen_one] # fails Here\n",
        "                        cell_summary_df.at[id,c] = consolidated.tolist()\n",
        "                    if c=='Firing_Duration_%':\n",
        "                        cell_summary_df.at[id,c] = np.nanmedian(cell_summary_df_messy.at[id,c])\n",
        "            else:\n",
        "                for c in cols:  \n",
        "                    cell_summary_df.at[id,c] = np.nan\n",
        "        except: \n",
        "            print('error on ', id)\n",
        "            print(multi_values_Spikes)\n",
        "            print(range_list)\n",
        "            print('c',c)\n",
        "\n",
        "    'IV ~I_Na , ~I_K'\n",
        "    cols = ['IV_V_stim', 'IV_I_peak', 'IV_I_mean']\n",
        "    # QC:  Look For normal Na 'swoop' and 'opposing Kv currents\n",
        "    # By eyeball, linear currents should not be extreme.\n",
        "    # Consolidate with averaging\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "\n",
        "    for id in cell_summary_df.index:\n",
        "        # print(id)\n",
        "        stim_values = cell_summary_df_messy['IV_V_stim'][id]\n",
        "        stim_values = [b for a in stim_values for b in a]\n",
        "        # print('stim_values', stim_values)\n",
        "        if len(stim_values)>0:\n",
        "            stim_set = list(set(stim_values))\n",
        "            stim_set.sort()\n",
        "            cell_summary_df.at[id,'IV_V_stim']=stim_set\n",
        "            for c in cols[1:]:\n",
        "                resp_vals = cell_summary_df_messy[c][id]\n",
        "                flattened_vals = [b for a in resp_vals for b in a]\n",
        "                # print('flattened_responses',flattened_vals)\n",
        "                response_list = []\n",
        "                for v in stim_set:\n",
        "                    # print(\"stim_values\",len(stim_values),v)\n",
        "                    # print('logical', stim_values == v)\n",
        "                    index_to_avg = np.where( np.equal(stim_values, v))\n",
        "                    # print('index_to_avg',index_to_avg)\n",
        "                    vals_to_avg = flattened_vals[index_to_avg[0][0]]\n",
        "                    # print('vals_to_avg',vals_to_avg)\n",
        "                    response_list.append( np.mean(vals_to_avg))\n",
        "                response_array = np.stack(response_list).tolist()\n",
        "                cell_summary_df.at[id,c]=response_array\n",
        "\n",
        "    # for c in cols:\n",
        "    #     cell_summary_df[c] = None\n",
        "    #     cell_summary_df[c] = cell_summary_df[c].astype(object)\n",
        "    #     for id in cell_summary_df.index:\n",
        "    #         print(cell_summary_df_messy.loc[id, 'IV_V_stim'])\n",
        "    #         multi_values = cell_summary_df_messy.at[id,c]\n",
        "    #         if len(multi_values)==0:\n",
        "    #             continue\n",
        "    #         multi_values = np.stack(multi_values)\n",
        "    #         consolidated = np.mean(multi_values,axis=0)\n",
        "    #         cell_summary_df.at[id,c] = consolidated\n",
        "    return cell_summary_df\n",
        "\n"
      ],
      "metadata": {
        "id": "UG6nSYbMMF0n"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def consolidate_store_data(abf_recordings_df):\n",
        "\n",
        "    unused = ['Ra_5.0','Rm_5.0','Cmq_5.0','Ra_Combo','Rm_Combo','Cmq_Combo']\n",
        "\n",
        "    abrg_col = ['protocol','abf_timestamp','Ra_10.0','Rm_10.0','Cmq_10.0', 'Ra_160.0','Rm_160.0','Cmq_160.0', 'Cm_pc_10.0',  'Cm_pc_160.0',\n",
        "             'Gain_Stims_pA','Gain_NumSpikes', 'Firing_Gain_(Hz/pA)', 'R2 (Firing_Gain_R2)', 'Firing_Duration_%',\n",
        "            'Rheobase_(pA)','AP_Threshold(mV)', \n",
        "            'Rinput_(MO)','Spike_Latency_(ms)', \n",
        "            'IV_I_peak', 'IV_V_stim', 'IV_I_mean',\n",
        "            'Cmf_IC_(pF)','Vhold_Rheo_(mV)', 'Rheo Ihold_(pA)','V_hold_(Latency)','passing_sweeps',]\n",
        "\n",
        "    abrg_abf_recs_df = abf_recordings_df[abrg_col].copy()\n",
        "    cell_summary_df, abf_recordings_df_tagged = parse_file_name(abrg_abf_recs_df)\n",
        "    cell_summary_df.sort_index(inplace=True)\n",
        "    \n",
        "    #################### Store Data ##################\n",
        "    abrg_abf_recs_df.to_hdf('abrg_abf_recs_df.hdf','abrg_abf_recs_df')\n",
        "    abrg_abf_recs_df.to_csv('abrg_abf_recs_df.csv')\n",
        "    abf_recordings_df_tagged.to_csv('abf_recordings_df_tagged.csv')\n",
        "    cell_summary_df.to_csv('cell_summary_df.csv')\n",
        "\n",
        "\n",
        "    from google.colab import files\n",
        "    files.download('abrg_abf_recs_df.csv')\n",
        "    files.download('abf_recordings_df_tagged.csv')\n",
        "    files.download('cell_summary_df.csv') \n",
        "\n",
        "    cell_summary_df_clean = stream_line_cell_summary(cell_summary_df)\n",
        "    cell_summary_df_clean.to_csv('cell_summary_df_clean.csv')\n",
        "    files.download('cell_summary_df_clean.csv')\n",
        "\n",
        "    return cell_summary_df_clean, abrg_abf_recs_df\n",
        "\n"
      ],
      "metadata": {
        "id": "uDhjbAyGhbf4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## The PipeLine Cell ###############################\n",
        "####### Depenencies ###########\n",
        "!pip install --upgrade pyabf\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyabf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from scipy import stats\n",
        "import os\n",
        "from scipy.signal import butter,filtfilt\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from IPython.display import clear_output\n",
        "from datetime import datetime\n",
        "clear_output(wait=False)\n",
        "import sys\n",
        "import warnings\n",
        "from google.colab import files\n",
        "warnings.filterwarnings('ignore')\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "############ Get Files & Populate Data Frame ############\n",
        "\n",
        "new_filename = str(datetime.now())[:10]+\"_hipp_data.zip\"\n",
        "new_filename = \"_hipp_data.zip\"\n",
        "file_loc = get_drobox_folder(link, new_filename)\n",
        "clear_output(wait=False)\n",
        "cell_id_order = ['Rec_date','Virus','GenoType','Sex','Age','Slice_Num','Cell_num','Cell_Type']\n",
        "abf_recordings_df,protocol_set = catalogue_recs(file_loc,cell_id_order)\n",
        "print(len(abf_recordings_df), 'files')\n",
        "print('Protocols:')\n",
        "_=[print(\"   \",p) for p in protocol_set]\n",
        "# print('ABFS:')\n",
        "# _=[print(\"   \",p) for p in abf_recordings_df.index]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_1cfqD5DbUDc",
        "outputId": "ff7c70d3-edb1-4a72-de89-b97e9a36bfa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256 files\n",
            "Protocols:\n",
            "    IC - R input\n",
            "    VC - Multi IV - 150ms\n",
            "    IC - Rheobase\n",
            "    IC - Latentcy 800pA-1s\n",
            "    IC - Sag - D50pA\n",
            "    IC - Gain - D50pA\n",
            "    VC - MemTest-10ms-160ms\n",
            "    VC - 3min GapFree\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "note_book = abf_recordings_df[abf_recordings_df.columns]\n",
        "note_book.set_index('Recording_name',inplace=True)\n",
        "note_book.drop(labels=['channelList'],axis=1,inplace=True)\n",
        "note_book.to_csv('note_book_page.csv')\n",
        "files.download('note_book_page.csv')\n",
        "display(note_book)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "JZgHgpbG-RgB",
        "outputId": "84867150-192b-4214-e89e-4af91226c0d4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e28a819b-2c49-42e7-b9b5-563504f02993\", \"note_book_page.csv\", 50250)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                                                            cell_id  \\\n",
              "Recording_name                                                                                        \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS   \n",
              "...                                                                                             ...   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG   \n",
              "\n",
              "                                                      Rec_date   Virus  \\\n",
              "Recording_name                                                           \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12  RNF182   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12  RNF182   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12  RNF182   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12  RNF182   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12  RNF182   \n",
              "...                                                        ...     ...   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17  RNF182   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17  RNF182   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17  RNF182   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17  RNF182   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17  RNF182   \n",
              "\n",
              "                                                   GenoType Sex   Age  \\\n",
              "Recording_name                                                          \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...     E4KI   F  P251   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...     E4KI   F  P251   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...     E4KI   F  P251   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...     E4KI   F  P251   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...     E4KI   F  P251   \n",
              "...                                                     ...  ..   ...   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...     E4KI   F  P256   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...     E4KI   F  P256   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...     E4KI   F  P256   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...     E4KI   F  P256   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...     E4KI   F  P256   \n",
              "\n",
              "                                                   Slice_Num Cell_num  \\\n",
              "Recording_name                                                          \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...      s001     c001   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...      s001     c001   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...      s001     c001   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...      s001     c001   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...      s001     c001   \n",
              "...                                                      ...      ...   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...      s002     c010   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...      s002     c010   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...      s002     c010   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...      s002     c010   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...      s002     c010   \n",
              "\n",
              "                                                   Cell_Type  \\\n",
              "Recording_name                                                 \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...   CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...   CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...   CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...   CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...   CA3xPOS   \n",
              "...                                                      ...   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...   CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...   CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...   CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...   CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...   CA3xNEG   \n",
              "\n",
              "                                                                   protocol  \\\n",
              "Recording_name                                                                \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...        VC - 3min GapFree   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  VC - MemTest-10ms-160ms   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...    VC - Multi IV - 150ms   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...    VC - Multi IV - 150ms   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...        IC - Gain - D50pA   \n",
              "...                                                                     ...   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...   IC - Latentcy 800pA-1s   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...             IC - R input   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...    VC - Multi IV - 150ms   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  VC - MemTest-10ms-160ms   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  VC - MemTest-10ms-160ms   \n",
              "\n",
              "                                                              abf_timestamp  \n",
              "Recording_name                                                               \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022-08-12T12:06:00.750  \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022-08-12T12:06:16.222  \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022-08-12T12:06:36.142  \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022-08-12T12:06:42.833  \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022-08-12T12:07:28.480  \n",
              "...                                                                     ...  \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022-08-17T17:32:59.098  \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022-08-17T17:33:42.410  \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022-08-17T17:34:50.985  \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022-08-17T17:35:28.268  \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022-08-17T17:35:43.981  \n",
              "\n",
              "[256 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c6b6e97-3d07-4477-805a-20a429d9a142\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cell_id</th>\n",
              "      <th>Rec_date</th>\n",
              "      <th>Virus</th>\n",
              "      <th>GenoType</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>Slice_Num</th>\n",
              "      <th>Cell_num</th>\n",
              "      <th>Cell_Type</th>\n",
              "      <th>protocol</th>\n",
              "      <th>abf_timestamp</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Recording_name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0000.abf</th>\n",
              "      <td>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS</td>\n",
              "      <td>2022x08x12</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P251</td>\n",
              "      <td>s001</td>\n",
              "      <td>c001</td>\n",
              "      <td>CA3xPOS</td>\n",
              "      <td>VC - 3min GapFree</td>\n",
              "      <td>2022-08-12T12:06:00.750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0001.abf</th>\n",
              "      <td>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS</td>\n",
              "      <td>2022x08x12</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P251</td>\n",
              "      <td>s001</td>\n",
              "      <td>c001</td>\n",
              "      <td>CA3xPOS</td>\n",
              "      <td>VC - MemTest-10ms-160ms</td>\n",
              "      <td>2022-08-12T12:06:16.222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0002.abf</th>\n",
              "      <td>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS</td>\n",
              "      <td>2022x08x12</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P251</td>\n",
              "      <td>s001</td>\n",
              "      <td>c001</td>\n",
              "      <td>CA3xPOS</td>\n",
              "      <td>VC - Multi IV - 150ms</td>\n",
              "      <td>2022-08-12T12:06:36.142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0003.abf</th>\n",
              "      <td>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS</td>\n",
              "      <td>2022x08x12</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P251</td>\n",
              "      <td>s001</td>\n",
              "      <td>c001</td>\n",
              "      <td>CA3xPOS</td>\n",
              "      <td>VC - Multi IV - 150ms</td>\n",
              "      <td>2022-08-12T12:06:42.833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0004.abf</th>\n",
              "      <td>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS</td>\n",
              "      <td>2022x08x12</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P251</td>\n",
              "      <td>s001</td>\n",
              "      <td>c001</td>\n",
              "      <td>CA3xPOS</td>\n",
              "      <td>IC - Gain - D50pA</td>\n",
              "      <td>2022-08-12T12:07:28.480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG_0006.abf</th>\n",
              "      <td>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG</td>\n",
              "      <td>2022x08x17</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P256</td>\n",
              "      <td>s002</td>\n",
              "      <td>c010</td>\n",
              "      <td>CA3xNEG</td>\n",
              "      <td>IC - Latentcy 800pA-1s</td>\n",
              "      <td>2022-08-17T17:32:59.098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG_0007.abf</th>\n",
              "      <td>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG</td>\n",
              "      <td>2022x08x17</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P256</td>\n",
              "      <td>s002</td>\n",
              "      <td>c010</td>\n",
              "      <td>CA3xNEG</td>\n",
              "      <td>IC - R input</td>\n",
              "      <td>2022-08-17T17:33:42.410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG_0008.abf</th>\n",
              "      <td>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG</td>\n",
              "      <td>2022x08x17</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P256</td>\n",
              "      <td>s002</td>\n",
              "      <td>c010</td>\n",
              "      <td>CA3xNEG</td>\n",
              "      <td>VC - Multi IV - 150ms</td>\n",
              "      <td>2022-08-17T17:34:50.985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG_0009.abf</th>\n",
              "      <td>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG</td>\n",
              "      <td>2022x08x17</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P256</td>\n",
              "      <td>s002</td>\n",
              "      <td>c010</td>\n",
              "      <td>CA3xNEG</td>\n",
              "      <td>VC - MemTest-10ms-160ms</td>\n",
              "      <td>2022-08-17T17:35:28.268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG_0010.abf</th>\n",
              "      <td>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG</td>\n",
              "      <td>2022x08x17</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P256</td>\n",
              "      <td>s002</td>\n",
              "      <td>c010</td>\n",
              "      <td>CA3xNEG</td>\n",
              "      <td>VC - MemTest-10ms-160ms</td>\n",
              "      <td>2022-08-17T17:35:43.981</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>256 rows  11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c6b6e97-3d07-4477-805a-20a429d9a142')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6c6b6e97-3d07-4477-805a-20a429d9a142 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6c6b6e97-3d07-4477-805a-20a429d9a142');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########### Run QC tests ###############\n",
        "print('running QC...')\n",
        "VC_prot = ['VC - MemTest-10ms-160ms',\n",
        "           'VC - Multi IV - 150ms',\n",
        "           'VC - 3min GapFree',\n",
        "           'VC - Spontaneous-3min-(MT)']\n",
        "IC_prot = ['IC - Gain - D20pA',\n",
        "           'IC - Gain - D50pA',\n",
        "           'IC - Rheobase',\n",
        "           'IC - R input',\n",
        "           'IC - Latentcy 800pA-1s']\n",
        "MT_prot = ['VC - MemTest-10ms-160ms']\n",
        "\n",
        "\n",
        "abf_recordings_df = QC_full_dataset(abf_recordings_df,to_plot=False,verbose=False,VC_prot=VC_prot,IC_prot=IC_prot,MT_prot=MT_prot)\n"
      ],
      "metadata": {
        "id": "J2bawVQmVmB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f80bee-b94f-4609-b71d-3e944fca0d7f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running QC...\n",
            "Voltage Clamp Protocols\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/103 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|         | 8/103 [00:00<00:07, 12.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|        | 12/103 [00:00<00:07, 12.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|        | 17/103 [00:01<00:05, 15.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|        | 21/103 [00:01<00:06, 12.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|       | 28/103 [00:02<00:04, 15.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|       | 30/103 [00:02<00:05, 14.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|      | 36/103 [00:02<00:06, 10.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|      | 38/103 [00:03<00:06, 10.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|      | 42/103 [00:03<00:06,  9.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|     | 45/103 [00:04<00:06,  8.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|     | 48/103 [00:04<00:07,  7.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|     | 51/103 [00:04<00:06,  7.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|    | 55/103 [00:05<00:05,  8.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|    | 58/103 [00:05<00:05,  8.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|    | 62/103 [00:06<00:05,  7.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|   | 65/103 [00:06<00:05,  7.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|   | 67/103 [00:06<00:04,  8.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|   | 70/103 [00:07<00:04,  8.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|   | 73/103 [00:07<00:03,  8.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|  | 75/103 [00:08<00:04,  6.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|  | 78/103 [00:08<00:03,  6.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|  | 83/103 [00:09<00:03,  6.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%| | 87/103 [00:09<00:01,  8.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%| | 91/103 [00:10<00:01,  8.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|| 94/103 [00:10<00:01,  8.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|| 99/103 [00:11<00:00,  8.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 103/103 [00:12<00:00,  8.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Clamp Protocols\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 137/137 [00:42<00:00,  3.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## Multi Mem Test ########################\n",
        "print('Passive Membrane params...')\n",
        "protocol_aliases = ['VC - MemTest-10ms-160ms']\n",
        "abf_recordings_df, correct_protocol = Icapacitance_analysis(abf_recordings_df, protocol_aliases,to_plot=False,verbose=False)\n",
        "\n",
        "################### Rheobase Analyisis ###########################\n",
        "print('Rheobase Analyisis...')\n",
        "Rheo_aliases = ['IC - Rheobase']\n",
        "abf_recordings_df = Rheo_curve_analysis(abf_recordings_df, Rheo_aliases,to_plot = False,single_spike=False)\n",
        "\n",
        "############## Fireing Rate Gain ################\n",
        "print('Fireing Rate Gain...')\n",
        "IFcurve_aliases = ['IC - Gain - D20pA', 'IC - Gain - D50pA']\n",
        "abf_recordings_df = IF_curve_analysis(abf_recordings_df, IFcurve_aliases,to_plot = 0)\n",
        "\n",
        "############## SpikeLatency ################\n",
        "print('SpikeLatency...')\n",
        "SpikeLatency_aliases = ['IC - Latentcy 800pA-1s']\n",
        "abf_recordings_df = Spike_latency(abf_recordings_df, SpikeLatency_aliases,to_plot = False)\n",
        "\n",
        "################### IClamp Input Resistance ##############\n",
        "print('IClamp Input Resistance...')\n",
        "inputR_aliases = ['IC - R input']\n",
        "abf_recordings_df = inputR_analysis(abf_recordings_df, inputR_aliases,to_plot=False)\n",
        "\n",
        "################### IV Curve ###################\n",
        "print('IV Curve...')\n",
        "protocol_aliases = ['VC - Multi IV - 150ms']\n",
        "abf_recordings_df = IV_analyisis(abf_recordings_df, protocol_aliases,to_plot=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QlTGCr9eKCcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f35f570-4da6-414c-e5c5-4c3dde154e95"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passive Membrane params...\n",
            "256 files to analyze...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|         | 4/43 [00:00<00:03, 10.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "passing_sweeps = [s for s in QC_check_df.index if all(QC_check_df.loc[s,['I_leak','HF_noise','LF_noise']])] # ignore Vhold Filters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|       | 11/43 [00:01<00:04,  7.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|     | 21/43 [00:03<00:04,  5.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"array must not contain infs or NaNs\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|  | 31/43 [00:05<00:02,  4.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"array must not contain infs or NaNs\")\n",
            "\"array must not contain infs or NaNs\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|  | 33/43 [00:06<00:02,  4.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"array must not contain infs or NaNs\")\n",
            "\"array must not contain infs or NaNs\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|  | 34/43 [00:06<00:01,  4.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"array must not contain infs or NaNs\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%| | 36/43 [00:06<00:01,  4.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"array must not contain infs or NaNs\")\n",
            "\"array must not contain infs or NaNs\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%| | 37/43 [00:06<00:01,  5.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"array must not contain infs or NaNs\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%| | 38/43 [00:07<00:01,  4.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"array must not contain infs or NaNs\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%| | 39/43 [00:07<00:00,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"array must not contain infs or NaNs\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|| 40/43 [00:07<00:00,  4.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"array must not contain infs or NaNs\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|| 41/43 [00:07<00:00,  4.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"array must not contain infs or NaNs\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 43/43 [00:08<00:00,  5.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"array must not contain infs or NaNs\")\n",
            "\"array must not contain infs or NaNs\")\n",
            "Rheobase Analyisis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 38/38 [00:45<00:00,  1.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fireing Rate Gain...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 38/38 [01:04<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpikeLatency...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 33/33 [00:13<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IClamp Input Resistance...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 28/28 [00:06<00:00,  4.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IV Curve...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|         | 2/27 [00:00<00:02, 12.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n",
            "[]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|        | 5/27 [00:00<00:03,  6.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|       | 7/27 [00:01<00:03,  5.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|       | 8/27 [00:01<00:04,  3.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|      | 10/27 [00:02<00:05,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|      | 11/27 [00:02<00:04,  3.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|     | 13/27 [00:03<00:05,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|    | 15/27 [00:03<00:03,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|   | 17/27 [00:04<00:02,  4.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|   | 19/27 [00:04<00:01,  4.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|  | 21/27 [00:05<00:01,  5.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%| | 23/27 [00:05<00:00,  5.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|| 25/27 [00:05<00:00,  5.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 27/27 [00:06<00:00,  4.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name =  '_hipp_data/2022_08_12_RNF182/2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0004.abf'\n",
        "file_name =  '_hipp_data/2022_08_12_RNF182/2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0007.abf'\n",
        "\n",
        "spike_args = {'spike_thresh':10,\n",
        "                'high_dv_thresh': 25,\n",
        "                'low_dv_thresh': -5,\n",
        "                    'window_ms': 2}\n",
        "\n",
        "R2_thresh = 0.8\n",
        "to_plot = 1\n",
        "\n",
        "gain_slope, R2, stim_currents, spike_counts, v_before_stim, fire_dur = analyze_gain_abf(file_name,spike_args,R2_thresh,to_plot) \n",
        "\n",
        "print(gain_slope, R2, stim_currents, spike_counts, v_before_stim, fire_dur)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "zuMZySTlMWhk",
        "outputId": "b06cbb6d-4989-4ad7-8b57-14a3cbffb714"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.012 0.8999999999999999 [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.] [0 0 0 0 1 1 2 3 3 6] [-64.3326 -65.8051 -65.4504 -64.8875 -69.5118 -69.7436 -69.1449 -68.4314\n",
            " -68.7292 -68.6359] [   nan    nan    nan    nan    nan    nan 0.8235 0.9386 0.7678 0.8958]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############## Fireing Rate Gain ################\n",
        "print('Fireing Rate Gain...')\n",
        "IFcurve_aliases = ['IC - Gain - D20pA', 'IC - Gain - D50pA']\n",
        "abf_recordings_df = IF_curve_analysis(abf_recordings_df, IFcurve_aliases,to_plot = 1)"
      ],
      "metadata": {
        "id": "2RvwLx1dGupP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abf_recordings_df\n",
        "\n",
        "# cell_list = []\n",
        "# abf_recordings_df.sort_values('file_name',inplace=True)\n",
        "# for f in abf_recordings_df.index:\n",
        "#     cell_id = f[f.rfind('/')+1:] # drop directory info\n",
        "#     cell_id = cell_id[:cell_id.find('.abf')] # drop file extension\n",
        "#     cell_id = cell_id[:cell_id.rfind('_')] # drop rec number\n",
        "#     abf_recordings_df.at[f,'cell_id'] = cell_id # write Id\n",
        "#     cell_list.append(cell_id) # keep a list\n",
        "\n",
        " \n",
        "# # populate new cell based dataframe\n",
        "# cell_list = list(set(cell_list))\n",
        "# cell_df = pd.DataFrame( {'cell_id': cell_list}).set_index('cell_id')\n",
        "# verbose = False\n",
        "\n",
        "# # LABELING\n",
        "# for c in cell_df.index:\n",
        "#     dashes = [i for i in range(len(str(c))) if '_' in str(c)[i] ]\n",
        "#     if verbose: _ = [print('   '+ c[:d]) for d in dashes   ]\n",
        "#     date = c[:dashes[0]].lower()\n",
        "#     virus = c[dashes[0]+1:dashes[1]].upper()\n",
        "#     genotype = c[dashes[1]+1:dashes[2]].upper()\n",
        "#     sex = c[dashes[2]+1:dashes[3]]\n",
        "#     age =c[dashes[3]+1:dashes[4]].upper()\n",
        "#     slice_num = c[dashes[4]+1:dashes[5]].upper()\n",
        "#     cell_num = c[dashes[5]+1:].upper()\n",
        "#     cell_type = c[dashes[6]+1:].upper()\n",
        "\n",
        "#     age = int(age[1:])\n",
        "\n",
        "#     # keeps\n",
        "#     cell_df.at[c,'Date'] = date\n",
        "#     cell_df.at[c,'virus'] = virus\n",
        "#     cell_df.at[c,'geno'] = genotype\n",
        "#     cell_df.at[c,'age (days)'] = age\n",
        "#     cell_df.at[c,'sex'] = sex\n",
        "#     cell_df.at[c,'slice'] = slice_num\n",
        "#     cell_df.at[c,'cell_type'] = cell_type\n",
        "#     if all([age>=(7*30), age<((9+1)*30)]):\n",
        "#         cell_df.at[c,'Age_bin'] = '7_to_9'\n",
        "#     elif all([age>=(17*30), age<((19+1)*30)]):\n",
        "#         cell_df.at[c,'Age_bin'] = '17_to_19'\n",
        "#     elif all([age>=(2*30), age<((5+1)*30)]): \n",
        "#         cell_df.at[c,'Age_bin'] = '2_to_5'\n",
        "#     else:\n",
        "#         cell_df.at[c,'Age_bin'] = 'other'\n",
        "\n",
        "# date_to_animal = {}\n",
        "# set_list = list(set(cell_df['Date']))\n",
        "# for i in range(len(set_list)):\n",
        "#     d = set_list[i]\n",
        "#     date_to_animal[d] = i\n",
        "\n",
        "# for c in cell_df.index:\n",
        "#     d = cell_df.at[c,'Date']\n",
        "#     cell_df.at[c,'Animal'] = date_to_animal[d]\n",
        "\n",
        "\n",
        "# #### Actual Values\n",
        "# cell_df['rec_list'] = None\n",
        "# for c in cell_df.index:\n",
        "#     is_cell = abf_recordings_df['cell_id'] == c\n",
        "#     cell_recs = list(abf_recordings_df.index[is_cell])\n",
        "#     cell_df.at[c,'rec_list'] = cell_recs\n",
        "\n",
        "\n",
        "data_columns = abf_recordings_df.columns\n",
        "data_columns = [c for c in data_columns if 'cell_id' not in c]\n",
        "for dc in data_columns:\n",
        "    cell_df[dc] = None\n",
        "    for c in cell_df.index:\n",
        "        is_cell = abf_recordings_df['cell_id'] == c\n",
        "        cell_data = list(abf_recordings_df.loc[is_cell,dc])\n",
        "        if not(type(cell_data[0]) == str):\n",
        "            # remove recs that report 'none'\n",
        "            cd = cell_data[0]\n",
        "            cell_data = [cd for cd in cell_data if not( str(type(cd)) == str(type(None)) )]\n",
        "            # remove recs that report 'nan' but check if its a float first because np.isnan is a little bitch.\n",
        "            float_data = []\n",
        "            for cd in cell_data:\n",
        "                if str(type(cd)) == \"<class 'float'>\":\n",
        "                    if np.isnan(cd):\n",
        "                        'skip'\n",
        "                    else:\n",
        "                        float_data.append(cd)\n",
        "                else:\n",
        "                    float_data.append(cd)\n",
        "\n",
        "            cell_data = float_data\n",
        "            # print('new',cell_data)\n",
        "            \n",
        "        cell_df.at[c,dc] = np.array(cell_data)\n",
        "\n",
        "for c in cell_df.index:\n",
        "    prots = cell_df.at[c,'protocol']\n",
        "    cell_df.at[c,'protocol'] = set(prots)\n",
        "\n",
        "print(len(cell_df),' cells from',len(set(cell_df['Date'])),'animals')\n",
        "\n",
        "\n",
        "print(cell_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4dtehBkGgZ4",
        "outputId": "5909f54f-864b-4b03-c0d7-d82a55e54d85"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30  cells from 4 animals\n",
            "Index(['Date', 'virus', 'geno', 'age (days)', 'sex', 'slice', 'cell_type',\n",
            "       'Age_bin', 'Animal', 'rec_list', 'Recording_name', 'Rec_date', 'Virus',\n",
            "       'GenoType', 'Sex', 'Age', 'Slice_Num', 'Cell_num', 'Cell_Type',\n",
            "       'protocol', 'abf_timestamp', 'channelList', 'passing_sweeps', 'Ra_10.0',\n",
            "       'Ra_160.0', 'Rm_10.0', 'Rm_160.0', 'tau_10.0', 'tau_160.0', 'Cmq_10.0',\n",
            "       'Cmq_160.0', 'Cmf_10.0', 'Cmf_160.0', 'Cmqf_10.0', 'Cmqf_160.0',\n",
            "       'Cm_pc_10.0', 'Cm_pc_160.0', 'Rheobase_(pA)', 'step_resolution_(pA)',\n",
            "       'Rheo Ihold_(pA)', 'Vhold_Rheo_(mV)', 'AP_Threshold(mV)',\n",
            "       'Gain_Stims_pA', 'Gain_NumSpikes', 'v_before_stim', 'Firing_Duration_%',\n",
            "       'Firing_Gain_(Hz/pA)', 'R2 (Firing_Gain_R2)', 'Spike_Latency_(ms)',\n",
            "       'V_hold_(Latency)', 'Rinput_(MO)', 'Cmf_IC_(pF)', 'IV_stats',\n",
            "       'IV_I_peak', 'IV_V_stim', 'IV_I_mean'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ser = cell_df['Gain_Stims_pA']\n",
        "for r in ser.index:\n",
        "    a = ser[r]\n",
        "    print(r, type(a), [type(b) for b in a]  )\n",
        "    _ = [print('   ', b) for b in a]\n",
        "ser = cell_df['Gain_NumSpikes']\n",
        "for r in ser.index:\n",
        "    a = ser[r]\n",
        "    print(r, type(a), [type(b) for b in a]  )\n",
        "    _ = [print('   ', b) for b in a]"
      ],
      "metadata": {
        "id": "pWsLwtqvTrhV",
        "outputId": "db2f95a9-2290-4061-c18b-f437f777373f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022x08x12_RNF182_E4KI_F_P251_s001_c004_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s004_c010_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s002_c002_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400.]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s002_c007_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s003_c003_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s002_c008_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s001_c004_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s004_c011_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s002_c006_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>, <class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x15_RNF182_E4KI_F_P251_s001_c001_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s002_c008_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s004_c009_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s001_c002_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s003_c005_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s001_c003_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s001_c005_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s004_c007_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s001_c002_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s002_c009_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s002_c007_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s004_c008_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s002_c006_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s001_c001_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s001_c003_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s003_c006_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>, <class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>, <class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400.]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s001_c005_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>, <class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s003_c004_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "2022x08x16_RNF182_E4KI_F_P251_s002_c002_CA3xPOS <class 'numpy.ndarray'> []\n",
            "2022x08x12_RNF182_E4KI_F_P251_s001_c004_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  6 18 30 41 51 61 67 68]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s004_c010_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  1  1  6  9 16 19 23 20]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s002_c002_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  6 14 20 26 32 37 42]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s002_c007_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  0  2  8 10 11 10 12 13]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s003_c003_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  1  5  9 16 24 30 38 43 52]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s002_c008_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  0  0  7 13 22 35 42 50]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s001_c004_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  3 18 31 39 48 56 56 47]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s004_c011_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  0  0  5 10 14 23 32 39]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s002_c006_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>, <class 'numpy.ndarray'>]\n",
            "    [ 0  0  1  3  5  8 11 17 19 20]\n",
            "    [ 0  0  1  6 12 18 24 28 33 37]\n",
            "2022x08x15_RNF182_E4KI_F_P251_s001_c001_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  1  9 16 25 29 26 24 20]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s002_c008_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  1  6 10 13 16 18 20  6  0]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s004_c009_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  1 11 23 26 33 43 50 52 59]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s001_c002_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  3 10 15 20 28 33 39 47]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s003_c005_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [  0   3  22  44  64  77  92 100  88  82]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s001_c003_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  3 12 19 27 35 42 50 50]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s001_c005_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  9 17 27 36 41 50 51 51 51]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s004_c007_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  8 22 32 45 52 58 61 63 53]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s001_c002_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  2  7 13 16 21 28 30 35]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s002_c009_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  0  2  5  8 12 14 16 20]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s002_c007_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  2  8 15 21 26 32 35 34]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s004_c008_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  1  5 11 18 23 32 42 46]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s002_c006_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0 10 22 30 30 25 29 27 27]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  1  4  9 16 20 24 28 31]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s001_c001_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  0  5 21 37 50 58 64 48]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s001_c003_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  0  9 23 36 48 61 66 73 82]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s003_c006_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>, <class 'numpy.ndarray'>]\n",
            "    [ 0  0  0  6 15 26 37 44 57 61]\n",
            "    [ 0  2 16 32 42 56 64 74 76 68]\n",
            "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS <class 'numpy.ndarray'> [<class 'numpy.ndarray'>, <class 'numpy.ndarray'>]\n",
            "    [0 0 0 0 0 1 1 0 0]\n",
            "    [0 0 0 0 1 1 2 3 3 6]\n",
            "2022x08x17_RNF182_E4KI_F_P256_s001_c005_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>, <class 'numpy.ndarray'>]\n",
            "    [ 0  0  2 13 23 33 45 54 65 74]\n",
            "    [ 0  0  9 23 35 46 60 71 80 85]\n",
            "2022x08x16_RNF182_E4KI_F_P255_s003_c004_CA3xNEG <class 'numpy.ndarray'> [<class 'numpy.ndarray'>]\n",
            "    [ 0  4 16 29 40 55 69 76 81 83]\n",
            "2022x08x16_RNF182_E4KI_F_P251_s002_c002_CA3xPOS <class 'numpy.ndarray'> []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = '_hipp_data/2022_08_12_RNF182/2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0004.abf'\n",
        "file_name = '_hipp_data/2022_08_12_RNF182/2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0007.abf'\n",
        "# abf = pyabf.ABF( file_name )\n",
        "# is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "# stim_currents, spike_counts, spike_rates,_,v_before_stim, fire_dur = spikes_per_stim(abf,spike_args, mode='count', to_plot=to_plot)\n",
        "# if_fit = fit_firing_gain( stim_currents, spike_counts, spike_rates ,to_plot=to_plot>0)\n",
        "# print(if_fit)\n",
        "\n",
        "\n",
        "result =  analyze_gain_abf(file_name,spike_args,R2_thresh = 0.8, to_plot = 0) \n",
        "_ = [print(x) for x in result]"
      ],
      "metadata": {
        "id": "AgxNQo2cfYYX",
        "outputId": "fe36bee9-3445-44a7-a541-7ea965c37d39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.012\n",
            "0.8999999999999999\n",
            "[  0.  50. 100. 150. 200. 250. 300. 350. 400. 450.]\n",
            "[0 0 0 0 1 1 2 3 3 6]\n",
            "[-64.3326 -65.8051 -65.4504 -64.8875 -69.5118 -69.7436 -69.1449 -68.4314\n",
            " -68.7292 -68.6359]\n",
            "[   nan    nan    nan    nan    nan    nan 0.8235 0.9386 0.7678 0.8958]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "#################### Consolidate and Summarize Data ##################\n",
        "########################################################################\n",
        "cell_summary_df_clean, abrg_abf_recs_df = consolidate_store_data(abf_recordings_df)\n",
        "# display(abrg_abf_recs_df)"
      ],
      "metadata": {
        "id": "zN9Fzc9Qmagu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "13c298eb-a3ba-4267-fcfb-0de95b329b7f"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30  cells from 4 animals\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fbeb360d-5d15-4dd8-8532-b91be4b82f3b\", \"abrg_abf_recs_df.csv\", 83751)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5d25fb09-94e9-4862-a6c4-68f9ffdb5439\", \"abf_recordings_df_tagged.csv\", 83751)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a2057ac2-63c5-4634-ab53-e3ad8b8dd309\", \"cell_summary_df.csv\", 63897)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error on  2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS\n",
            "[array([0, 0, 0, 0, 0, 1, 1, 0, 0]) array([0, 0, 0, 0, 1, 1, 2, 3, 3, 6])]\n",
            "[1, 6]\n",
            "c Firing_Gain_(Hz/pA)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cdb05e12-ef9d-4652-a57d-c119ed8cca8c\", \"cell_summary_df_clean.csv\", 33474)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_col(df_old,col_name,verbose=False,default_names=True):\n",
        "    'Blow up composite columns into individual columns'\n",
        "    df = df_old.copy()\n",
        "    ser_to_exp = df[col_name]\n",
        "    def safe_len(x):\n",
        "        try: return len(x)\n",
        "        except: return 0\n",
        "    \n",
        "    new_col_num = np.max( [safe_len(i) for i in ser_to_exp])\n",
        "    new_names = [col_name +'_'+str(i) for i in range(new_col_num)]\n",
        "    if default_names:\n",
        "        new_names = [col_name +'_'+str(i) for i in range(new_col_num)]\n",
        "    else:\n",
        "        new_names = default_names\n",
        "    \n",
        "    for ni in range(new_col_num):\n",
        "        name = new_names[ni]\n",
        "        df[name] = np.nan\n",
        "    for row in df.index:\n",
        "        for i in range(safe_len(df.loc[row,col_name])):\n",
        "            df.at[row,new_names[i]] = df.loc[row,col_name][i]\n",
        "\n",
        "    \n",
        "    old_cols = list(df_old.columns)\n",
        "    old_col_ind = [i for i in range(len(old_cols)) if col_name in old_cols[i]][0]\n",
        "\n",
        "    \n",
        "    new_order = old_cols[:old_col_ind] + new_names + old_cols[old_col_ind+1:]\n",
        "    df = df[new_order]\n",
        "    if verbose: _ = [print(c) for c in cell_summary_exl_friendly.columns]\n",
        "    return df\n",
        "\n",
        "\n",
        "cell_summary_exl_friendly = cell_summary_df_clean.copy()\n",
        "cell_summary_exl_friendly = expand_col(cell_summary_exl_friendly,'Gain_Stims_pA')\n",
        "cell_summary_exl_friendly = expand_col(cell_summary_exl_friendly,'Gain_NumSpikes')\n",
        "cell_summary_exl_friendly.to_csv('cell_summary_exl_friendly.csv')\n",
        "files.download('cell_summary_exl_friendly.csv')"
      ],
      "metadata": {
        "id": "BbtXnXhPvD8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cell_summary_exl_friendly.columns)"
      ],
      "metadata": {
        "id": "wf5PGzqBmhhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### PCA CLUSTER ##########\n",
        "\n",
        "# _ = [print(c,cell_summary_exl_friendly.columns[c]) for c in range(len(cell_summary_exl_friendly.columns))]\n",
        "col_inds_for_pca = [9] + [12] + list(range(13,17)) + [18] + list(range(31,40))\n",
        "cols_for_pca = cell_summary_exl_friendly.columns[col_inds_for_pca]\n",
        "\n",
        "cols_for_pca = [ 'Ra_160.0', 'Rm_160.0','Cmq_10.0', 'Cmq_160.0','Spike_Latency_(ms)', 'Rheobase_(pA)',\n",
        "                'AP_Threshold(mV)', 'Rinput_(MO)', 'Firing_Gain_(Hz/pA)',\n",
        "                'Firing_Duration_%' ] #,'Gain_NumSpikes_0', 'Gain_NumSpikes_1',\n",
        "                # 'Gain_NumSpikes_2', 'Gain_NumSpikes_3', 'Gain_NumSpikes_4',\n",
        "                # 'Gain_NumSpikes_5', 'Gain_NumSpikes_6', 'Gain_NumSpikes_7',\n",
        "                # 'Gain_NumSpikes_8', 'Gain_NumSpikes_9']\n",
        "\n",
        "\n",
        "\n",
        "# _ = [print(c) for c in cols_for_pca]\n",
        "df = cell_summary_exl_friendly[cols_for_pca]\n",
        "# display(df)\n",
        "X_ = df.to_numpy(dtype='float32')\n",
        "nan_bool = np.logical_not(np.isnan(X_))\n",
        "row_nan = np.all(nan_bool,axis=1)\n",
        "df = df[row_nan]\n",
        "# display(df)\n",
        "X_ = X_[row_nan,:]\n",
        "# print(X_)\n",
        "\n",
        "\n",
        "import sklearn.decomposition\n",
        "import sklearn.cluster\n",
        "\n",
        "pca = sklearn.decomposition.PCA(n_components=np.min(X_.shape)).fit(X_)\n",
        "print(np.cumsum(pca.explained_variance_ratio_))\n",
        "pca = sklearn.decomposition.PCA(n_components=3).fit(X_)\n",
        "print(np.cumsum(pca.explained_variance_ratio_))\n",
        "\n",
        "X_trans = pca.transform(X_)\n",
        "X_red = pca.inverse_transform(X_trans)\n",
        "\n",
        "\n",
        "RANDOM_SEED = 5\n",
        "kmeans = sklearn.cluster.KMeans(n_clusters=2,random_state=RANDOM_SEED).fit(X_trans)\n",
        "Centroids_red = pca.inverse_transform(kmeans.cluster_centers_)\n",
        "null_kmeans = sklearn.cluster.KMeans(n_clusters=1).fit(X_trans)\n",
        "null_centoid =  pca.inverse_transform(null_kmeans.cluster_centers_)[0]\n",
        "# print(Centroids_red)\n",
        "# print(null_centoid)\n",
        "\n",
        "type_0_cent = 100*(Centroids_red[0] / null_centoid - 1)\n",
        "type_1_cent = 100*(Centroids_red[1] / null_centoid - 1)\n",
        "\n",
        "X_rel = (100*(X_ / null_centoid - 1))\n",
        "\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(12, 6))\n",
        "x = np.arange(len(cols_for_pca))*-1\n",
        "\n",
        "wid = .3\n",
        "\n",
        "\n",
        "is_type_0 = kmeans.labels_ == 0\n",
        "is_type_1 = kmeans.labels_ == 1\n",
        "X_at0 = X_rel[is_type_0,:]  \n",
        "X_at1 = X_rel[is_type_1,:] \n",
        "\n",
        "\n",
        "axs[0].barh(x-wid/2, type_0_cent, wid,color = 'turquoise',label='Ephys type_0'+' n=' +str(sum(is_type_0)))\n",
        "axs[0].barh(x+wid/2, type_1_cent, wid,color = 'magenta',label='Ephys type_1'+' n=' +str(sum(is_type_1)))\n",
        "\n",
        "x_exp0 = np.repeat(np.expand_dims(x, axis=0),repeats =sum(is_type_0), axis=0)\n",
        "axs[1].plot(X_at0.transpose(), x_exp0.transpose(), '-.',color = 'turquoise')\n",
        "\n",
        "x_exp1 = np.repeat(np.expand_dims(x, axis=0),repeats =sum(is_type_1), axis=0)\n",
        "axs[1].plot(X_at1.transpose(), x_exp1.transpose(), '-.',color = 'magenta' )\n",
        "\n",
        "for ax in axs:\n",
        "    axs[0].legend()\n",
        "    ax.set_yticks(x)\n",
        "    axs[0].set_yticklabels(cols_for_pca, rotation=0)\n",
        "    ax.set_xlabel('% Diff Global Mean')\n",
        "plt.show()\n",
        "\n",
        "fig.savefig('clusters.png',dpi=300)\n",
        "files.download('clusters.png')"
      ],
      "metadata": {
        "id": "Bvw7XZ7fz13F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X_trans)\n",
        "# print(X_trans[is_type_0])\n",
        "\n",
        "fig, ax = plt.subplots(3,3,figsize = [4,4])\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        ax[i,j].scatter(X_trans[is_type_0][:,i],X_trans[is_type_0][:,j],color='turquoise')\n",
        "        ax[i,j].scatter(X_trans[is_type_1][:,i],X_trans[is_type_1][:,j],color='magenta')\n"
      ],
      "metadata": {
        "id": "OlkBcnLBv3-O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}