{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crispy_Analyze_ABF_files.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMc7S2/rLr7/PvqOj2eNYNE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtabuena/CrispMice/blob/main/Crispy_Analyze_ABF_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZKwnij3xH0D7"
      },
      "outputs": [],
      "source": [
        "##### Git Test\n",
        "################ Helpers ######################\n",
        "def print_assert():\n",
        "    'Print assertion failure messages (typicaly when try/except is employed)'\n",
        "    import sys\n",
        "    import traceback\n",
        "    _, _, tb = sys.exc_info()\n",
        "    # traceback.print_tb(tb) # Fixed format\n",
        "    tb_info = traceback.extract_tb(tb)\n",
        "    filename, line, func, text = tb_info[-1]\n",
        "    print(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def flatten(list_of_lists):\n",
        "    'Recursively Flatten list of lists'\n",
        "    if len(list_of_lists) == 0:\n",
        "        return list_of_lists\n",
        "    if isinstance(list_of_lists[0], list):\n",
        "        return flatten(list_of_lists[0]) + flatten(list_of_lists[1:])\n",
        "    return list_of_lists[:1] + flatten(list_of_lists[1:])\n",
        "\n",
        "\n",
        "def protocol_baseline_and_stim(abf):\n",
        "    'Return two boolean arrays, distiguishing holding I/V and electrical stimuli'\n",
        "    # use command signal variance to determine stimulus periods\n",
        "    commands = []\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(sweepNumber=s)\n",
        "        commands.append(abf.sweepC)\n",
        "    commands = np.stack(commands)\n",
        "    \n",
        "    std = np.std(commands, axis=0)\n",
        "    is_base = std==0\n",
        "    is_stim = np.logical_not(is_base)\n",
        "    return is_base, is_stim\n",
        "\n",
        "def abf_or_name(abf):\n",
        "    if str(type(abf))==\"<class 'str'>\":\n",
        "        abf = pyabf.ABF( abf )\n",
        "    return abf\n",
        "\n",
        "def plot_sweeps_and_command(abf,figsize = [8,2],windows=[]):\n",
        "    'Plot an abf file with sweeps and command'\n",
        "    'also attempts to calibrate telegraph offset when a Ch1 is a secondary ouput of the amp'\n",
        "    abf = abf_or_name(abf)\n",
        "    axs_2_right = []\n",
        "    # plot sweeps and command (single channel)\n",
        "\n",
        "\n",
        "    discretized_cmap = matplotlib.cm.get_cmap('viridis', len(abf.sweepList))\n",
        "\n",
        "    num_ch = 1;\n",
        "    if len(abf.channelList)>1:\n",
        "        num_ch = 2\n",
        "    fig, axs = plt.subplots(num_ch, figsize = np.array(figsize)*np.array([1,num_ch]))\n",
        "    if num_ch ==1:\n",
        "        axs=[axs]\n",
        "    theta, offset, correct_ch1 = predict_telegraph(abf)\n",
        "    if num_ch==2:\n",
        "        axs1_r = axs[1].twinx()\n",
        "        axs[1].set_ylabel(str('Ch1 '+str(abf.sweepLabelC)))\n",
        "        axs1_r.set_ylabel(str('Corrrected '+str(abf.sweepLabelC)[-4:]) )\n",
        "        axs[1].set_title(str(theta) +','+ str(offset))\n",
        "        axs[1].set_title('Command Correction')\n",
        "    axs0_r = axs[0].twinx()\n",
        "    for sweep in abf.sweepList:\n",
        "        abf.setSweep(sweepNumber=sweep)\n",
        "        axs[0].plot(abf.sweepX, abf.sweepY,color=discretized_cmap(sweep))        \n",
        "        axs0_r.plot(abf.sweepX, abf.sweepC,color='grey' )#discretized_cmap(sweep),linestyle='dotted')\n",
        "        com = abf.sweepC\n",
        "        if len(abf.channelList)>1:\n",
        "            abf.setSweep(sweepNumber=sweep, channel=1)\n",
        "            axs[1].plot(abf.sweepX, abf.sweepY,'m')\n",
        "            corrected_com = correct_ch1(theta,abf.sweepY)\n",
        "            axs1_r.plot(abf.sweepX, corrected_com,'c',linestyle='dotted')\n",
        "            # axs_2.set_title( str(theta)+','+ str(correct_ch1(theta,abf.sweepY[0])) )\n",
        "    axs[0].set_title(abf.abfID)\n",
        "    abf.setSweep(0)\n",
        "    axs[0].set_ylabel(str(abf.sweepLabelY))\n",
        "    axs[0].set_xlabel(str(abf.sweepLabelX))\n",
        "    axs0_r.set_ylabel(str(abf.sweepLabelC))\n",
        "    axs0_r.set_xlabel(str(abf.sweepLabelX))\n",
        "\n",
        "    cmap = matplotlib.cm.get_cmap('Dark2')\n",
        "    color_num=0\n",
        "    for w in windows:\n",
        "        color_num+=1\n",
        "        rgba = cmap(color_num/len(windows))\n",
        "        ylim = axs[0].get_ylim()\n",
        "        sr = np.mean(np.diff( w ))\n",
        "        gaps = np.where(np.diff(w)>sr*4)[0]\n",
        "        sw = flatten([w[0],[ list(w[[i,i+1]]) for i in np.arange(len(w)-1) if np.diff(w)[i] > sr*3 ] , w[-1] ])\n",
        "        for i in range(0, len(sw), 2):\n",
        "            axs[0].axvspan(sw[i], sw[i+1], ylim[0], ylim[1], alpha=0.2,color=rgba)\n",
        "\n",
        "\n",
        "    axs[0].set_zorder(1)  # default zorder is 0 for ax1 and ax2\n",
        "    axs[0].patch.set_visible(False)  # prevents ax1 from hiding ax2\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    return fig, axs , theta\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################# Math ######################\n",
        "import math\n",
        "\n",
        "\n",
        "def movmean(x, w):\n",
        "    'A moving mean filter'\n",
        "    w = int(w)\n",
        "    # plt.plot(np.arange(len(x)) - int(len(x)/2), x)\n",
        "    if len(x) < w: w = len(x)\n",
        "    px = np.pad(x,int(np.ceil((w-1)/2)),'edge') \n",
        "    if len(px)-len(x) == w: px = px[0:-1]\n",
        "    conv = np.convolve(px, np.ones(w), 'valid') / w\n",
        "    return conv\n",
        "\n",
        "def mono_exp(time, peak, tau, ss):\n",
        "    'A single exponential decay function'\n",
        "    return (peak * np.exp(-time / (tau)) + ss)\n",
        "\n",
        "def rms_noise(x):\n",
        "    return np.sqrt(    np.sum((x-x.mean())**2)/len(x)    )   \n",
        "\n",
        "def predict_telegraph_DEFUNCT(abf,to_plot=False,stabilize_one_to_one = True,v_res = 10):\n",
        "    'Uses the difference between the voltage protocol command and'\n",
        "    'Secondary ouput channel to calcualte the offset and gain between'\n",
        "    'amplifier and digitizer'\n",
        "    abf = abf_or_name(abf)\n",
        "    theta = [1,0]\n",
        "    try:\n",
        "        if len(abf.channelList)>1:\n",
        "            all_ch1_y =[]\n",
        "            all_ch0_com =[]\n",
        "            for sweep in abf.sweepList:\n",
        "                abf.setSweep(sweepNumber=sweep, channel=1)\n",
        "                all_ch1_y.append(abf.sweepY)\n",
        "                abf.setSweep(sweepNumber=sweep, channel=0)\n",
        "                all_ch0_com.append(abf.sweepC)\n",
        "            all_ch1_y = np.concatenate(all_ch1_y)\n",
        "            all_ch0_com = np.concatenate(all_ch0_com)\n",
        "            theta = np.polyfit(all_ch0_com, all_ch1_y, 1)\n",
        "            # print(theta,'theta_1')\n",
        "            modes = []\n",
        "            steps = list(set(all_ch0_com))\n",
        "            steps.sort()\n",
        "            steps = np.array(steps)\n",
        "            for s in steps:\n",
        "                mode = scipy.stats.mode( all_ch1_y[all_ch0_com==s] )[0]\n",
        "                modes.append(mode)\n",
        "            modes = np.concatenate(modes)\n",
        "            m_ind = [i for i in np.arange(len(modes)) if modes[i]<-40]\n",
        "            steps = steps[m_ind]\n",
        "            modes = modes[m_ind]\n",
        "            theta = np.polyfit(steps, modes, 1)\n",
        "            # print(theta,'theta_2')\n",
        "            if to_plot:\n",
        "                plt.scatter(steps,modes)\n",
        "                y_hat = steps*theta[0]+theta[1]\n",
        "                plt.plot(steps,  y_hat )\n",
        "                plt.show()\n",
        "            if to_plot:\n",
        "                for s in steps:\n",
        "                    counts, bin_edges = np.histogram( all_ch1_y[all_ch0_com==s],bins=50,range=(-120,70),density=True )\n",
        "                    plt.plot(bin_edges[1:],counts)\n",
        "                    mode = scipy.stats.mode( all_ch1_y[all_ch0_com==s] )\n",
        "                    # plt.scatter(s,mode)\n",
        "                plt.gca().xaxis.set_major_locator(plt.MultipleLocator(10))\n",
        "                plt.gca().grid()\n",
        "                plt.show()\n",
        "    except:\n",
        "        'keep default theta'\n",
        "    if stabilize_one_to_one:\n",
        "        # if np.round(theta[0],1)==1.0:\n",
        "        theta = [theta[0] , round(theta[1]/v_res)*v_res ]\n",
        "        if theta[0]>1: theta[0]=math.floor(theta[0])\n",
        "\n",
        "    offset = theta[1]/theta[0]\n",
        "    def correct_ch1(t,signal):\n",
        "        return t[1]/t[0] + (signal - t[1]) / t[0]\n",
        "    return theta, offset, correct_ch1\n",
        "\n",
        "def predict_telegraph(abf,to_plot=False,stabilize_one_to_one = True,v_res = 10):\n",
        "    'Uses the difference between the voltage protocol command and'\n",
        "    'Secondary ouput channel to calcualte the offset and gain between'\n",
        "    'amplifier and digitizer'\n",
        "    abf = abf_or_name(abf)\n",
        "    theta = [1,0]\n",
        "    try:\n",
        "        if len(abf.channelList)>1:\n",
        "            all_ch1_y =[]\n",
        "            all_ch0_com =[]\n",
        "            for sweep in abf.sweepList:\n",
        "                abf.setSweep(sweepNumber=sweep, channel=1)\n",
        "                all_ch1_y.append(abf.sweepY)\n",
        "                abf.setSweep(sweepNumber=sweep, channel=0)\n",
        "                all_ch0_com.append(abf.sweepC)\n",
        "            all_ch1_y = np.concatenate(all_ch1_y)\n",
        "            all_ch0_com = np.concatenate(all_ch0_com)\n",
        "\n",
        "            test_theta = np.polyfit(all_ch0_com, all_ch1_y, 1)\n",
        "            y_hat = test_theta[1]/test_theta[0] + (all_ch0_com - test_theta[1]) / test_theta[0]\n",
        "            err = abs(y_hat-all_ch1_y)\n",
        "            small_err = err<(np.mean(err)+0.2*np.std(err))\n",
        "            if to_plot:\n",
        "                plt.scatter(all_ch0_com[small_err],all_ch1_y[small_err])\n",
        "                plt.show()\n",
        "            theta = np.polyfit(all_ch0_com[small_err], all_ch1_y[small_err], 1)\n",
        "    except:\n",
        "        'keep default theta'\n",
        "    offset = theta[1]/theta[0]\n",
        "    def correct_ch1(theta,signal):\n",
        "        return theta[1]/theta[0] + (signal - theta[1]) / theta[0]\n",
        "    return theta, offset, correct_ch1"
      ],
      "metadata": {
        "id": "IFLDC-cJGMWb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########## Importing ABFs From DropBox ################\n",
        "\n",
        "def get_drobox_folder(link, new_filename):\n",
        "    'Download a folder from dropbox and unzip'\n",
        "\n",
        "    suffix_start = new_filename.find(\".zip\")\n",
        "    new_filename_stripped = new_filename[0:suffix_start]\n",
        "    zipped_file_path = \"/content/\"+new_filename\n",
        "    unzipped_file_path = \"/content/\"+new_filename_stripped\n",
        "    if not( os.path.exists(zipped_file_path)):\n",
        "        !wget -O $new_filename $link    # download with new name\n",
        "    # if not( os.path.exists(new_filename_stripped)):\n",
        "    !echo A | unzip $zipped_file_path -d $unzipped_file_path \n",
        "    return new_filename_stripped\n",
        "\n",
        "def get_sub_files(rootdir):\n",
        "    'Recursively search subfolders and return a list of all files'\n",
        "    file_list =[]\n",
        "    for rootdir, dirs, files in os.walk(file_loc): \n",
        "            file_list.extend([os.path.join(rootdir,f) for f in files])\n",
        "    return file_list\n",
        "\n",
        "\n",
        "\n",
        "def catalogue_recs(file_loc):\n",
        "    'Read metadata from abf files stored in chosen folder and assigns'\n",
        "    'them to a dataframe for further processing. All further abf analyses'\n",
        "    'read files from this df and report values in the df.'\n",
        "\n",
        "    file_list = get_sub_files(file_loc)\n",
        "    # file_list = [file_loc+'/'+f for f in file_list]\n",
        "\n",
        "    file_list=[f for f in file_list if '.abf' in f]\n",
        "\n",
        "    abf_recordings_df = pd.DataFrame(data = file_list, columns=['file_name'])    \n",
        "    abf_recordings_df = abf_recordings_df.set_index('file_name')\n",
        "    abf_recordings_df[\"info\"] = None\n",
        "    abf_recordings_df[\"protocol\"] = None\n",
        "    abf_recordings_df[\"datetime\"] = None\n",
        "    abf_recordings_df[\"sweepList\"] = None\n",
        "    abf_recordings_df[\"channelList\"] = None\n",
        "\n",
        "\n",
        "    for r in np.arange(len(abf_recordings_df)):\n",
        "        row_filename = abf_recordings_df.index[r]\n",
        "        if '.sta' in row_filename:\n",
        "            continue\n",
        "        abf = pyabf.ABF(row_filename)\n",
        "        abf_recordings_df.loc[row_filename,'info'] = str(abf)\n",
        "        abf_recordings_df.loc[row_filename,'protocol'] = abf.protocol\n",
        "        abf_recordings_df.at[row_filename,'sweepList'] = abf.sweepList\n",
        "        abf_recordings_df.at[row_filename,'channelList'] = abf.channelList\n",
        "        abf_recordings_df.at[row_filename,'datetime'] = abf.abfDateTimeString\n",
        "    abf_recordings_df.sort_values('file_name',inplace=True)\n",
        "    protocol_set = list(set(abf_recordings_df['protocol']))\n",
        "    return abf_recordings_df, protocol_set\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_pulses(command):\n",
        "    'Searches a command signal for start and stop times'\n",
        "    'of stimuli'\n",
        "    is_base = command==command[0]\n",
        "    is_step = np.logical_not(is_base)\n",
        "    step_start = np.logical_and(is_base[:-1], is_step[1:])\n",
        "    step_stop = np.logical_and(is_step[:-1], is_base[1:])\n",
        "    starts = np.where(step_start)[0]\n",
        "    stops = np.where(step_stop)[0]\n",
        "    return starts, stops\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R67DNyK1GReQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "##########################################################\n",
        "##########  FIRING RATE GAIN ################\n",
        "\n",
        "def IF_curve_analysis(abf_recordings_df,protocol_aliases,R2_thresh = 0.8, to_plot = 0 ):\n",
        "    'Loop through all abfs finding approproate voltage protocols and calculates the firing rate gain, spikes per pA of all'\n",
        "\n",
        "    abf_recordings_df['Gain_Stims_pA'] = None\n",
        "    abf_recordings_df['Gain_NumSpikes'] = None\n",
        "    abf_recordings_df['Gain_Stims_pA'] = abf_recordings_df['Gain_Stims_pA'].astype(object)\n",
        "    abf_recordings_df['Gain_NumSpikes'] = abf_recordings_df['Gain_NumSpikes'].astype(object)\n",
        "    abf_recordings_df['v_before_stim'] = None\n",
        "\n",
        "    \n",
        "    spike_args = {'high_dv_thresh': 40,\n",
        "                  'low_dv_thresh': -5,\n",
        "                  'window_ms': 5}\n",
        "\n",
        "\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol] ):\n",
        "\n",
        "        gain_slope, R2, stim_currents, spike_counts, v_before_stim = analyze_gain_abf(file_name,spike_args,R2_thresh,to_plot) \n",
        "\n",
        "        abf_recordings_df.at[file_name,'Gain_Stims_pA'] = stim_currents\n",
        "        abf_recordings_df.at[file_name,'Gain_NumSpikes'] = spike_counts\n",
        "        abf_recordings_df.at[file_name,'v_before_stim'] = np.mean(v_before_stim)\n",
        "        abf_recordings_df.at[file_name,'Firing_Gain_(Hz/pA)'] = gain_slope\n",
        "        abf_recordings_df.at[file_name,'R2 (Firing_Gain_R2)'] = R2\n",
        "\n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "\n",
        "def analyze_gain_abf(file_name,spike_args,R2_thresh = 0.8, to_plot = 0):\n",
        "    # print(\"#\" , list(abf_recordings_df.index).index(file_name))\n",
        "    abf = pyabf.ABF( file_name )\n",
        "    if len(abf.sweepList)<5: #print( 'not enough sweeps')\n",
        "        return np.nan, np.nan, np.nan, np.nan, np.nan \n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "    stim_currents, spike_counts, spike_rates,_,v_before_stim = spikes_per_stim(abf,spike_args, mode='count', to_plot=to_plot>1)\n",
        "    if sum(spike_counts)==0:    #if no spikes return none\n",
        "        return np.nan, np.nan, np.nan, np.nan, np.nan \n",
        "    if_fit = fit_firing_gain( stim_currents, spike_counts, spike_rates ,to_plot=to_plot>0)\n",
        "    gain_slope = if_fit['slope']\n",
        "    R2 = if_fit['R2']\n",
        "    return gain_slope, R2, stim_currents, spike_counts, v_before_stim\n",
        "\n",
        "def spikes_per_stim(abf,spike_args,thresh=20,mode='count', to_plot=False):\n",
        "    'Loops through sweeps of and abf to find spikes'\n",
        "    # init\n",
        "    stim_currents = []\n",
        "    spike_rates = []\n",
        "    spike_counts = []\n",
        "    v_before_spike1 = []\n",
        "    v_before_stim = []\n",
        "    # get sweep info\n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "\n",
        "    # get spike per sweep\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s)\n",
        "        dVds, over_thresh, inds, mean_spike_rate = find_spike_in_trace(abf.sweepY,abf.sampleRate,spike_args,thresh=thresh,is_stim=is_stim,mode='count')\n",
        "        # plot id'd spikes\n",
        "        if to_plot:\n",
        "            fig, axs = plt.subplots(1)\n",
        "            axs.plot(abf.sweepX ,abf.sweepY)\n",
        "            axs.scatter(abf.sweepX[inds],abf.sweepY[inds],color='red')\n",
        "            plt.show()\n",
        "        # calc multi sweep params\n",
        "        stim_level = np.median(abf.sweepC[is_stim])\n",
        "        stim_currents.append(stim_level)\n",
        "        spike_rates.append(mean_spike_rate)\n",
        "        spike_counts.append(len(inds))\n",
        "        is_prestim = np.equal(np.cumsum( np.diff(is_base,prepend=1)),0)\n",
        "        v_before_stim.append( np.mean(abf.sweepY[is_prestim] ))\n",
        "\n",
        "        if len(inds)>0:\n",
        "            v_before_spike1.append(abf.sweepY[inds[0]])\n",
        "        else:\n",
        "            v_before_spike1.append(np.nan)\n",
        "\n",
        "    return np.array(stim_currents), np.array(spike_counts), np.array(spike_rates), np.array(v_before_spike1), np.array(v_before_stim)\n",
        "\n",
        "\n",
        "def find_spike_in_trace(trace,rate,spike_args,thresh=20,refract=0.005,is_stim = None ,mode='count',sanity_check=True,to_plot=False):\n",
        "    'Takes in a voltage trace from current clamp mode and uses derivative (dVds) to find action potentials.'\n",
        "    'Returns the dVds trace, boolean array indicating if dVds>threshold, inicies where dV crossed threshold,'\n",
        "    'and the mean firing rate given # spikes in trace of given length. Optional ways to count are:'\n",
        "    'isi (1/interspike interval) or count (spike count per second). Default is count'\n",
        "\n",
        "    high_dv_thresh = spike_args['high_dv_thresh']\n",
        "    low_dv_thresh = spike_args['low_dv_thresh']\n",
        "    window_ms = spike_args['window_ms']\n",
        "\n",
        "    if any(is_stim == None):\n",
        "        is_stim = [True for i in trace]\n",
        "    dVds = np.diff(trace, prepend=trace[0])*rate/1000\n",
        "    over_thresh = dVds>thresh\n",
        "    over_thresh[np.logical_not(is_stim)] = False\n",
        "    refract_window = int(np.round((refract*rate)))\n",
        "    inds = [t for t in np.arange(refract_window,len(over_thresh)) if all([over_thresh[t], all(over_thresh[t-refract_window:t]==False)])]    \n",
        "    if to_plot:\n",
        "        plt.plot(dVds)\n",
        "        plt.show()\n",
        "    if sanity_check:\n",
        "        old_inds = inds\n",
        "        inds = []\n",
        "        for i in old_inds:\n",
        "            samp_window = window_ms/1000 * rate\n",
        "            ind_range = np.arange(i-samp_window,i+samp_window).astype(int)\n",
        "            nearby_dVds = dVds[ind_range]\n",
        "            if False: print(i,'max', np.max(nearby_dVds))\n",
        "            if False: print(i,'min', np.min(nearby_dVds))\n",
        "            if np.max(nearby_dVds)>high_dv_thresh and np.min(nearby_dVds) < low_dv_thresh:\n",
        "                inds.append(i)\n",
        "                if False: print(inds)\n",
        "    if len(inds)<1:\n",
        "        mean_spike_rate = 0\n",
        "    else:\n",
        "        if mode=='isi':\n",
        "            mean_spike_rate = np.mean(rate/np.diff(inds))\n",
        "        elif mode=='count':\n",
        "            mean_spike_rate = len(inds)/(np.sum(is_stim)/rate)\n",
        "        else:\n",
        "            print('invalid mode. using default (count)')\n",
        "    return dVds, over_thresh, inds, mean_spike_rate\n",
        "\n",
        "def fit_firing_gain(stim_currents, spike_counts, spike_rates, to_plot=False):\n",
        "    'Gathers the firing rate of each stimuli and fits the linear portion of the curve to return the Gain in Hz/pA (the slope)'\n",
        "    is_pos_slope = np.diff(spike_counts,prepend=0)>0\n",
        "    is_pos_slope = movmean(np.diff(spike_counts,prepend=0),4)>0\n",
        "    peak_ind = np.where(spike_counts==np.max(spike_counts))[0]\n",
        "    if len(peak_ind)>1:\n",
        "        peak_ind = np.min(peak_ind)\n",
        "    \n",
        "    before_peak = np.arange(len(spike_counts))<peak_ind+2\n",
        "    is_nonzero = spike_counts>0\n",
        "    use_for_fit = np.logical_and.reduce((is_pos_slope,is_nonzero,before_peak))\n",
        "\n",
        "    if_fit = {}\n",
        "    if_fit['stim_currents'] = stim_currents\n",
        "    if_fit['spike_rates'] = spike_rates\n",
        "    if 0 == np.sum(spike_rates):\n",
        "        # print('no spikes detected')\n",
        "        if_fit['slope'] = np.nan\n",
        "        if_fit['intercept'] = np.nan\n",
        "        if_fit['R2'] = 0\n",
        "        return if_fit\n",
        "\n",
        "    if np.sum(spike_rates>0)<3:\n",
        "        # print('not enough spikes generated')\n",
        "        if_fit['slope'] = np.nan\n",
        "        if_fit['intercept'] = np.nan\n",
        "        if_fit['R2'] = 0\n",
        "        return if_fit\n",
        "\n",
        "    if_fit['slope'], if_fit['intercept'] , r_value, p_value, std_err = stats.linregress(stim_currents[use_for_fit], spike_rates[use_for_fit])\n",
        "    if_fit['R2'] = r_value**2\n",
        "\n",
        "    if to_plot:\n",
        "        fig, ax = plt.subplots(1)\n",
        "        ax.scatter( if_fit['stim_currents'] ,if_fit['spike_rates'] )\n",
        "        ax.plot( if_fit['stim_currents'], if_fit['slope']* if_fit['stim_currents']+if_fit['intercept'])\n",
        "        ax.scatter( if_fit['stim_currents'][use_for_fit] ,if_fit['spike_rates'][use_for_fit], color='r' )\n",
        "        ax.scatter(if_fit['stim_currents'][peak_ind],if_fit['spike_rates'][peak_ind], color='m')\n",
        "        ax.set_xlabel('current')\n",
        "        ax.set_ylabel('Spike Rate (Hz)')\n",
        "        (min,max) = ax.get_ylim()\n",
        "        ax.text(0, max/2, 'R**2='+str(round(if_fit['R2'],2)),fontsize='large')\n",
        "        plt.show()\n",
        "    return if_fit\n",
        "\n",
        "\n",
        "# def find_spike_in_trace(trace,rate,thresh=20,refract=0.005,is_stim = None ,mode='count',sanity_check=True):\n",
        "#     'Takes in a voltage trace from current clamp mode and uses derivative (dVds) to find action potentials.'\n",
        "#     'Returns the dVds trace, boolean array indicating if dVds>threshold, inicies where dV crossed threshold,'\n",
        "#     'and the mean firing rate given # spikes in trace of given length. Optional ways to count are:'\n",
        "#     'isi (1/interspike interval) or count (spike count per second). Default is count'\n",
        "\n",
        "#     if any(is_stim == None):\n",
        "#         is_stim = [True for i in trace]\n",
        "#     dVds = np.diff(trace, prepend=trace[0])*rate/1000\n",
        "#     over_thresh = dVds>thresh\n",
        "#     over_thresh[np.logical_not(is_stim)] = False\n",
        "#     refract_window = int(np.round((refract*rate)))\n",
        "#     inds = [t for t in np.arange(refract_window,len(over_thresh)) if all([over_thresh[t], all(over_thresh[t-refract_window:t]==False)])]    \n",
        "    \n",
        "#     if sanity_check:\n",
        "#         old_inds = inds\n",
        "#         inds = []\n",
        "#         for i in old_inds:\n",
        "#             time_window = 4 #ms\n",
        "#             samp_window = time_window/1000 * rate\n",
        "#             ind_range = np.arange(i-samp_window,i+samp_window).astype(int)\n",
        "#             nearby_dVds = dVds[ind_range]\n",
        "#             if False: print(i,'max', np.max(nearby_dVds))\n",
        "#             if False: print(i,'min', np.min(nearby_dVds))\n",
        "#             if np.max(nearby_dVds)>40 and np.min(nearby_dVds) < -20:\n",
        "#                 inds.append(i)\n",
        "#                 if False: print(inds)\n",
        "\n",
        "    \n",
        "#     if len(inds)<1:\n",
        "#         mean_spike_rate = 0\n",
        "#     else:\n",
        "#         if mode=='isi':\n",
        "#             mean_spike_rate = np.mean(rate/np.diff(inds))\n",
        "#         elif mode=='count':\n",
        "#             mean_spike_rate = len(inds)/(np.sum(is_stim)/rate)\n",
        "#         else:\n",
        "#             print('invalid mode. using default (count)')\n",
        "\n",
        "\n",
        "\n",
        "#     return dVds, over_thresh, inds, mean_spike_rate\n",
        "\n",
        "file_name = '2022-08-08_hipp_data/2022x08x04_NEL2_E4KI_F_P243_s001_c001_DGxPOS_0002.abf'\n",
        "R2_thresh = 0\n",
        "to_plot = 1\n",
        "\n",
        "spike_args = {'high_dv_thresh': 40,\n",
        "                'low_dv_thresh': -5,\n",
        "                'window_ms': 5}\n",
        "\n",
        "\n",
        "gain_slope, R2, stim_currents, spike_counts, v_before_stim = analyze_gain_abf(file_name,spike_args,R2_thresh,to_plot) "
      ],
      "metadata": {
        "id": "dZQqWaElmaE8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "2caa28d1-6926-45d4-9c7a-6179680ed7b4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########## Rheobase analysis ################\n",
        "def Rheo_curve_analysis(abf_recordings_df, protocol_aliases,to_plot = False,single_spike=True,verbose=False):\n",
        "    'Loop through abfs to calculate Rheobase. For each abf the first sweep with '\n",
        "    'a spike that occurs after a sweep without is chosen, and its stimulus amplitude'\n",
        "    'is written to the dataframe'\n",
        "    # return dataframe with updated min stim intensity\n",
        "    abf_recordings_df['Rheobase_(pA)'] = np.nan\n",
        "    abf_recordings_df['step_resolution_(pA)']  = np.nan\n",
        "    abf_recordings_df['Rheo Ihold_(pA)']  = np.nan\n",
        "    abf_recordings_df['Vhold_Rheo_(mV)']  = np.nan\n",
        "\n",
        "\n",
        "    spike_args = {'high_dv_thresh': 40,\n",
        "                  'low_dv_thresh': -5,\n",
        "                  'window_ms': 5}\n",
        "\n",
        "\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol]):\n",
        "        if verbose: print(file_name)\n",
        "        rheo, step_resolution, offset, Vhold_spike, ap_thresh = analyze_rheo(file_name,spike_args,to_plot=to_plot,verbose=verbose,single_spike=single_spike)\n",
        "\n",
        "        abf_recordings_df.at[file_name,'Rheobase_(pA)'] = rheo\n",
        "        abf_recordings_df.at[file_name,'step_resolution_(pA)'] = step_resolution\n",
        "        abf_recordings_df.at[file_name,'Rheo Ihold_(pA)'] = offset\n",
        "        abf_recordings_df.at[file_name,'Vhold_Rheo_(mV)'] = Vhold_spike\n",
        "        abf_recordings_df.at[file_name,'AP_Threshold(mV)'] = ap_thresh\n",
        "\n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "\n",
        "def analyze_rheo(file_name,spike_args,to_plot=False,verbose=False,single_spike=True):\n",
        "    abf = abf_or_name(file_name)\n",
        "    if to_plot: plot_sweeps_and_command(abf)\n",
        "    if len(abf.sweepList)<2:\n",
        "        return np.nan,np.nan,np.nan,np.nan,np.nan  \n",
        "    else:\n",
        "        is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "        stim_currents, spike_counts, spike_rates, V_before,_ = spikes_per_stim(abf, spike_args,thresh=20,to_plot=to_plot)\n",
        "        if verbose: print(spike_counts)\n",
        "        single_spikes = spike_counts==1\n",
        "        zero_spikes = spike_counts==0\n",
        "        if single_spike:\n",
        "            none_to_one = np.full(single_spikes.shape, False)\n",
        "            none_to_one[1:] = np.logical_and(single_spikes[1:], zero_spikes[:-1])\n",
        "            first_spike_stim = np.where(none_to_one)[0]\n",
        "        else:\n",
        "            some_spikes = spike_counts>0\n",
        "            none_to_some = np.full(single_spikes.shape, False)\n",
        "            none_to_some[1:] = np.logical_and(some_spikes[1:], zero_spikes[:-1])\n",
        "            first_spike_stim = np.where(none_to_some)[0]\n",
        "    if first_spike_stim.size == 0:\n",
        "        return np.nan,np.nan,np.nan,np.nan,np.nan \n",
        "    else:\n",
        "        if first_spike_stim.size >1:\n",
        "            first_spike_stim = np.min(first_spike_stim)\n",
        "        rheo = stim_currents[first_spike_stim]\n",
        "        _, _, _, QC_val_df = Iclamp_QC(file_name)\n",
        "        offset = np.mean(QC_val_df['I_leak'])\n",
        "        Vhold_spike = QC_val_df['V_hold'][first_spike_stim]\n",
        "        ap_thresh = V_before[first_spike_stim]\n",
        "        step_resolution = np.mean(np.diff(stim_currents))\n",
        "\n",
        "    return rheo, step_resolution, offset, Vhold_spike, ap_thresh \n",
        "\n",
        "\n",
        "\n",
        "def spikes_per_stim(abf, spike_args,thresh=20,mode='count', to_plot=False):\n",
        "    'Loops through sweeps of and abf to find spikes'\n",
        "    # init\n",
        "    stim_currents = []\n",
        "    spike_rates = []\n",
        "    spike_counts = []\n",
        "    v_before_spike1 = []\n",
        "    v_before_stim = []\n",
        "    # get sweep info\n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "\n",
        "    # get spike per sweep\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s)\n",
        "        dVds, over_thresh, inds, mean_spike_rate = find_spike_in_trace(abf.sweepY,abf.sampleRate,spike_args,thresh=thresh,to_plot=to_plot,is_stim=is_stim,mode='count')\n",
        "        # plot id'd spikes\n",
        "        if to_plot:\n",
        "            fig, axs = plt.subplots(1)\n",
        "            axs.plot(abf.sweepX ,abf.sweepY)\n",
        "            axs.scatter(abf.sweepX[inds],abf.sweepY[inds],color='red')\n",
        "            plt.show()\n",
        "        # calc multi sweep params\n",
        "        stim_level = np.median(abf.sweepC[is_stim])\n",
        "        stim_currents.append(stim_level)\n",
        "        spike_rates.append(mean_spike_rate)\n",
        "        spike_counts.append(len(inds))\n",
        "        is_prestim = np.equal(np.cumsum( np.diff(is_base,prepend=1)),0)\n",
        "        v_before_stim.append( np.mean(abf.sweepY[is_prestim] ))\n",
        "\n",
        "        if len(inds)>0:\n",
        "            v_before_spike1.append(abf.sweepY[inds[0]])\n",
        "        else:\n",
        "            v_before_spike1.append(np.nan)\n",
        "\n",
        "    return np.array(stim_currents), np.array(spike_counts), np.array(spike_rates), np.array(v_before_spike1), np.array(v_before_stim)\n",
        "\n",
        "\n",
        "\n",
        "def find_spike_in_trace(trace,rate,spike_args,thresh=20,refract=0.005,is_stim = None ,mode='count',sanity_check=True,to_plot=False):\n",
        "    'Takes in a voltage trace from current clamp mode and uses derivative (dVds) to find action potentials.'\n",
        "    'Returns the dVds trace, boolean array indicating if dVds>threshold, inicies where dV crossed threshold,'\n",
        "    'and the mean firing rate given # spikes in trace of given length. Optional ways to count are:'\n",
        "    'isi (1/interspike interval) or count (spike count per second). Default is count'\n",
        "\n",
        "    high_dv_thresh = spike_args['high_dv_thresh']\n",
        "    low_dv_thresh = spike_args['low_dv_thresh']\n",
        "    window_ms = spike_args['window_ms']\n",
        "\n",
        "    if any(is_stim == None):\n",
        "        is_stim = [True for i in trace]\n",
        "    dVds = np.diff(trace, prepend=trace[0])*rate/1000\n",
        "    over_thresh = dVds>thresh\n",
        "    over_thresh[np.logical_not(is_stim)] = False\n",
        "    refract_window = int(np.round((refract*rate)))\n",
        "    inds = [t for t in np.arange(refract_window,len(over_thresh)) if all([over_thresh[t], all(over_thresh[t-refract_window:t]==False)])]    \n",
        "    if to_plot:\n",
        "        plt.plot(dVds)\n",
        "        plt.show()\n",
        "    if sanity_check:\n",
        "        old_inds = inds\n",
        "        inds = []\n",
        "        for i in old_inds:\n",
        "            samp_window = window_ms/1000 * rate\n",
        "            ind_range = np.arange(i-samp_window,i+samp_window).astype(int)\n",
        "            nearby_dVds = dVds[ind_range]\n",
        "            if False: print(i,'max', np.max(nearby_dVds))\n",
        "            if False: print(i,'min', np.min(nearby_dVds))\n",
        "            if np.max(nearby_dVds)>high_dv_thresh and np.min(nearby_dVds) < low_dv_thresh:\n",
        "                inds.append(i)\n",
        "                if False: print(inds)\n",
        "    if len(inds)<1:\n",
        "        mean_spike_rate = 0\n",
        "    else:\n",
        "        if mode=='isi':\n",
        "            mean_spike_rate = np.mean(rate/np.diff(inds))\n",
        "        elif mode=='count':\n",
        "            mean_spike_rate = len(inds)/(np.sum(is_stim)/rate)\n",
        "        else:\n",
        "            print('invalid mode. using default (count)')\n",
        "    return dVds, over_thresh, inds, mean_spike_rate"
      ],
      "metadata": {
        "id": "WRo06YBZG3Ze"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################## Analyze R-Input In Current Clamp ##################################\n",
        "\n",
        "def inputR_analysis(abf_recordings_df, protocol_aliases,to_plot=False, verbose = False):\n",
        "    'Loop through abfs to calculate the input resistance in Current Clamp'\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol] ):\n",
        "        abf = pyabf.ABF( file_name )\n",
        "        if len(abf.sweepList)<2:\n",
        "            # print(list(abf_recordings_df.index).index(file_name),    'not enough sweeps')\n",
        "            abf_recordings_df.at[file_name,'Rinput_(MO)'] = np.nan\n",
        "            abf_recordings_df.at[file_name,'Cmf_IC_(pF)'] = np.nan\n",
        "            continue\n",
        "            \n",
        "        inputR_fit = input_res_curve(abf,to_plot=to_plot)\n",
        "        abf_recordings_df.at[file_name,'Rinput_(MO)'] = inputR_fit['slope']*1000\n",
        "        if len(abf_recordings_df.at[file_name,'passing_sweeps'])==0:\n",
        "            abf_recordings_df.at[file_name,'Cmf_IC_(pF)'] = np.nan\n",
        "            continue\n",
        "        Cmf_IC = IC_sweep_capacitance_mean(abf,abf_recordings_df.at[file_name,'passing_sweeps'],to_plot=to_plot,verbose=verbose)\n",
        "        abf_recordings_df.at[file_name,'Cmf_IC_(pF)'] = Cmf_IC\n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "def input_res_curve(abf,to_plot=False):\n",
        "    'Calulates the series of delta Vs and delta Is and fits with a line to find the resistance.'\n",
        "    from scipy.signal import butter,filtfilt\n",
        "    stim_currents = []\n",
        "    ss_voltage = []\n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s)\n",
        "        delta_v, _, _ = sweep_VIR(abf.sweepY, abf.sampleRate, is_stim = is_stim)\n",
        "        delta_I, _, _    = sweep_VIR(abf.sweepC, abf.sampleRate, is_stim = is_stim) # repurpose but for command current\n",
        "        stim_currents.append( delta_I)\n",
        "        ss_voltage.append(delta_v)\n",
        "    \n",
        "    inputR_fit = {}\n",
        "    inputR_fit['slope'], inputR_fit['intercept'] , r_value, p_value, std_err = stats.linregress(stim_currents, ss_voltage)\n",
        "    inputR_fit['R2'] = r_value**2\n",
        "    if to_plot:\n",
        "        plt.scatter(stim_currents, ss_voltage)\n",
        "        matplotlib.pyplot.text(10, 0, 'Ri = ' + str(round(inputR_fit['slope']*1000)) + 'MO')\n",
        "    return inputR_fit        \n",
        "\n",
        "def sweep_VIR(trace,rate,is_stim = None, window_t=0.100):\n",
        "    'Takes a trace snd calulates the steady state delta V from'\n",
        "    'a stimulus in Current Clamp'\n",
        "    if any(is_stim == None):\n",
        "        is_stim = [True for i in trace]\n",
        "    base_v = trace[:np.where(is_stim==True)[0][0]]\n",
        "    cutoff = 5\n",
        "    nyq = rate/2\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(3, normal_cutoff, btype='low')\n",
        "    filtered_step_v = filtfilt(b, a, trace[is_stim])\n",
        "    window_wid = int(window_t*rate)\n",
        "    med_base_v = np.median(base_v[-window_wid:-1])\n",
        "    med_stim_v = np.median(filtered_step_v[-window_wid:-1])\n",
        "    delta_v = med_stim_v - med_base_v\n",
        "    return delta_v, med_base_v, med_stim_v\n",
        "\n",
        "def binary_exp(time, amp_1, amp_2, tau_1, tau_2, ss):\n",
        "    'A double exponential decay function'\n",
        "    return (amp_1 * np.exp(-time /tau_1)) + (amp_2 * np.exp(-time / tau_2))  + ss\n",
        "\n",
        "\n",
        "def IC_sweep_capacitance_mean(abf,passing_sweeps,to_plot=False, verbose = False):\n",
        "    'Takes an abf and calulates the membrane capacitance using'\n",
        "    'methods described in https://journals.physiology.org/doi/epdf/10.1152/jn.00160.2009'\n",
        "    \n",
        "    \n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "    rVm_list = []\n",
        "    V0_list = []\n",
        "    SS_list = []\n",
        "    Stim_list = []\n",
        "    time = abf.sweepX[is_stim]\n",
        "    time = time - time[0]\n",
        "    # for s in abf.sweepList:\n",
        "    for s in passing_sweeps:\n",
        "        abf.setSweep(s)\n",
        "        Istim = abf.sweepC[is_stim]\n",
        "        if not( np.mean(Istim) < 0 ):\n",
        "            continue\n",
        "        Vm = abf.sweepY[is_stim]\n",
        "        SS = np.percentile(Vm,.0001)\n",
        "        V0 = Vm[0]\n",
        "        relative_Vm = (Vm - SS) / (V0-SS)\n",
        "        rVm_list.append(relative_Vm)\n",
        "        V0_list.append(V0)\n",
        "        SS_list.append(SS)\n",
        "        Stim_list.append(np.median(Istim))\n",
        "    if len(V0_list)<1:\n",
        "        'likely there are passing sweeps but they are positive current'\n",
        "        return np.nan\n",
        "    \n",
        "    rVm_array = np.stack(rVm_list).T\n",
        "    V0_array = np.array(V0_list)\n",
        "    SS_array = np.array(SS_list)\n",
        "    Stim_array = np.array(Stim_list)\n",
        "    mean_rVm = np.mean(rVm_array,axis=-1)\n",
        "    if to_plot:\n",
        "        plt.plot(time,rVm_array,'gray')\n",
        "        plt.plot(time,mean_rVm,'k')\n",
        "\n",
        "\n",
        "    p0 = ( 0.1, 0.8, 0.0001,  0.01, 0)\n",
        "    bounds=([0,.3,0,.01,-.2], [0.25,1.2,.01,2,.2])\n",
        "\n",
        "    try:\n",
        "        fit_params, cv = scipy.optimize.curve_fit(binary_exp, time, mean_rVm, p0, bounds=bounds) #\n",
        "        amp_1, amp_2, tau_1, tau_2, ss = fit_params\n",
        "        Vm_hat = binary_exp(time, amp_1, amp_2, tau_1, tau_2, ss)\n",
        "\n",
        "        Vm_hat1 = binary_exp(time, amp_1, 0, tau_1, tau_2, ss) + amp_2\n",
        "        Vm_hat2 = binary_exp(time, 0, amp_2,  tau_1, tau_2, ss) + amp_1\n",
        "\n",
        "        t_slow = tau_2*np.ones_like(V0_array)\n",
        "        dv_slow = (SS_array - V0_array) * amp_2 *1e-3\n",
        "        i_stim = Stim_array*1e-12\n",
        "        r_slow = dv_slow/i_stim\n",
        "        c_slow = t_slow /r_slow\n",
        "        Cmf_IC = c_slow*1e12\n",
        "    except:\n",
        "        print(V0_array)\n",
        "        print(Stim_array)\n",
        "        print(SS_array)\n",
        "        return np.nan \n",
        "\n",
        "    if verbose:\n",
        "        print('V0', V0_array)\n",
        "        print('I', i_stim*1e12)\n",
        "        print('dV', dv_slow*1e3)\n",
        "        print('SSv', SS_array)\n",
        "        print('Rm', r_slow*1e-6)\n",
        "        print('Cm', Cmf_IC)\n",
        "        print('amp_1, amp_2, tau_1, tau_2, ss')\n",
        "        print('Fit', fit_params)\n",
        "\n",
        "    if to_plot:\n",
        "        plt.plot(time,Vm_hat,'r')\n",
        "        plt.plot(time,Vm_hat1,'c')\n",
        "        plt.plot(time,Vm_hat2,'m')\n",
        "        plt.show()\n",
        "\n",
        "    return np.mean(Cmf_IC)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "TiU-YFY-G0kq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################  Quality Control Filtering ##################################\n",
        "def QC_full_dataset(abf_recordings_df,to_plot=False,verbose=False,VC_prot=[],IC_prot=[],MT_prot=[]):\n",
        "    'Loop through all abfs to look for signs of a poor recording including leak current, unstable holding V/I,'\n",
        "    'high noise, and in appropriate holidng potential'\n",
        "\n",
        "    # abf_recordings_df['QC_checks'] = None\n",
        "    # abf_recordings_df['QC_values'] = None\n",
        "    abf_recordings_df['passing_sweeps'] = None\n",
        "\n",
        "    print('Voltage Clamp Protocols')\n",
        "    VC_idx = [p in VC_prot for p in abf_recordings_df['protocol']]\n",
        "    for fn in tqdm( abf_recordings_df[VC_idx].index ):\n",
        "        if verbose: print(np.where(fn==abf_recordings_df.index)[0])\n",
        "        try:\n",
        "            pass_rate, passing_sweeps, QC_check_df, QC_val_df = Vclamp_QC(fn,to_plot=to_plot,verbose=verbose)\n",
        "            abf_recordings_df.at[fn,'passing_sweeps'] = np.array(passing_sweeps)\n",
        "        except AssertionError:\n",
        "            if verbose: print_assert()\n",
        "            abf_recordings_df.at[fn,'passing_sweeps'] = []\n",
        "\n",
        "    print('Current Clamp Protocols')\n",
        "    IC_idx = [p in IC_prot for p in abf_recordings_df['protocol']]\n",
        "    for fn in tqdm( abf_recordings_df[IC_idx].index ):\n",
        "        if verbose: print(np.where(fn==abf_recordings_df.index)[0])\n",
        "        try:\n",
        "            pass_rate, passing_sweeps, QC_check_df, QC_val_df = Iclamp_QC(fn,to_plot=to_plot,verbose=verbose)\n",
        "            abf_recordings_df.at[fn,'passing_sweeps'] = np.array(passing_sweeps)\n",
        "        except AssertionError:\n",
        "            if verbose: print_assert()\n",
        "            abf_recordings_df.at[fn,'passing_sweeps'] = []\n",
        "       \n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "def Vclamp_QC(file_name, max_leak=200,max_high_freq_noise = 10, max_low_freq_noise = 15,\n",
        "              Vhold_range = 8, to_plot=False,verbose=False):\n",
        "    'Look for signs of a poor recording for Voltage Clamp recordings'\n",
        "    \n",
        "    abf = pyabf.ABF( file_name )\n",
        "    if 'mV' in abf.sweepLabelY:\n",
        "        is_IC = True\n",
        "        is_VC = False\n",
        "    if 'pA' in abf.sweepLabelY:\n",
        "        is_IC = False\n",
        "        is_VC = True\n",
        "\n",
        "\n",
        "    try:\n",
        "        assert is_VC==True, 'Wrong clamp mode for protocol. Voltage protocol used during current clamp!'\n",
        "    except AssertionError as e:\n",
        "        print(e)\n",
        "        return 0, [], [], []\n",
        "\n",
        "\n",
        "\n",
        "    # assert is_VC==True, 'Wrong clamp mode for protocol. Voltage protocol used during current clamp!'\n",
        "    \n",
        "\n",
        "    theta, command_offset, correct_ch1 =  predict_telegraph(abf)\n",
        "\n",
        "\n",
        "    QC_check_list = []\n",
        "    QC_val_list = []\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s,0)\n",
        "        QC_checks, QC_values, windows = qc_sweep(abf.sweepX,abf.sweepC,abf.sweepY,command_offset,is_IC,is_VC,abf.sampleRate,                                                 \n",
        "                                                 max_leak=max_leak,\n",
        "                                                 max_high_freq_noise = max_high_freq_noise,\n",
        "                                                 max_low_freq_noise = max_low_freq_noise,\n",
        "                                                 Vhold_range = Vhold_range, to_plot=False)\n",
        "        QC_check_list.append(QC_checks)\n",
        "        QC_val_list.append(QC_values)\n",
        "    \n",
        "    QC_check_df = pd.DataFrame({'sweep' : np.arange(len(QC_val_list))}).set_index('sweep')\n",
        "    for i in np.arange(len(QC_check_list)):\n",
        "        d = QC_check_list[i]\n",
        "        for (k,v) in d.items():\n",
        "            QC_check_df.at[i,k]=v\n",
        "\n",
        "    QC_val_df = pd.DataFrame({'sweep' : np.arange(len(QC_val_list))}).set_index('sweep')\n",
        "    for i in np.arange(len(QC_val_list)):\n",
        "        d = QC_val_list[i]\n",
        "        for (k,v) in d.items():\n",
        "            QC_val_df.at[i,k]=v\n",
        "\n",
        "    pass_rate = {}\n",
        "    for c in QC_check_df.columns:\n",
        "        pass_rate[c] = np.round(np.mean(np.array(QC_check_df[c].values).astype(int))*100,2)\n",
        "\n",
        "    if to_plot:\n",
        "        fig, ax, theta = plot_sweeps_and_command(abf,windows=windows)\n",
        "        plt.show()\n",
        "\n",
        "    passing_sweeps = [s for s in QC_check_df.index if all(QC_check_df.loc[s,:])]\n",
        "    if verbose: print('\\n','pass_rate:',pass_rate)\n",
        "    if verbose: print('passing_sweeps:',passing_sweeps)\n",
        "    return pass_rate, passing_sweeps, QC_check_df, QC_val_df\n",
        "\n",
        "def Iclamp_QC(file_name, max_leak=250, to_plot=False,verbose=False):\n",
        "    'Look for signs of a poor recording for Current Clamp recordings'\n",
        "    abf = abf_or_name(file_name)\n",
        "    # abf = pyabf.ABF( file_name )\n",
        "    if 'mV' in abf.sweepLabelY:\n",
        "        is_IC = True\n",
        "        is_VC = False\n",
        "    if 'pA' in abf.sweepLabelY:\n",
        "        is_IC = False\n",
        "        is_VC = True\n",
        "    try:\n",
        "        assert is_IC==True, 'Wrong clamp mode for protocol. IC protocol used during voltage clamp!'\n",
        "    except AssertionError as e:\n",
        "        print(e)\n",
        "\n",
        "    # assert is_IC==True, 'Wrong clamp mode for protocol. IC protocol used during voltage clamp!'\n",
        "\n",
        "    theta, command_offset, correct_ch1 =  predict_telegraph(abf)\n",
        "    \n",
        "    QC_check_list = []\n",
        "    QC_val_list = []\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s,0)\n",
        "        QC_checks, QC_values, windows = qc_sweep(abf.sweepX,abf.sweepC,abf.sweepY,command_offset,is_IC,is_VC,\n",
        "                                                 abf.sampleRate,to_plot=to_plot, max_high_freq_noise = .05, max_low_freq_noise = 0.4, Vhold_range=3)\n",
        "        QC_check_list.append(QC_checks)\n",
        "        QC_val_list.append(QC_values)\n",
        "    \n",
        "    QC_check_df = pd.DataFrame({'sweep' : np.arange(len(QC_val_list))}).set_index('sweep')\n",
        "    for i in np.arange(len(QC_check_list)):\n",
        "        d = QC_check_list[i]\n",
        "        for (k,v) in d.items():\n",
        "            QC_check_df.at[i,k]=v\n",
        "\n",
        "    QC_val_df = pd.DataFrame({'sweep' : np.arange(len(QC_val_list))}).set_index('sweep')\n",
        "    for i in np.arange(len(QC_val_list)):\n",
        "        d = QC_val_list[i]\n",
        "        for (k,v) in d.items():\n",
        "            QC_val_df.at[i,k]=v\n",
        "\n",
        "    pass_rate = {}\n",
        "    mean_values = {}\n",
        "    for c in QC_check_df.columns:\n",
        "        pass_rate[c] = np.round(np.mean(np.array(QC_check_df[c].values).astype(int))*100,2)\n",
        "        mean_values[c] = np.round(np.mean(np.array(QC_val_df[c].values)),3)\n",
        "    passing_sweeps = [s for s in QC_check_df.index if all(QC_check_df.loc[s,:])]\n",
        "    \n",
        "    if verbose==True: \n",
        "        print('')\n",
        "        print('pass_rate:',pass_rate)\n",
        "        print('mean_values:',mean_values)\n",
        "    if verbose==2:\n",
        "        print('\\n','pass_rate:',pass_rate)\n",
        "        print('passing_sweeps:',passing_sweeps, str(len(passing_sweeps)/len(QC_check_df)*100)+'%')\n",
        "        print(QC_val_df)\n",
        "\n",
        "    if to_plot:\n",
        "        fig, ax, theta = plot_sweeps_and_command(abf,windows=windows)\n",
        "        plt.show()\n",
        "\n",
        "    return [pass_rate,passing_sweeps,QC_check_df,QC_val_df] # return pass_rate, passing_sweeps, QC_check_df, QC_val_df\n",
        "\n",
        "def qc_sweep(sweepX,sweepC,sweepY,command_offset,is_IC,is_VC,sampleRate,\n",
        "             max_leak=100, \n",
        "             max_high_freq_noise = 10,\n",
        "             max_low_freq_noise = 10,\n",
        "             Vhold_range = 5, to_plot=False):\n",
        "    'Recieves a sweep and and calculates leak, noise and holding potential'\n",
        "    'returns a dict of calculated values and a dict of boolean indicating'\n",
        "    'pass/fail, (True/False)'\n",
        "\n",
        "\n",
        "    stim_buffer_time = 250 #ms\n",
        "    filtered_command = movmean((sweepC==sweepC[0])*1, stim_buffer_time/1000*sampleRate)\n",
        "    ss_no_stim_bool = filtered_command==1\n",
        "    ss_no_stim_idx = np.arange(len(ss_no_stim_bool))[ss_no_stim_bool]\n",
        "    no_stim_sig = sweepY[ss_no_stim_bool]\n",
        "    no_stim_t = sweepX[ss_no_stim_idx]\n",
        "    baseline = np.mean( no_stim_sig )\n",
        "\n",
        "    QC_checks = {}\n",
        "    QC_values = {}\n",
        "    if is_IC:\n",
        "        QC_values['V_hold'] = baseline\n",
        "        QC_values['I_leak'] = command_offset\n",
        "    if is_VC:\n",
        "        QC_values['V_hold'] = command_offset\n",
        "        QC_values['I_leak'] = baseline\n",
        "    \n",
        "    QC_checks['V_hold'] = abs(QC_values['V_hold'] - -70)< Vhold_range\n",
        "    QC_checks['I_leak'] = QC_values['I_leak']<max_leak\n",
        "\n",
        "\n",
        "    \n",
        "    HF_noise_idx = ss_no_stim_idx[:int(0.0015*sampleRate)]\n",
        "    HF_noise_signal = sweepY[ HF_noise_idx ] \n",
        "    HF_noise = rms_noise(HF_noise_signal)     \n",
        "    \n",
        "    QC_checks['HF_noise'] = HF_noise<max_high_freq_noise\n",
        "    QC_values['HF_noise'] = HF_noise\n",
        "    \n",
        "    LF_noise_idx = ss_no_stim_idx[-int(0.1*sampleRate):] \n",
        "    if int(0.1*sampleRate)>len(LF_noise_idx): LF_noise_idx = np.random.choice(LF_noise_idx, size=int(0.1*sampleRate))\n",
        "    LF_noise_signal = sweepY[ LF_noise_idx ] \n",
        "    LF_noise = rms_noise(LF_noise_signal)\n",
        "    QC_checks['LF_noise'] = LF_noise<max_low_freq_noise\n",
        "    QC_values['LF_noise'] = LF_noise\n",
        "\n",
        "    HF_noise_time = sweepX[HF_noise_idx]\n",
        "    LF_noise_time = sweepX[LF_noise_idx]\n",
        "    LF_noise_time = np.sort(np.array(list(set(LF_noise_time))))\n",
        "\n",
        "    return QC_checks, QC_values, [HF_noise_time, LF_noise_time]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x46MR5nvGrV4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################## Membrane Resistance & Capacitance Testing ##################################\n",
        "def Icapacitance_analysis(abf_recordings_df, protocol_aliases, to_plot=False,verbose=False,report_params=True):\n",
        "    'Loops through abfs with Vclamp pulstrains to calculate membrane properties: Ra, Rm, Cm'\n",
        "    if report_params:\n",
        "        report_params = ['Ra', 'Rm', 'Cm', 'tau',\t'Cmq',\t'Cmf',\t'Cmqf']\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    print(len(correct_protocol),'files to analyze...')\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol] ) : \n",
        "        abf = pyabf.ABF( file_name )\n",
        "        # passing_sweeps=abf_recordings_df.at[file_name,'passing_sweeps']\n",
        "\n",
        "        try:\n",
        "            pass_rate, passing_sweeps, QC_check_df, QC_val_df = Vclamp_QC(file_name,to_plot=False,verbose=verbose)\n",
        "            passing_sweeps = [s for s in QC_check_df.index if all(QC_check_df.loc[s,['I_leak','HF_noise','LF_noise']])] # ignore Vhold Filters\n",
        "            mem_params_df = fit_Icapacitave_mean_current(abf,to_plot=to_plot,verbose=verbose,passing_sweeps=passing_sweeps)\n",
        "        except:\n",
        "            _=print_assert()\n",
        "            mem_params_df = pd.DataFrame()\n",
        "        for c in mem_params_df.columns:\n",
        "            for d in mem_params_df.index:\n",
        "                if c in report_params:\n",
        "                    abf_recordings_df.at[file_name, c+'_'+str(d)] = mem_params_df.at[d,c]\n",
        "    return abf_recordings_df, correct_protocol\n",
        "       \n",
        "\n",
        "def fit_Icapacitave_mean_current(abf, to_plot=False, verbose=False, passing_sweeps = []):\n",
        "    'Takes in an abf file and finds all pulses. Pulses with matching duration are averaged together.'\n",
        "    'For each pulse duration the mean pulse is fit using the methods described at https://swharden.com/blog/2020-10-11-model-neuron-ltspice/ '\n",
        "    'For each pulse length returns, Ra, Rm, and three Cm measures (Cmf, Cmq, Cmqf).'\n",
        "    'Respectively these are capacitance determined by: fitting tau and computing,'\n",
        "    'calculating the area under the capcitave transient, and calculating the area'\n",
        "    'under the fit line.'\n",
        "\n",
        "    command = abf.sweepC\n",
        "    # trace,time,command,rate,\n",
        "\n",
        "    base_v = command[0]\n",
        "    step_v = np.median( command[np.logical_not(command==base_v)])\n",
        "    is_base = command==base_v\n",
        "    is_step = command==step_v\n",
        "\n",
        "    delta_V = abs(step_v-base_v)\n",
        "\n",
        "    step_start = np.logical_and(is_base[:-1], is_step[1:])\n",
        "    step_stop = np.logical_and(is_step[:-1], is_base[1:])\n",
        "\n",
        "    \n",
        "\n",
        "    starts = np.where(step_start)[0]\n",
        "    stops = np.where(step_stop)[0]\n",
        "\n",
        "    assert len(starts)==len(stops), 'unable to match pulse starts and stops'\n",
        "    assert any(( starts-stops)<0), 'unable to match pulse starts and stops'\n",
        "    assert len(starts)>0, 'no pulse found'\n",
        "    # parse_pulses\n",
        "\n",
        "    if verbose: print('passing_sweeps',passing_sweeps)\n",
        "\n",
        "    params = []\n",
        "    p_len_list = []\n",
        "    Icap_list = []\n",
        "    step_time_list = []\n",
        "    # for s in abf.sweepList:\n",
        "    for s in passing_sweeps:\n",
        "        abf.setSweep(s)\n",
        "        trace = abf.sweepY\n",
        "        sweep_time = abf.sweepX\n",
        "        if (base_v>step_v):\n",
        "            trace = -trace\n",
        "        for p in np.arange(len(starts)):\n",
        "            pulse_start = starts[p]\n",
        "            pulse_stop = stops[p]\n",
        "            pulse_len = stops[p] - starts[p]\n",
        "            p_len_list.append(pulse_len)\n",
        "            pulse_index = np.arange(int(pulse_start-pulse_len*0.05),pulse_stop)\n",
        "\n",
        "            step_times = sweep_time[pulse_index]\n",
        "            step_times = step_times-sweep_time[starts[p]]\n",
        "            step_time_list.append(step_times)\n",
        "\n",
        "            Icap_transient = trace[pulse_index]\n",
        "            Icap_list.append(Icap_transient)\n",
        "\n",
        "    p_len_list = np.array(p_len_list)/abf.sampleRate*1000\n",
        "    pulse_set = np.array(sorted(set(p_len_list)))\n",
        "    mem_params_df = pd.DataFrame(None,index=pulse_set,columns=['>90%','Ib','Iss','Ip','Ra','Rm','tau','Cmq','Cmf','Cmqf'])\n",
        "    \n",
        "    if to_plot:\n",
        "        fig, axs = plt.subplots(1,len(pulse_set),figsize=[12, 3])\n",
        "        fig.suptitle(abf.abfFilePath)\n",
        "        if verbose: print(abf.abfFilePath)\n",
        "        if str(type(axs)) == \"<class 'matplotlib.axes._subplots.AxesSubplot'>\":\n",
        "            axs = [axs]\n",
        "    \n",
        "    \n",
        "    for p in pulse_set:\n",
        "        # pulse_dur =p/abf.sampleRate*1000\n",
        "        matching_traces = [Icap_list[n] for n in np.arange(len(p_len_list)) if p_len_list[n]==p ]\n",
        "        matching_traces = np.stack(matching_traces)\n",
        "\n",
        "        mean_trace = np.mean(matching_traces,axis=0)\n",
        "        mean_time = np.mean(np.stack([step_time_list[n] for n in np.arange(len(p_len_list)) if p_len_list[n]==p ]),axis=0)\n",
        "\n",
        "        sweep_var = abs((matching_traces-mean_trace)/mean_trace)\n",
        "        outlier_percent = round(np.mean(sweep_var>1.645)*100,3)\n",
        "        # base_ind = np.arange(len(mean_time))\n",
        "        base_t = np.mean(mean_time[mean_time<0])\n",
        "        base_I = np.mean(mean_trace[mean_time<0])\n",
        "        \n",
        "        steady_state_t = np.mean(mean_time[mean_time>mean_time[-1]*0.95])\n",
        "        steady_state_I = np.mean(mean_trace[mean_time>mean_time[-1]*0.95])\n",
        "\n",
        "\n",
        "        peak_I = np.max(mean_trace)\n",
        "        peak_t = mean_time[mean_trace==peak_I]\n",
        "        if peak_t.shape[0]>1: peak_t = min(peak_t)\n",
        "        Icap_curve = (mean_trace[mean_time>=peak_t])\n",
        "        Icap_curve_t = mean_time[mean_time>=peak_t]\n",
        "\n",
        "\n",
        "        rel_dif_Icap = movmean(np.diff(Icap_curve,append=Icap_curve[-1]),10)/peak_I\n",
        "        excess_plat_t = Icap_curve_t[rel_dif_Icap>=0]\n",
        "        if len(excess_plat_t)>0:\n",
        "            excess_plat_start = np.min(excess_plat_t)*10\n",
        "            if excess_plat_start >0.005:\n",
        "                Icap_curve = Icap_curve[Icap_curve_t<excess_plat_start]\n",
        "                Icap_curve_t = Icap_curve_t[Icap_curve_t<excess_plat_start]\n",
        "                steady_state_t = np.mean(Icap_curve_t[Icap_curve_t>Icap_curve_t[-1]*0.95])\n",
        "                steady_state_I = np.mean(Icap_curve[Icap_curve_t>Icap_curve_t[-1]*0.95])\n",
        "                # plt.scatter(Icap_curve_t,Icap_curve)\n",
        "                # plt.scatter(excess_plat_t,Icap_curve[rel_dif_Icap>=0])\n",
        "                # plt.gca().set_xscale('log')\n",
        "                # plt.scatter(excess_plat_start,peak_I)\n",
        "                # plt.show()\n",
        "\n",
        "\n",
        "        \n",
        "        delta_I_steady = steady_state_I - base_I\n",
        "        delta_I_peak = peak_I - steady_state_I\n",
        "        # if verbose: print('len(Icap_curve)',len(Icap_curve))\n",
        "        # if verbose: print('delta_V',delta_V)\n",
        "        # if verbose: print('delta_I_peak',delta_I_peak)\n",
        "        Ra = (delta_V*1e-3)/(delta_I_peak*1e-12) *1e-6 #(O/MO)\n",
        "        Rm = ((delta_V*1e-3) - Ra*1e6 * delta_I_steady*1e-12) / (delta_I_steady*1e-12) *1e-6 #(O/MO)\n",
        "        Q = np.sum(Icap_curve-steady_state_I) * (1/abf.sampleRate)\n",
        "        Cmq = Q / delta_V*1000\n",
        "        # if verbose: print('Cmq',Cmq)\n",
        "        \n",
        "\n",
        "        try:\n",
        "            bounds=([peak_I*0.1,.0001,0], [peak_I*1.5,500, steady_state_I*3])\n",
        "            p0 = (peak_I, 0.02 , steady_state_I) # start with values near those we expect\n",
        "            fit_params, cv = scipy.optimize.curve_fit(mono_exp, Icap_curve_t[int(0.0005*abf.sampleRate):], Icap_curve[int(0.0005*abf.sampleRate):], p0, bounds=bounds) #\n",
        "            peak_hat, tau_hat, ss_hat = fit_params\n",
        "            Icap_hat =  mono_exp(Icap_curve_t, peak_hat, tau_hat, ss_hat)\n",
        "            perr = np.sqrt(np.diag(cv))\n",
        "            # if verbose: print('tau_hat',tau_hat)\n",
        "            # if verbose: print('Icap_curve_t',Icap_curve_t)\n",
        "            # if verbose: print('Ra',Ra)\n",
        "            # if verbose: print('Rm',Rm)\n",
        "            Cmf = tau_hat / (1/(1/(Ra*1e6) + 1/(Rm*1e6)))\n",
        "            Cmf = Cmf*1e12\n",
        "            # if verbose: print('Cmf',Cmf)\n",
        "            \n",
        "        except:\n",
        "            Cmf = None\n",
        "            Icap_hat = np.empty_like(Icap_curve_t)\n",
        "            Icap_hat[:] =np.nan\n",
        "            ss_hat = np.nan\n",
        "            tau_hat = np.nan\n",
        "\n",
        "        Cmqf = np.sum(Icap_hat-ss_hat) * (1/abf.sampleRate) / delta_V*1000\n",
        "        # if verbose: print('Cmqf',Cmqf)\n",
        "\n",
        "        mem_params_df.at[p] = [outlier_percent,base_I,steady_state_I,peak_I,Ra,Rm,tau_hat,Cmq,Cmf,Cmqf]\n",
        "        \n",
        "\n",
        "        if to_plot:\n",
        "            i = int(np.where(p==pulse_set)[0][0])\n",
        "            mean_time_0 = -mean_time[0]\n",
        "            axs[i].plot(mean_time_0+mean_time,matching_traces.T,color = (0.8,0.8,0.8))\n",
        "            axs[i].plot(mean_time_0+mean_time,mean_trace,color='k')\n",
        "            axs[i].plot(mean_time_0+Icap_curve_t[[0,-1]],base_I*np.array([1,1]),color='r',linestyle = 'dotted')\n",
        "            axs[i].scatter(mean_time_0+peak_t,peak_I,color='r',zorder=5)\n",
        "            axs[i].plot(mean_time_0+Icap_curve_t[[0,-1]],steady_state_I*np.array([1,1]),color='r',linestyle = 'dotted')\n",
        "            axs[i].plot(mean_time_0+Icap_curve_t[int(0.001*abf.sampleRate):],Icap_curve[int(0.001*abf.sampleRate):],color='m')\n",
        "            axs[i].plot(mean_time_0+Icap_curve_t, Icap_hat,'c',linestyle = 'dashed')\n",
        "            # if verbose: print(steady_state_I)\n",
        "            # axs[i].plot(mean_time_0+Icap_curve_t, np.cumsum(Icap_hat-ss_hat)/3)\n",
        "            # axs[i].set_xscale('log')\n",
        "            axs[i].set_xlim([0,mean_time_0+Icap_curve_t[-1]*1.2]) #(mean_time_0+peak_t)*0.7\n",
        "            axs[i].set_title(str(p)+'ms')\n",
        "            \n",
        "    if verbose: display(mem_params_df)\n",
        "    if to_plot:\n",
        "        plt.tight_layout()\n",
        "        fig.subplots_adjust(top=0.8)\n",
        "        plt.show()          \n",
        "    return mem_params_df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ne5WoYGnGmhY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "####################### SPIKE LATENCY #######################\n",
        "\n",
        "def Spike_latency(abf_recordings_df, protocol_aliases,to_plot=False):\n",
        "    'Loops through abfs and calcualtes the time to first action potential'\n",
        "    'during a ramp current stimulation'\n",
        "\n",
        "\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    # print(np.sum(correct_protocol),'files to analyze...')\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol]): #tqdm( ) : \n",
        "        abf = pyabf.ABF( file_name )\n",
        "        # pass_rate, passing_sweeps, QC_check_df, QC_val_df = Vclamp_QC(file_name,to_plot=False,verbose=False)\n",
        "        passing_sweeps=abf_recordings_df.at[file_name,'passing_sweeps']\n",
        "        latencey_list = []\n",
        "        v_hold_list = []\n",
        "        \n",
        "\n",
        "        for s in abf.sweepList:\n",
        "            abf.setSweep(s)\n",
        "            # plot_sweeps_and_command(abf)\n",
        "            latencey, v_hold = analyze_ramp_sweep(abf.sweepX,abf.sweepY,abf.sweepC,\n",
        "                                              abf.sampleRate,to_plot=to_plot)\n",
        "            latencey_list.append(latencey)\n",
        "            v_hold_list.append(v_hold)\n",
        "        latencey_list =np.array(latencey_list)\n",
        "        v_hold_list =np.array(v_hold_list)\n",
        "        # if len(passing_sweeps)==0:\n",
        "        #     abf_recordings_df.at[file_name,'Spike_Latency_(ms)'] = np.nan\n",
        "        #     abf_recordings_df.at[file_name,'V_hold_(Latency)'] = np.nan\n",
        "        # else:      \n",
        "        # #     latencey_list = latencey_list[passing_sweeps]\n",
        "        # #     v_hold_list = v_hold_list[passing_sweeps]\n",
        "        abf_recordings_df.at[file_name,'Spike_Latency_(ms)'] = np.median(latencey_list)\n",
        "        abf_recordings_df.at[file_name,'V_hold_(Latency)'] = np.median(v_hold_list)\n",
        "    return abf_recordings_df\n",
        "        \n",
        "        \n",
        "def analyze_ramp_sweep(sweepX,sweepY,sweepC,rate,to_plot=False):\n",
        "    'Receives sweep data and finds the first AP and returns it.'\n",
        "    'Also retuns Vhold for quality control.'\n",
        "    is_base = sweepC==sweepC[0]\n",
        "    is_stim = np.logical_not(sweepC==sweepC[0])\n",
        "    ramp_start_ind = np.min(np.where(is_base==False))\n",
        "    v_hold = np.mean( sweepY[0:ramp_start_ind])\n",
        "    # print(sweepX,sweepY)\n",
        "\n",
        "    dVds, over_thresh, inds, mean_spike_rate = find_spike_in_trace(sweepY, rate,is_stim=is_stim,thresh=20)\n",
        "    if len(inds)==0:\n",
        "        # print('no spikes found')\n",
        "        return np.nan,v_hold\n",
        "    latencey = sweepX[np.min(inds)-ramp_start_ind]*1000\n",
        "    if to_plot:\n",
        "        # plt.scatter(sweepX,dVds,color='k')\n",
        "        plt.plot(sweepX,sweepY,color='k')\n",
        "        plt.scatter(sweepX[inds],sweepY[inds],color='r' )\n",
        "        zoom_x_relativ = np.array([ 0.75, 1.5])\n",
        "        zoom_x = zoom_x_relativ*(latencey/1000+sweepX[ramp_start_ind])\n",
        "        # print('zoom_x',zoom_x)\n",
        "        # print('latencey',latencey)\n",
        "        # print('sweepX[ramp_start_ind]',sweepX[ramp_start_ind])\n",
        "        # print('ramp_start_ind',ramp_start_ind)\n",
        "        plt.gca().set_xlim(zoom_x)\n",
        "        plt.show()\n",
        "\n",
        "    return latencey, v_hold\n"
      ],
      "metadata": {
        "id": "6nDDntRwGZ2C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################### Combine memtest measures ##########################\n",
        "def combine_memtest_durations(column_pairs,abf_recordings_df):\n",
        "    'For the niche case where membrane parameter calculated from'\n",
        "    'two pulse durations need to be combined, take a list of columns'\n",
        "    'to combine as a list of tuples, and creates a new column tagged'\n",
        "    '_Combo which consolidates the columns pairs.'\n",
        "\n",
        "    print('Combine long MemTest pulses...')\n",
        "    for p in tqdm(column_pairs) :\n",
        "        col_name = p[0][:p[0].index('_')]+'_Combo'\n",
        "        abf_recordings_df[col_name] = None\n",
        "        for rec in abf_recordings_df.index:\n",
        "            p0 = abf_recordings_df.at[rec,p[0]]\n",
        "            p1 = abf_recordings_df.at[rec,p[1]]\n",
        "            if np.isnan(p0) and np.isnan(p1):\n",
        "                abf_recordings_df.at[rec,col_name] = np.nan\n",
        "            else:\n",
        "                if np.isnan(p0):\n",
        "                    abf_recordings_df.at[rec,col_name] = p1\n",
        "                elif np.isnan(p1):\n",
        "                    abf_recordings_df.at[rec,col_name] = p0\n",
        "                else:\n",
        "                    print('unspecified types:', p0,p1)\n",
        "                    abf_recordings_df.at[rec,col_name] = np.nan\n",
        "    return abf_recordings_df\n"
      ],
      "metadata": {
        "id": "XdC03YaXX2eU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#################################### Summarize Data Cell by Cell ####################################\n",
        "def parse_file_name(abf_recordings_df):\n",
        "    'Takes the abf data frame and sorts the files in to groups based on cell ids in the abf file name.'\n",
        "    'Parses the file name extract the cell meta data and stores in a new summary dataframe'\n",
        "    'The new dataframe is populated with measured data from each abf. repeated measures are combined into'\n",
        "    'a list'\n",
        "\n",
        "    cell_list = []\n",
        "    abf_recordings_df.sort_values('file_name',inplace=True)\n",
        "    for f in abf_recordings_df.index:\n",
        "        cell_id = f[f.rfind('/')+1:] # drop directory info\n",
        "        cell_id = cell_id[:cell_id.find('.abf')] # drop file extension\n",
        "        cell_id = cell_id[:cell_id.rfind('_')] # drop rec number\n",
        "        abf_recordings_df.at[f,'cell_id'] = cell_id # write Id\n",
        "        cell_list.append(cell_id) # keep a list\n",
        "    # print(cell_list)\n",
        " \n",
        "    # populate new cell based dataframe\n",
        "    cell_list = list(set(cell_list))\n",
        "    cell_df = pd.DataFrame( {'cell_id': cell_list}).set_index('cell_id')\n",
        "\n",
        "\n",
        "    verbose = False\n",
        "    # LABELING\n",
        "    for c in cell_df.index:\n",
        "        dashes = [i for i in range(len(str(c))) if '_' in str(c)[i] ]\n",
        "        if verbose: _ = [print('   '+ c[:d]) for d in dashes   ]\n",
        "        date = c[:dashes[0]].lower()\n",
        "        virus = c[dashes[0]+1:dashes[1]].upper()\n",
        "        genotype = c[dashes[1]+1:dashes[2]].upper()\n",
        "        sex = c[dashes[2]+1:dashes[3]]\n",
        "        age =c[dashes[3]+1:dashes[4]].upper()\n",
        "        slice_num = c[dashes[4]+1:dashes[5]].upper()\n",
        "        cell_num = c[dashes[5]+1:].upper()\n",
        "        cell_type = c[dashes[6]+1:].upper()\n",
        "\n",
        "        age = int(age[1:])\n",
        "\n",
        "        # keeps\n",
        "        cell_df.at[c,'Date'] = date\n",
        "        cell_df.at[c,'virus'] = virus\n",
        "        cell_df.at[c,'geno'] = genotype\n",
        "        cell_df.at[c,'age (days)'] = age\n",
        "        cell_df.at[c,'sex'] = sex\n",
        "        cell_df.at[c,'slice'] = slice_num\n",
        "        cell_df.at[c,'cell_type'] = cell_type\n",
        "        if all([age>=(7*30), age<((9+1)*30)]):\n",
        "            cell_df.at[c,'Age_bin'] = '7_to_9'\n",
        "        elif all([age>=(17*30), age<((19+1)*30)]):\n",
        "            cell_df.at[c,'Age_bin'] = '17_to_19'\n",
        "        elif all([age>=(2*30), age<((5+1)*30)]): \n",
        "            cell_df.at[c,'Age_bin'] = '2_to_5'\n",
        "        else:\n",
        "            cell_df.at[c,'Age_bin'] = 'other'\n",
        "\n",
        "    date_to_animal = {}\n",
        "    set_list = list(set(cell_df['Date']))\n",
        "    for i in range(len(set_list)):\n",
        "        d = set_list[i]\n",
        "        date_to_animal[d] = i\n",
        "    \n",
        "    for c in cell_df.index:\n",
        "        d = cell_df.at[c,'Date']\n",
        "        cell_df.at[c,'Animal'] = date_to_animal[d]\n",
        "\n",
        "    #### Actual Values\n",
        "    cell_df['rec_list'] = None\n",
        "    for c in cell_df.index:\n",
        "        is_cell = abf_recordings_df['cell_id'] == c\n",
        "        cell_recs = list(abf_recordings_df.index[is_cell])\n",
        "        cell_df.at[c,'rec_list'] = cell_recs\n",
        "\n",
        "    data_columns = abf_recordings_df.columns\n",
        "    data_columns = [c for c in data_columns if 'cell_id' not in c]\n",
        "    for dc in data_columns:\n",
        "        cell_df[dc] = None\n",
        "        for c in cell_df.index:\n",
        "            is_cell = abf_recordings_df['cell_id'] == c\n",
        "            cell_data = list(abf_recordings_df.loc[is_cell,dc])\n",
        "            if not(type(cell_data[0]) == str):\n",
        "                # remove recs that report 'none'\n",
        "                cd = cell_data[0]\n",
        "                cell_data = [cd for cd in cell_data if not( str(type(cd)) == str(type(None)) )]\n",
        "                # remove recs that report 'nan' but check if its a float first because np.isnan is a little bitch.\n",
        "                float_data = []\n",
        "                for cd in cell_data:\n",
        "                    if str(type(cd)) == \"<class 'float'>\":\n",
        "                        if np.isnan(cd):\n",
        "                            'skip'\n",
        "                        else:\n",
        "                            float_data.append(cd)\n",
        "                    else:\n",
        "                        float_data.append(cd)\n",
        "\n",
        "                cell_data = float_data\n",
        "                # print('new',cell_data)\n",
        "                \n",
        "            cell_df.at[c,dc] = np.array(cell_data)\n",
        "\n",
        "    for c in cell_df.index:\n",
        "        prots = cell_df.at[c,'protocol']\n",
        "        cell_df.at[c,'protocol'] = set(prots)\n",
        "\n",
        "    print(len(cell_df),' cells from',len(set(cell_df['Date'])),'animals')\n",
        "\n",
        "    return cell_df, abf_recordings_df\n"
      ],
      "metadata": {
        "id": "WafNAcyEGhk5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#################### Pull Data From Summary ##############################\n",
        "\n",
        "def get_summary_data(cell_summary_df, data_keys, conditional_dict,filter_empty=False,single_val=False):\n",
        "    'WIP. Takes in a summary dataframe, a list of data keys to look up, and'\n",
        "    'as dict of cell type conditions eg: {\"cell_type\"; \"CA3\"}. Returns cell data.'\n",
        "    'Options include collapsing lists of repeat measures into a single value, or removing'\n",
        "    'cells without values for all data keys (any missing value disqualifies the cell)'\n",
        "\n",
        "\n",
        "    condition_list = [] \n",
        "    for (k,v) in conditional_dict.items():\n",
        "        cond_met =cell_summary_df[k] == v\n",
        "        condition_list.append( cond_met )\n",
        "\n",
        "    condition_list = np.stack(condition_list).T\n",
        "    all_met = [all(r) for r in condition_list]\n",
        "    all_met_ind = cell_summary_df.index[all_met]\n",
        "    cell_vals = {}\n",
        "    for dk in data_keys:\n",
        "        cell_vals[dk] = cell_summary_df.loc[all_met_ind,dk].values\n",
        "   \n",
        "    not_empty_list = []\n",
        "    for dk in data_keys:\n",
        "        # print(cell_vals[dk])\n",
        "        # [print(v) for v in cell_vals[dk]]\n",
        "        # cell_vals[dk] = [ [] if  else v for v in cell_vals[dk] ]\n",
        "        not_empty = [ len(v)>0 for v in cell_vals[dk] ]\n",
        "        not_empty_list.append( not_empty )\n",
        "\n",
        "    not_empty_list = np.stack(not_empty_list).T\n",
        "    all_not_empty = [all(r) for r in not_empty_list]\n",
        "    if filter_empty:\n",
        "        for dk in data_keys:\n",
        "            cell_vals[dk] = cell_vals[dk][all_not_empty]\n",
        "\n",
        "    if single_val:\n",
        "        for dk in data_keys:\n",
        "            # cell_vals[dk] = np.array([v[0] if len(v)>0 else v for v in cell_vals[dk]]) # first value\n",
        "            # cell_vals[dk] = np.array([np.mean(v) for v in cell_vals[dk]]) # mean value\n",
        "            cell_vals[dk] = np.array([np.median(v) for v in cell_vals[dk]]) # median value\n",
        "\n",
        "    return cell_vals, all_met, all_not_empty\n",
        "\n"
      ],
      "metadata": {
        "id": "Oj9ESi3oGYo6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################### ANALYZE IV CURVE #######################\n",
        "\n",
        "\n",
        "\n",
        "from IPython.core.pylabtools import figsize\n",
        "def IV_analyisis(abf_recordings_df,protocol_aliases, post_stim_times=[(0.0165,.03),(0.100,0.120)], stats_to_split= [(0, 'I_peak'),  (0, 'V_stim'), (1, 'I_mean')],to_plot=False):\n",
        "    'Calulates results from an IV protocol. The peak and mean are measured'\n",
        "    'from windows defined in a list of tuples [(start,stop),...]. All values'\n",
        "    'are written to the main dataframe under \"IV_stats\". This is a list of dicts'\n",
        "    'each dict corresponds to one anaysis window reporting the time range, peak etc.'\n",
        "    'Optional variable stats_to_split is used to pull specific measures from IV_stats'\n",
        "    'and write them to their own column. Stats_to_split is a list of tuples, indicating'\n",
        "    'which window and witch measure to write to the dataframe. [(window index,measure key)...]'\n",
        "\n",
        "\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    abf_recordings_df['IV_stats'] = None\n",
        "    abf_recordings_df['IV_stats'] = abf_recordings_df['IV_stats'].astype(object)\n",
        "\n",
        "    stats_to_split = ( (0, 'I_peak'),  (0, 'V_stim'), ( (1, 'I_mean')))\n",
        "    for t in stats_to_split:\n",
        "        name = t[1]\n",
        "        abf_recordings_df['IV_'+name] = None\n",
        "        abf_recordings_df['IV_'+name] = abf_recordings_df['IV_'+name].astype(object)\n",
        "\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol]):\n",
        "        abf = pyabf.ABF( file_name )\n",
        "        passing_sweeps=abf_recordings_df.at[file_name,'passing_sweeps']\n",
        "\n",
        "        pass_rate, passing_sweeps, QC_check_df, QC_val_df = Vclamp_QC(file_name,to_plot=False,verbose=False,Vhold_range = 100)\n",
        "        # display(QC_val_df)\n",
        "        if to_plot: plot_sweeps_and_command(abf)\n",
        "        # passing_sweeps = abf.sweepList # OVERRIDE\n",
        "        print(passing_sweeps)\n",
        "        abf = pyabf.ABF( file_name )\n",
        "        if to_plot: print(file_name)\n",
        "        IV_stats =analyze_IV(abf, passing_sweeps, post_stim_times,to_plot =to_plot)\n",
        "        # [print(r) for r in IV_stats]\n",
        "        \n",
        "        abf_recordings_df.at[file_name,'IV_stats'] = IV_stats\n",
        "\n",
        "        #### \n",
        "        if len(IV_stats) > 0:\n",
        "            for t in stats_to_split:\n",
        "                name = t[1]\n",
        "                range = t[0]\n",
        "                values = IV_stats[range][name]\n",
        "                values = np.array(values).tolist()\n",
        "                abf_recordings_df.at[file_name,'IV_'+name] = values\n",
        "\n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "\n",
        "def get_IV_measures(abf,passing_sweeps,post_stim_times,to_plot=False,spike_thresh=100000):\n",
        "    'Takes the abf data and calculates the mean and peak for each window.'\n",
        "    'Optionaly sweeps with APs can be filtered out using spike_threshold.'\n",
        "    'Only sweeps passing the QC filter are used. Returns a list of dicts with'\n",
        "    'each dict corresponding to one anaysis window reporting the time range, peak etc.'\n",
        "\n",
        "    IV_stats = []\n",
        "    theta, offset, correct_ch1 = predict_telegraph(abf)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    if len(passing_sweeps)==0:\n",
        "        return IV_stats\n",
        "\n",
        "    if to_plot:\n",
        "        fig, axs = plt.subplots(len(post_stim_times),figsize = [12,4] )\n",
        "        if len(post_stim_times)==1: axs = [axs]\n",
        "    \n",
        "    for t in range(len(post_stim_times)):\n",
        "        window_stats = {}\n",
        "        start = post_stim_times[t][0]\n",
        "        stop = post_stim_times[t][1]\n",
        "        delta_t = stop-start\n",
        "        flank_neg = start - (delta_t)*0.5\n",
        "        flank_pos = stop + (delta_t)*0.5\n",
        "        full_plot_range_ind =   np.logical_and(  abf.sweepX>flank_neg, abf.sweepX<flank_pos )\n",
        "        spikeless_sweeps = []\n",
        "        for s in passing_sweeps:\n",
        "            if to_plot: s_shift_t = delta_t*0.03*s\n",
        "            if to_plot: s_shift_I = 100*s\n",
        "            abf.setSweep(s)\n",
        "            time_abrg = abf.sweepX[full_plot_range_ind]\n",
        "            I_abrg = abf.sweepY[full_plot_range_ind]\n",
        "\n",
        "            time_abrg = abf.sweepX[full_plot_range_ind]\n",
        "            I_abrg = abf.sweepY[full_plot_range_ind]\n",
        "            analysis_range = np.logical_and(  abf.sweepX>start, abf.sweepX<stop )\n",
        "            time_analysis = abf.sweepX[analysis_range]\n",
        "            I_analysis = movmean( abf.sweepY[analysis_range], 4)\n",
        "            dIdt = movmean(np.diff(I_abrg, prepend=I_abrg[0]),1)\n",
        "\n",
        "            if np.max(abs(dIdt))>spike_thresh:\n",
        "                'indent holder'\n",
        "                # if to_plot:axs[t].plot( time_abrg+s_shift_t, dIdt+s_shift_I,'r')\n",
        "            else:\n",
        "                # if to_plot:axs[t].plot( time_abrg+s_shift_t, dIdt+s_shift_I,'k')\n",
        "                spikeless_sweeps.append(s)\n",
        "\n",
        "\n",
        "        I_peak_l = []\n",
        "        time_peak_l = []\n",
        "        I_mean_l = []\n",
        "        V_stim_l = []\n",
        "        \n",
        "        for s in  spikeless_sweeps :\n",
        "            abf.setSweep(s)\n",
        "            s_shift_t = delta_t*0.03*s * 0\n",
        "            s_shift_I = 100*s * 0\n",
        "\n",
        "            time_abrg = abf.sweepX[full_plot_range_ind]\n",
        "            I_abrg = abf.sweepY[full_plot_range_ind]\n",
        "            analysis_range = np.logical_and(  abf.sweepX>start, abf.sweepX<stop )\n",
        "            time_analysis = abf.sweepX[analysis_range]\n",
        "            I_analysis = abf.sweepY[analysis_range]\n",
        "            \n",
        "\n",
        "            # Peaks\n",
        "            if to_plot: axs[t].plot( time_abrg+s_shift_t, I_abrg+s_shift_I,'k')\n",
        "            \n",
        "            I_mean = np.median( I_analysis )\n",
        "            if to_plot: axs[t].plot( time_analysis+s_shift_t, (I_mean*np.ones_like(time_analysis))+s_shift_I,'r' )            \n",
        "            \n",
        "            local_I_mean = movmean(I_analysis,100)\n",
        "            peak_ind = np.where(abs(I_analysis-I_mean) == np.max(abs(I_analysis-I_mean)))[0]\n",
        "            # print(peak_ind)\n",
        "            if len(peak_ind)>1: peak_ind=[np.min(peak_ind)]\n",
        "                # print('drop')\n",
        "                # print(peak_ind)\n",
        "\n",
        "            I_peak =I_analysis[peak_ind]\n",
        "            t_peak = time_analysis[peak_ind]\n",
        "            if to_plot: axs[t].scatter( t_peak+s_shift_t, I_peak+s_shift_I, color='m' )\n",
        "\n",
        "            abf.setSweep(sweepNumber=s, channel=1)\n",
        "            command_trace = abf.sweepY\n",
        "\n",
        "\n",
        "            I_peak_l.append(I_peak)\n",
        "            time_peak_l.append(t_peak)\n",
        "            I_mean_l.append(I_mean)\n",
        "            # raw_v = np.median( abf.sweepC[analysis_range])\n",
        "            # V_stim_l.append( 10*np.round( (raw_v +offset )/10))\n",
        "\n",
        "            command_v = np.median( command_trace[analysis_range])\n",
        "            V_stim_l.append(command_v)\n",
        "    \n",
        "\n",
        "        I_peak_l = np.concatenate(I_peak_l)\n",
        "        time_peak_l = np.concatenate(time_peak_l)\n",
        "\n",
        "\n",
        "        # print(I_peak_l)\n",
        "        # print(time_peak_l)\n",
        "        # print(I_mean_l)\n",
        "        # print(V_stim_l)\n",
        "\n",
        "\n",
        "        window_stats['range'] = (start,stop)\n",
        "        window_stats['I_peak'] = I_peak_l\n",
        "        window_stats['I_peak_time'] = time_peak_l\n",
        "        window_stats['I_mean'] = I_mean_l\n",
        "        window_stats['V_stim'] = np.stack(V_stim_l).flatten()\n",
        "        IV_stats.append(window_stats)\n",
        "\n",
        "    return IV_stats\n",
        "    \n",
        "\n",
        "def analyze_IV(abf, passing_sweeps, post_stim_times, to_plot=False):\n",
        "    'This is largely a wrapper for get_IV_measures(). The main role'\n",
        "    'of this function is to plot the summary data, ie the traditional IV plot'\n",
        "\n",
        "    IV_stats = get_IV_measures(abf, passing_sweeps,post_stim_times,to_plot=to_plot,spike_thresh=1e6)\n",
        "    any_sweeps = any(np.greater([len(w['V_stim']) for w in IV_stats],0))\n",
        "    if not any_sweeps:\n",
        "        return IV_stats\n",
        "\n",
        "    if to_plot:\n",
        "        fig, axs = plt.subplots(1,2,figsize = (9,4))\n",
        "        early_peak = IV_stats[0]['I_peak']\n",
        "        V_stim = IV_stats[0]['V_stim']\n",
        "        axs[0].plot(V_stim,early_peak,color='m',marker='o',label='Peak Current')\n",
        "\n",
        "\n",
        "        # ax.scatter(V_stim,early_peak,color='k')\n",
        "        late_mean = IV_stats[1]['I_mean']\n",
        "        V_stim = IV_stats[1]['V_stim']\n",
        "        axs[0].plot(V_stim,late_mean,color='r',marker='o',label='Steady State Current')\n",
        "\n",
        "        from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
        "        axs[0].xaxis.set_major_locator(MultipleLocator(20))\n",
        "        axs[0].legend(loc=\"upper left\",frameon=True,framealpha=1)\n",
        "        axs[0].set_ylabel('Current (pA)')\n",
        "        axs[0].set_xlabel('Membrane Potential (mV)')\n",
        "\n",
        "        \n",
        "        # ax.grid(True)\n",
        "        axs[0].spines['top'].set_visible(False)\n",
        "        axs[0].spines['right'].set_visible(False)\n",
        "        axs[0].spines['bottom'].set_visible(False)\n",
        "        axs[0].spines['left'].set_visible(False)\n",
        "        yL = axs[0].get_ylim()\n",
        "        xL = axs[0].get_xlim()\n",
        "        axs[0].plot(xL,[0,0],':k')\n",
        "        axs[0].plot([-70,-70],yL,':k')\n",
        "\n",
        "\n",
        "        for s in passing_sweeps:\n",
        "            abf.setSweep(s)\n",
        "            axs[1].plot(abf.sweepX,abf.sweepC,'k')\n",
        "        \n",
        "        yL = axs[1].get_ylim()\n",
        "        axs[1].axvspan( IV_stats[0]['range'][0],IV_stats[0]['range'][1],yL[0],yL[1], alpha=0.2 ,color='m')\n",
        "        axs[1].axvspan( IV_stats[1]['range'][0],IV_stats[1]['range'][1],yL[0],yL[1], alpha=0.2 ,color='r')\n",
        "        plt.show()\n",
        "    return IV_stats\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJTX1AU04ypU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################### STREAMLINE SUMMARY ###########################\n",
        "def stream_line_cell_summary(cell_summary_df_messy):\n",
        "    'Takes in the cell-summary_df and stream lines it into single'\n",
        "    'measures per cell that can be easily collated into a readable csv'\n",
        "    'that is broadly shareable between analysis platforms.'\n",
        "    cell_summary_df =pd.DataFrame(cell_summary_df_messy.index ).set_index('cell_id')\n",
        "\n",
        "    # print(cell_summary_df_messy.columns)\n",
        "\n",
        "    'Cell Details'\n",
        "    cols = ['Date', 'geno', 'age (days)', 'sex', 'slice', 'cell_type', 'Age_bin']\n",
        "    # Copy relevant info\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "\n",
        "\n",
        "\n",
        "    'Fast Pulse Capcitance VC'\n",
        "    cols = ['Ra_10.0','Rm_10.0','Cmq_10.0']\n",
        "    # Test Pulse No QC here\n",
        "    # Consolidate with averaging\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "    for c in cols:\n",
        "        for id in cell_summary_df.index:\n",
        "            multi_values = cell_summary_df.at[id,c]\n",
        "            consolidated = np.mean(multi_values)\n",
        "            cell_summary_df.at[id,c] = consolidated\n",
        "    \n",
        "    'Slow Pulse Capcitance VC'\n",
        "    cols = ['Ra_160.0','Rm_160.0','Cmq_160.0']\n",
        "    # Test Pulse No QC here\n",
        "    # Consolidate with averaging\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "    for c in cols:\n",
        "        for id in cell_summary_df.index:\n",
        "            multi_values = cell_summary_df.at[id,c]\n",
        "            consolidated = np.mean(multi_values)\n",
        "            cell_summary_df.at[id,c] = consolidated\n",
        "\n",
        "\n",
        "\n",
        "    'Spike Latency - IC'\n",
        "    cols = ['Spike_Latency_(ms)', 'V_hold_(Latency)']\n",
        "    # QC: V_hold should be -70+/-3 to get consistant charge\n",
        "    # Consolidate with averaging\n",
        "    for id in cell_summary_df.index:\n",
        "        multi_values_SL = cell_summary_df_messy.at[id,'Spike_Latency_(ms)']\n",
        "        multi_values_VH = cell_summary_df_messy.at[id,'V_hold_(Latency)']\n",
        "        condition  = np.logical_and(multi_values_VH>-80,multi_values_VH<-60)\n",
        "        good_vals = [multi_values_SL[i] for i in range(len(multi_values_SL)) if condition[i]]\n",
        "        consolidated = np.mean(  good_vals  )\n",
        "        cell_summary_df.at[id,'Spike_Latency_(ms)'] = consolidated\n",
        "\n",
        "\n",
        "    'Rheobase - IC'\n",
        "    cols = ['Rheobase_(pA)', 'AP_Threshold(mV)', 'Vhold_Rheo_(mV)', 'Rheo Ihold_(pA)']\n",
        "    # QC: V_hold should be -70+/-3 to get consistant charge. I_hold_(pA) should be ~<100 for good clamping\n",
        "    # Consolidate with mean\n",
        "    for c in cols[:3]:\n",
        "        for id in cell_summary_df.index:\n",
        "            multi_values = cell_summary_df_messy.at[id,c]\n",
        "            multi_values_VH = cell_summary_df_messy.at[id,'Vhold_Rheo_(mV)']\n",
        "            multi_values_IH = cell_summary_df_messy.at[id,'Rheo Ihold_(pA)']\n",
        "            condition_V  = np.logical_and(multi_values_VH>-80,multi_values_VH<-60)\n",
        "            condition_I  = abs(multi_values_IH)<250\n",
        "            condition  = np.logical_and(condition_V,condition_I)\n",
        "            good_vals = [multi_values[i] for i in range(len(multi_values)) if condition[i]]\n",
        "            consolidated = np.mean(  good_vals  )\n",
        "            cell_summary_df.at[id,c] = consolidated\n",
        "\n",
        "\n",
        "    'Input Resistance - IC'\n",
        "    cols = ['Rinput_(MO)', 'Cmf_IC_(pF)']\n",
        "    # QC: None\n",
        "    # Consolidate with averaging\n",
        "    for c in cols:\n",
        "        for id in cell_summary_df.index:\n",
        "            multi_values = cell_summary_df_messy.at[id,c]\n",
        "            consolidated = np.mean(  multi_values )\n",
        "            cell_summary_df.at[id,c] = consolidated\n",
        "\n",
        "\n",
        "    'Fireing Rate Gain - IC'\n",
        "    cols = ['Firing_Gain_(Hz/pA)','R2 (Firing_Gain_R2)','Gain_Stims_pA','Gain_NumSpikes']\n",
        "    # QC: R2 (r squared) should be decent so that (>0.8) we fit a reasonable section of curve with a line\n",
        "    # Consolidate by selecting best Range of Response\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "    for id in cell_summary_df.index:\n",
        "        # print(id)\n",
        "        multi_values_Spikes = cell_summary_df_messy.at[id,'Gain_NumSpikes']\n",
        "        range_list =[]\n",
        "        for trial in range(len(multi_values_Spikes)):\n",
        "            range_list.append(np.max( multi_values_Spikes[trial]) - np.min( multi_values_Spikes[trial]))\n",
        "        if len(multi_values_Spikes) > 0  :\n",
        "            chosen_one = np.where(range_list == np.max(range_list))[0]\n",
        "            if len(chosen_one)>0: chosen_one = chosen_one[0]\n",
        "            for c in cols: \n",
        "                if len(cell_summary_df_messy.at[id,c]) == 0:\n",
        "                    cell_summary_df.at[id,c] = np.nan\n",
        "                else:\n",
        "                    consolidated = cell_summary_df_messy.at[id,c][chosen_one]\n",
        "                    cell_summary_df.at[id,c] = consolidated.tolist()\n",
        "        else:\n",
        "            for c in cols:  \n",
        "                cell_summary_df.at[id,c] = np.nan\n",
        "\n",
        "    'IV ~I_Na , ~I_K'\n",
        "    cols = ['IV_V_stim', 'IV_I_peak', 'IV_I_mean']\n",
        "    # QC:  Look For normal Na 'swoop' and 'opposing Kv currents\n",
        "    # By eyeball, linear currents should not be extreme.\n",
        "    # Consolidate with averaging\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "\n",
        "    for id in cell_summary_df.index:\n",
        "        # print(id)\n",
        "        stim_values = cell_summary_df_messy['IV_V_stim'][id]\n",
        "        stim_values = [b for a in stim_values for b in a]\n",
        "        # print('stim_values', stim_values)\n",
        "        if len(stim_values)>0:\n",
        "            stim_set = list(set(stim_values))\n",
        "            stim_set.sort()\n",
        "            cell_summary_df.at[id,'IV_V_stim']=stim_set\n",
        "            for c in cols[1:]:\n",
        "                resp_vals = cell_summary_df_messy[c][id]\n",
        "                flattened_vals = [b for a in resp_vals for b in a]\n",
        "                # print('flattened_responses',flattened_vals)\n",
        "                response_list = []\n",
        "                for v in stim_set:\n",
        "                    # print(\"stim_values\",len(stim_values),v)\n",
        "                    # print('logical', stim_values == v)\n",
        "                    index_to_avg = np.where( np.equal(stim_values, v))\n",
        "                    # print('index_to_avg',index_to_avg)\n",
        "                    vals_to_avg = flattened_vals[index_to_avg[0][0]]\n",
        "                    # print('vals_to_avg',vals_to_avg)\n",
        "                    response_list.append( np.mean(vals_to_avg))\n",
        "                response_array = np.stack(response_list).tolist()\n",
        "                cell_summary_df.at[id,c]=response_array\n",
        "\n",
        "    # for c in cols:\n",
        "    #     cell_summary_df[c] = None\n",
        "    #     cell_summary_df[c] = cell_summary_df[c].astype(object)\n",
        "    #     for id in cell_summary_df.index:\n",
        "    #         print(cell_summary_df_messy.loc[id, 'IV_V_stim'])\n",
        "    #         multi_values = cell_summary_df_messy.at[id,c]\n",
        "    #         if len(multi_values)==0:\n",
        "    #             continue\n",
        "    #         multi_values = np.stack(multi_values)\n",
        "    #         consolidated = np.mean(multi_values,axis=0)\n",
        "    #         cell_summary_df.at[id,c] = consolidated\n",
        "    return cell_summary_df\n",
        "\n"
      ],
      "metadata": {
        "id": "UG6nSYbMMF0n"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def consolidate_store_data(abf_recordings_df):\n",
        "\n",
        "    unused = ['Ra_5.0','Rm_5.0','Cmq_5.0','Ra_Combo','Rm_Combo','Cmq_Combo']\n",
        "\n",
        "    abrg_col = ['protocol','datetime','Ra_10.0','Rm_10.0','Cmq_10.0', 'Ra_160.0','Rm_160.0','Cmq_160.0', \n",
        "             'Gain_Stims_pA','Gain_NumSpikes', 'Firing_Gain_(Hz/pA)', 'R2 (Firing_Gain_R2)',\n",
        "            'Rheobase_(pA)','AP_Threshold(mV)', \n",
        "            'Rinput_(MO)','Spike_Latency_(ms)', \n",
        "            'IV_I_peak', 'IV_V_stim', 'IV_I_mean',\n",
        "            'Cmf_IC_(pF)','Vhold_Rheo_(mV)', 'Rheo Ihold_(pA)','V_hold_(Latency)','passing_sweeps',]\n",
        "\n",
        "    abrg_abf_recs_df = abf_recordings_df[abrg_col].copy()\n",
        "    cell_summary_df, abf_recordings_df_tagged = parse_file_name(abrg_abf_recs_df)\n",
        "    cell_summary_df.sort_index(inplace=True)\n",
        "    cell_summary_df_clean = stream_line_cell_summary(cell_summary_df)\n",
        "    #################### Store Data ##################\n",
        "    abrg_abf_recs_df.to_hdf('abrg_abf_recs_df.hdf','abrg_abf_recs_df')\n",
        "    abrg_abf_recs_df.to_csv('abrg_abf_recs_df.csv')\n",
        "    abf_recordings_df_tagged.to_csv('abf_recordings_df_tagged.csv')\n",
        "    cell_summary_df.to_csv('cell_summary_df.csv')\n",
        "    cell_summary_df_clean.to_csv('cell_summary_df_clean.csv')\n",
        "\n",
        "    from google.colab import files\n",
        "    files.download('abrg_abf_recs_df.csv')\n",
        "    files.download('abf_recordings_df_tagged.csv')\n",
        "    files.download('cell_summary_df.csv') \n",
        "    files.download('cell_summary_df_clean.csv')\n",
        "\n",
        "    return cell_summary_df_clean, abrg_abf_recs_df\n",
        "\n"
      ],
      "metadata": {
        "id": "uDhjbAyGhbf4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## The PipeLine Cell ###############################\n",
        "####### Depenencies ###########\n",
        "!pip install --upgrade pyabf\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyabf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from scipy import stats\n",
        "import os\n",
        "from scipy.signal import butter,filtfilt\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from IPython.display import clear_output\n",
        "from datetime import datetime\n",
        "clear_output(wait=False)\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "############ Get Files & Populate Data Frame ############\n",
        "\n",
        "link = \"https://www.dropbox.com/sh/95zg7iihp7vqizj/AAB8vdvsCqVjWDlE2GmCHT2fa?dl=0\"\n",
        "new_filename = str(datetime.now())[:10]+\"_hipp_data.zip\"\n",
        "file_loc = get_drobox_folder(link, new_filename)\n",
        "clear_output(wait=False)\n",
        "abf_recordings_df,protocol_set = catalogue_recs(file_loc)\n",
        "print(len(abf_recordings_df), 'files')\n",
        "print('Protocols:')\n",
        "_=[print(\"   \",p) for p in protocol_set]\n",
        "print('ABFS:')\n",
        "_=[print(\"   \",p) for p in abf_recordings_df.index]\n"
      ],
      "metadata": {
        "id": "_1cfqD5DbUDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### Run QC tests ###############\n",
        "print('running QC...')\n",
        "VC_prot = ['VC - MemTest-10ms-160ms',\n",
        "           'VC - Multi IV - 150ms',\n",
        "           'VC - 3min GapFree',\n",
        "           'VC - Spontaneous-3min-(MT)']\n",
        "IC_prot = ['IC - Gain - D20pA',\n",
        "           'IC - Gain - D50pA',\n",
        "           'IC - Rheobase',\n",
        "           'IC - R input',\n",
        "           'IC - Latentcy 800pA-1s']\n",
        "MT_prot = ['VC - MemTest-10ms-160ms']\n",
        "\n",
        "\n",
        "abf_recordings_df = QC_full_dataset(abf_recordings_df,to_plot=False,verbose=False,VC_prot=VC_prot,IC_prot=IC_prot,MT_prot=MT_prot)\n"
      ],
      "metadata": {
        "id": "J2bawVQmVmB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2468269-57ac-4faf-9b99-b4fbd283db1c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running QC...\n",
            "Voltage Clamp Protocols\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|         | 1/35 [00:00<00:07,  4.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|        | 5/35 [00:00<00:04,  7.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|        | 7/35 [00:00<00:03,  8.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|      | 12/35 [00:01<00:02,  7.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|     | 17/35 [00:01<00:01, 11.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|    | 21/35 [00:02<00:01,  7.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|  | 26/35 [00:03<00:01,  7.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|| 33/35 [00:03<00:00, 11.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 35/35 [00:04<00:00,  7.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Clamp Protocols\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 41/41 [00:18<00:00,  2.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## Multi Mem Test ########################\n",
        "print('Passive Membrane params...')\n",
        "protocol_aliases = ['VC - MemTest-10ms-160ms']\n",
        "abf_recordings_df, correct_protocol = Icapacitance_analysis(abf_recordings_df, protocol_aliases,to_plot=False,verbose=False)\n",
        "# column_pairs = [('Ra_250.0','Ra_500.0'), ('Rm_250.0','Rm_500.0'), ('Cmq_250.0','Cmq_500.0')]\n",
        "# abf_recordings_df = combine_memtest_durations(column_pairs,abf_recordings_df)\n",
        "\n",
        "\n",
        "################### Rheobase Analyisis ###########################\n",
        "print('Rheobase Analyisis...')\n",
        "Rheo_aliases = ['IC - Rheobase']\n",
        "abf_recordings_df = Rheo_curve_analysis(abf_recordings_df, Rheo_aliases,to_plot = False,single_spike=False)\n",
        "\n",
        "############## SpikeLatency ################\n",
        "print('SpikeLatency...')\n",
        "SpikeLatency_aliases = ['IC - Latentcy 800pA-1s']\n",
        "abf_recordings_df = Spike_latency(abf_recordings_df, SpikeLatency_aliases,to_plot = False)\n",
        "\n",
        "################### IClamp Input Resistance ##############\n",
        "print('IClamp Input Resistance...')\n",
        "inputR_aliases = ['IC - R input']\n",
        "abf_recordings_df = inputR_analysis(abf_recordings_df, inputR_aliases,to_plot=False)\n",
        "\n",
        "################### IV Curve ###################\n",
        "print('IV Curve...')\n",
        "protocol_aliases = ['VC - Multi IV - 150ms']\n",
        "abf_recordings_df = IV_analyisis(abf_recordings_df, protocol_aliases,to_plot=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QlTGCr9eKCcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## Fireing Rate Gain ################\n",
        "print('Fireing Rate Gain...')\n",
        "IFcurve_aliases = ['IC - Gain - D20pA']\n",
        "abf_recordings_df = IF_curve_analysis(abf_recordings_df, IFcurve_aliases,to_plot = [False, False])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eEcZQ1buEdB",
        "outputId": "5f61a2ff-7b57-4a39-83bc-3c5009b1f934"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rheobase Analyisis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 12/12 [00:09<00:00,  1.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "#################### Consolidate and Summarize Data ##################\n",
        "########################################################################\n",
        "cell_summary_df_clean, abrg_abf_recs_df = consolidate_store_data(abf_recordings_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "zN9Fzc9Qmagu",
        "outputId": "77e8f3ec-c058-41bb-95e3-2d6a3ace1c24"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10  cells from 1 animals\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_64cf1db5-30a6-4e3d-889a-1f757bc6fe69\", \"abrg_abf_recs_df.csv\", 24596)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f1e2ed84-fc45-4e06-8384-b102aa4d9361\", \"abf_recordings_df_tagged.csv\", 24596)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_597df53d-61c9-4eda-b2a8-57b240bf3a2e\", \"cell_summary_df.csv\", 19163)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bf31328d-cab6-4ff8-8828-2b2cd61ba21e\", \"cell_summary_df_clean.csv\", 7100)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_col(df_old,col_name,verbose=False,default_names=True):\n",
        "    'Blow up composite columns into individual columns'\n",
        "    df = df_old.copy()\n",
        "    ser_to_exp = df[col_name]\n",
        "    def safe_len(x):\n",
        "        try: return len(x)\n",
        "        except: return 0\n",
        "    \n",
        "    new_col_num = np.max( [safe_len(i) for i in ser_to_exp])\n",
        "    new_names = [col_name +'_'+str(i) for i in range(new_col_num)]\n",
        "    if default_names:\n",
        "        new_names = [col_name +'_'+str(i) for i in range(new_col_num)]\n",
        "    else:\n",
        "        new_names = default_names\n",
        "    \n",
        "    for ni in range(new_col_num):\n",
        "        name = new_names[ni]\n",
        "        df[name] = np.nan\n",
        "    for row in df.index:\n",
        "        for i in range(safe_len(df.loc[row,col_name])):\n",
        "            df.at[row,new_names[i]] = df.loc[row,col_name][i]\n",
        "\n",
        "    \n",
        "    old_cols = list(df_old.columns)\n",
        "    old_col_ind = [i for i in range(len(old_cols)) if col_name in old_cols[i]][0]\n",
        "\n",
        "    \n",
        "    new_order = old_cols[:old_col_ind] + new_names + old_cols[old_col_ind+1:]\n",
        "    df = df[new_order]\n",
        "    if verbose: _ = [print(c) for c in cell_summary_exl_friendly.columns]\n",
        "    return df\n",
        "\n",
        "from google.colab import files\n",
        "cell_summary_exl_friendly = cell_summary_df_clean.copy()\n",
        "cell_summary_exl_friendly = expand_col(cell_summary_exl_friendly,'Gain_Stims_pA')\n",
        "cell_summary_exl_friendly = expand_col(cell_summary_exl_friendly,'Gain_NumSpikes')\n",
        "cell_summary_exl_friendly.to_csv('cell_summary_exl_friendly.csv')\n",
        "files.download('cell_summary_exl_friendly.csv')"
      ],
      "metadata": {
        "id": "BbtXnXhPvD8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### PCA CLUSTER ##########\n",
        "\n",
        "# _ = [print(c,cell_summary_exl_friendly.columns[c]) for c in range(len(cell_summary_exl_friendly.columns))]\n",
        "col_inds_for_pca = [9] + [12] + list(range(13,17)) + [18] + list(range(31,40))\n",
        "cols_for_pca = cell_summary_exl_friendly.columns[col_inds_for_pca]\n",
        "# _ = [print(c) for c in cols_for_pca]\n",
        "df = cell_summary_exl_friendly[cols_for_pca]\n",
        "# display(df)\n",
        "X_ = df.to_numpy(dtype='float32')\n",
        "nan_bool = np.logical_not(np.isnan(X_))\n",
        "row_nan = np.all(nan_bool,axis=1)\n",
        "df = df[row_nan]\n",
        "# display(df)\n",
        "X_ = X_[row_nan,:]\n",
        "# print(X_)\n",
        "\n",
        "import sklearn.decomposition\n",
        "import sklearn.cluster\n",
        "\n",
        "pca = sklearn.decomposition.PCA(n_components=9).fit(X_)\n",
        "print(np.cumsum(pca.explained_variance_ratio_))\n",
        "pca = sklearn.decomposition.PCA(n_components=5).fit(X_)\n",
        "print(np.cumsum(pca.explained_variance_ratio_))\n",
        "\n",
        "X_trans = pca.transform(X_)\n",
        "X_red = pca.inverse_transform(X_trans)\n",
        "\n",
        "\n",
        "RANDOM_SEED = 5\n",
        "kmeans = sklearn.cluster.KMeans(n_clusters=2,random_state=RANDOM_SEED).fit(X_trans)\n",
        "Centroids_red = pca.inverse_transform(kmeans.cluster_centers_)\n",
        "null_kmeans = sklearn.cluster.KMeans(n_clusters=1).fit(X_trans)\n",
        "null_centoid =  pca.inverse_transform(null_kmeans.cluster_centers_)[0]\n",
        "# print(Centroids_red)\n",
        "# print(null_centoid)\n",
        "\n",
        "type_0_cent = 100*(Centroids_red[0] / null_centoid - 1)\n",
        "type_1_cent = 100*(Centroids_red[1] / null_centoid - 1)\n",
        "\n",
        "X_rel = (100*(X_ / null_centoid - 1))\n",
        "\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(12, 6))\n",
        "x = np.arange(len(cols_for_pca))*-1\n",
        "\n",
        "wid = .3\n",
        "\n",
        "\n",
        "is_type_0 = kmeans.labels_ == 0\n",
        "is_type_1 = kmeans.labels_ == 1\n",
        "X_at0 = X_rel[is_type_0,:]  \n",
        "X_at1 = X_rel[is_type_1,:] \n",
        "\n",
        "\n",
        "axs[0].barh(x-wid/2, type_0_cent, wid,color = 'turquoise',label='Ephys type_0'+' n=' +str(sum(is_type_0)))\n",
        "axs[0].barh(x+wid/2, type_1_cent, wid,color = 'magenta',label='Ephys type_1'+' n=' +str(sum(is_type_1)))\n",
        "\n",
        "x_exp0 = np.repeat(np.expand_dims(x, axis=0),repeats =sum(is_type_0), axis=0)\n",
        "axs[1].plot(X_at0.transpose(), x_exp0.transpose(), '-.',color = 'turquoise')\n",
        "\n",
        "x_exp1 = np.repeat(np.expand_dims(x, axis=0),repeats =sum(is_type_1), axis=0)\n",
        "axs[1].plot(X_at1.transpose(), x_exp1.transpose(), '-.',color = 'magenta' )\n",
        "\n",
        "for ax in axs:\n",
        "    axs[0].legend()\n",
        "    ax.set_yticks(x)\n",
        "    axs[0].set_yticklabels(cols_for_pca, rotation=0)\n",
        "    ax.set_xlabel('% Diff Global Mean')\n",
        "plt.show()\n",
        "\n",
        "fig.savefig('clusters.png',dpi=300)\n",
        "files.download('clusters.png')"
      ],
      "metadata": {
        "id": "Bvw7XZ7fz13F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}